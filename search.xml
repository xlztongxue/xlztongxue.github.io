<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>特征工程-特征降维</title>
    <url>/2020/02/03/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4/</url>
    <content><![CDATA[]]></content>
  </entry>
  <entry>
    <title>聚类算法</title>
    <url>/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h4 id="聚类算法简介"><a href="#聚类算法简介" class="headerlink" title="聚类算法简介"></a>聚类算法简介</h4><p> <strong>使用不同的聚类准则，产生的聚类结果不同</strong>。 </p>
<a id="more"></a>
<ol>
<li><h5 id="聚类算法在现实中的应用"><a href="#聚类算法在现实中的应用" class="headerlink" title="聚类算法在现实中的应用"></a>聚类算法在现实中的应用</h5><ul>
<li>用户画像，广告推荐，Data Segmentation，搜索引擎的流量推荐，恶意流量识别</li>
<li>基于位置信息的商业推送，新闻聚类，筛选排序</li>
<li>图像分割，降维，识别；离群点检测；信用卡异常消费；发掘相同功能的基因片段</li>
</ul>
</li>
<li><h5 id="聚类算法的概念"><a href="#聚类算法的概念" class="headerlink" title="聚类算法的概念"></a>聚类算法的概念</h5><p>一种典型的<strong>无监督</strong>学习算法，主要用于将相似的样本自动归到一个类别中。在聚类算法中根据样本之间的相似性，将样本划分到不同的类别中，对于不同的相似度计算方法，会得到不同的聚类结果，常用的相似度计算方法有欧式距离法。</p>
</li>
<li><h5 id="聚类算法与分类算法最大的区别"><a href="#聚类算法与分类算法最大的区别" class="headerlink" title="聚类算法与分类算法最大的区别"></a>聚类算法与分类算法最大的区别</h5><p> 聚类算法是无监督的学习算法，而分类算法属于监督的学习算法。 </p>
</li>
</ol>
<h4 id="聚类算法API"><a href="#聚类算法API" class="headerlink" title="聚类算法API"></a>聚类算法API</h4><ol>
<li><h5 id="API介绍"><a href="#API介绍" class="headerlink" title="API介绍"></a>API介绍</h5><figure class="highlight gml"><table><tr><td class="code"><pre><span class="line">sklearn.cluster.KMeans(n_clusters=<span class="number">8</span>)</span><br><span class="line">参数:</span><br><span class="line">	n_clusters:开始的聚类中心数量</span><br><span class="line">		整型，缺省值=<span class="number">8</span>，生成的聚类数，即产生的质心（centroids）数。</span><br><span class="line">方法:</span><br><span class="line">	estimator.fit(<span class="symbol">x</span>)</span><br><span class="line">	estimator.predict(<span class="symbol">x</span>)</span><br><span class="line">	estimator.fit_predict(<span class="symbol">x</span>)</span><br><span class="line">		计算聚类中心并预测每个样本属于哪个类别,相当于先调用fit(<span class="symbol">x</span>),然后再调用predict(<span class="symbol">x</span>)</span><br></pre></td></tr></table></figure>
</li>
<li><h5 id="小案例"><a href="#小案例" class="headerlink" title="小案例"></a>小案例</h5><p> 随机创建不同二维数据集作为训练集，并结合k-means算法将其聚类，你可以尝试分别聚类不同数量的簇，并观察聚类效果： </p>
<p> <img src="/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/cluser_demo1.png" alt="image-20190219163451509" style="zoom:33%;"> </p>
<p> 聚类参数n_cluster传值不同，得到的聚类结果不同 </p>
<p> <img src="/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/cluster_demo2.png" alt="image-20190219163505530" style="zoom:33%;"> </p>
<p>1.<strong>流程分析</strong></p>
<p> <img src="/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/cluster_demo3.png" alt="image-20190219163649472" style="zoom:33%;"> </p>
<p>2.<strong>代码实现</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> calinski_harabaz_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.创建数据集</span></span><br><span class="line"><span class="comment"># X为样本特征，Y为样本簇类别， 共1000个样本，每个样本4个特征，共4个簇，</span></span><br><span class="line"><span class="comment"># 簇中心在[-1,-1], [0,0],[1,1], [2,2]， 簇方差分别为[0.4, 0.2, 0.2, 0.2]</span></span><br><span class="line">X, y = make_blobs(n_samples=<span class="number">1000</span>, n_features=<span class="number">2</span>, centers=[[<span class="number">-1</span>, <span class="number">-1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>]],</span><br><span class="line">                  cluster_std=[<span class="number">0.4</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>],</span><br><span class="line">                  random_state=<span class="number">9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集可视化</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], marker=<span class="string">'o'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.使用k-means进行聚类,并使用CH方法评估</span></span><br><span class="line">y_pred = KMeans(n_clusters=<span class="number">2</span>, random_state=<span class="number">9</span>).fit_predict(X)</span><br><span class="line"><span class="comment"># 分别尝试n_cluses=2\3\4,然后查看聚类效果</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y_pred)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用Calinski-Harabasz Index评估的聚类分数</span></span><br><span class="line">print(calinski_harabaz_score(X, y_pred))</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="聚类算法实现流程"><a href="#聚类算法实现流程" class="headerlink" title="聚类算法实现流程"></a>聚类算法实现流程</h4><blockquote>
<p><strong>k-means其实包含两层内容：</strong></p>
<ul>
<li>K : 初始中心点个数（计划聚类数）</li>
<li>means：求中心点到其他数据点距离的平均值</li>
</ul>
</blockquote>
<h5 id="1-k-means聚类步骤"><a href="#1-k-means聚类步骤" class="headerlink" title="1. k-means聚类步骤"></a>1. k-means聚类步骤</h5><figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="number">1</span>、随机设置K个特征空间内的点作为初始的聚类中心</span><br><span class="line"><span class="number">2</span>、对于其他每个点计算到K个中心的距离，未知的点选择最近的一个聚类中心点作为标记类别</span><br><span class="line"><span class="number">3</span>、接着对着标记的聚类中心之后，重新计算出每个聚类的新中心点（平均值）</span><br><span class="line"><span class="number">4</span>、如果计算得出的新中心点与原中心点一样（质心不再移动），那么结束，否则重新进行第二步过程</span><br></pre></td></tr></table></figure>
<p> 通过下图解释实现流程： </p>
<p> <img src="/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/K-means%E8%BF%87%E7%A8%8B%E5%88%86%E6%9E%90.png" alt="K-meansè¿‡ç¨‹åˆ†æž" style="zoom:33%;"></p>
<p>  k聚类动态效果图 </p>
<p><img src="/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/kmeans_dynamic.png" alt="2019-02-19 17.06.49" style="zoom: 33%;"></p>
<h5 id="2-案例练习"><a href="#2-案例练习" class="headerlink" title="2.案例练习"></a>2.案例练习</h5><p> <img src="/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/kmeans_demo1.png" alt="image-20190219171158984" style="zoom:33%;"></p>
<p>  1.随机设置K个特征空间内的点作为初始的聚类中心（本案例中设置p1和p2） </p>
<p> <img src="/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/kmeans_demo2.png" alt="image-20190219171244828" style="zoom:33%;"></p>
<p>  2.对于其他每个点计算到K个中心的距离，未知的点选择最近的一个聚类中心点作为标记类别 </p>
<p> <img src="/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/kmeans_demo3.png" alt="image-20190219171326923" style="zoom:33%;"></p>
<p>  <img src="/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/kmeans_demo4.png" alt="image-20190219171338441" style="zoom:33%;"></p>
<p>  3.接着对着标记的聚类中心之后，重新计算出每个聚类的新中心点（平均值） </p>
<p> <img src="/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/kmeans_demo5.png" alt="image-20190219171727035" style="zoom:33%;"></p>
<p> 4.如果计算得出的新中心点与原中心点一样（质心不再移动），那么结束，否则重新进行第二步过程【经过判断，需要重复上述步骤，开始新一轮迭代】 </p>
<p> <img src="/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/kmeans_demo6.png" alt="image-20190219171951607" style="zoom:33%;"></p>
<p>  <img src="/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/kmeans_demo7.png" alt="image-20190219172011618" style="zoom:33%;"> </p>
<p>  5.当每次迭代结果不变时，认为算法收敛，聚类完成，<strong>K-Means一定会停下，不可能陷入一直选质心的过程。</strong> </p>
<p> <img src="/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/kmeans_demo8.png" alt="image-20190219172125388" style="zoom:33%;"></p>
<h4 id="聚类算法模型评估"><a href="#聚类算法模型评估" class="headerlink" title="聚类算法模型评估"></a>聚类算法模型评估</h4><h5 id="1-误差平方和-SSE-The-sum-of-squares-due-to-error"><a href="#1-误差平方和-SSE-The-sum-of-squares-due-to-error" class="headerlink" title="1.误差平方和(SSE \The sum of squares due to error)"></a>1.误差平方和(SSE \The sum of squares due to error)</h5><p> 举例:(下图中数据-0.2, 0.4, -0.8, 1.3, -0.7, 均为真实值和预测值的差) </p>
<p> <img src="/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/sse1.png" alt="image-20190308211436382"></p>
<p>  在k-means中的应用: </p>
<p> <img src="/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/sse2.png" alt="img"></p>
<p>  <img src="/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/sse3.png" alt="image-20190219173610490"> </p>
<p> 公式各部分内容: </p>
<p> <img src="/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/sse4.png" alt="image-20190308211313308"></p>
<p> 上图中: k=2</p>
<ul>
<li><p><strong>SSE图最终的结果，对图松散度的衡量.</strong>(eg: **SSE(左图))</p>
</li>
<li><p>SSE随着聚类迭代，其值会越来越小，直到最后趋于稳定</p>
<p><img src="/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/sse5.png" alt="2019-02-19 17.38.11"> </p>
</li>
<li><p>如果质心的初始值选择不好，SSE只会达到一个不怎么好的局部最优解. </p>
<p><img src="/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/sse6.png" alt="2019-02-19 17.39.29"> </p>
</li>
</ul>
<h5 id="2-“肘”方法-Elbow-method-—-K值确定"><a href="#2-“肘”方法-Elbow-method-—-K值确定" class="headerlink" title="2.“肘”方法 (Elbow method) — K值确定"></a>2.“肘”方法 (Elbow method) — K值确定</h5><p> <img src="/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/elbow_method.png" alt="image-20190219174520309"></p>
<p> （1）对于n个点的数据集，迭代计算k from 1 to n，每次聚类完成后计算每个点到其所属的簇中心的距离的平方和；</p>
<p>（2）平方和是会逐渐变小的，直到k==n时平方和为0，因为每个点都是它所在的簇中心本身。</p>
<p>（3）在这个平方和变化过程中，会出现一个拐点也即“肘”点，<strong>下降率突然变缓时即认为是最佳的k值</strong>。</p>
<p>在决定什么时候停止训练时，肘形判据同样有效，数据通常有更多的噪音，在<strong>增加分类无法带来更多回报时，我们停止增加类别</strong>。</p>
<h5 id="3-轮廓系数法（Silhouette-Coefficient）"><a href="#3-轮廓系数法（Silhouette-Coefficient）" class="headerlink" title="3.轮廓系数法（Silhouette Coefficient）"></a>3.轮廓系数法（Silhouette Coefficient）</h5><p> 结合了聚类的凝聚度（Cohesion）和分离度（Separation），用于评估聚类的效果： </p>
<p> <img src="/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/sc.png" alt="image-20190219174853018"></p>
<p> <strong>目的：</strong> 内部距离最小化，外部距离最大化</p>
<p> <img src="/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/sc1.png" alt="image-20190219175813875"> </p>
<p>计算样本$i$到同簇其他样本的平均距离$a_i$，$a_i$ 越小样本i的簇内不相似度越小，说明样本$i$越应该被聚类到该簇。</p>
<p>计算样本i到最近簇$C<em>j$ 的所有样本的平均距离$b</em>(ij)$，称样本$i$与最近簇$C_j$ 的不相似度，定义为样本i的簇间不相似度：</p>
<p>$b<em>i =min{b</em>(i1), b<em>(i2), …, b</em>(ik)}$，$b_i$越大，说明样本$i$越不属于其他簇。</p>
<p>求出所有样本的轮廓系数后再求平均值就得到了<strong>平均轮廓系数</strong>。</p>
<p>平均轮廓系数的取值范围为[-1,1]，系数越大，聚类效果越好。</p>
<p>簇内样本的距离越近，簇间样本距离越远</p>
<p><strong>案例：</strong></p>
<p>下图是500个样本含有2个feature的数据分布情况，我们对它进行SC系数效果衡量：</p>
<p> <img src="/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/sc2.png" alt="image-20190219175321181"> </p>
<p><strong>n_clusters = 2 The average silhouette_score is : 0.7049787496083262</strong></p>
<p>n_clusters = 3 The average silhouette_score is : 0.5882004012129721</p>
<p><strong>n_clusters = 4 The average silhouette_score is : 0.6505186632729437</strong></p>
<p>n_clusters = 5 The average silhouette_score is : 0.56376469026194</p>
<p>n_clusters = 6 The average silhouette_score is : 0.4504666294372765</p>
<p>n_clusters 分别为 2，3，4，5，6时，SC系数如下，是介于[-1,1]之间的度量指标：</p>
<p><strong>每次聚类后，每个样本都会得到一个轮廓系数，当它为1时，说明这个点与周围簇距离较远，结果非常好，当它为0，说明这个点可能处在两个簇的边界上，当值为负时，暗含该点可能被误分了。</strong></p>
<p>从平均SC系数结果来看，K取3，5，6是不好的，那么2和4呢？</p>
<p>k=2的情况：</p>
<p> <img src="/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/sc3.png" alt="image-20190219175529440"></p>
<p>  k=4的情况： </p>
<p> <img src="/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/sc4.png" alt="image-20190219175611967"></p>
<p> n_clusters = 2时，第0簇的宽度远宽于第1簇；</p>
<p>n_clusters = 4时，所聚的簇宽度相差不大，因此选择K=4，作为最终聚类个数。</p>
<h5 id="4-CH系数（Calinski-Harabasz-Index）"><a href="#4-CH系数（Calinski-Harabasz-Index）" class="headerlink" title="4.CH系数（Calinski-Harabasz Index）"></a>4.CH系数（Calinski-Harabasz Index）</h5><p><strong>Calinski-Harabasz：</strong>类别内部数据的协方差越小越好，类别之间的协方差越大越好（换句话说：类别内部数据的距离平方和越小越好，类别之间的距离平方和越大越好），</p>
<p>这样的Calinski-Harabasz分数s会高，分数s高则聚类效果越好。</p>
<p> <img src="/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/ch1.png" alt="image-20190219182033877"> </p>
<p>tr为<strong>矩阵的迹</strong>, Bk为类别之间的协方差矩阵，Wk为类别内部数据的协方差矩阵;</p>
<p>m为训练集样本数，k为类别数。</p>
<p> <img src="/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/ch2.png" alt="image-20190219182615777"></p>
<p> 使用矩阵的迹进行求解的理解：</p>
<p>矩阵的对角线可以表示一个物体的相似性</p>
<p>在机器学习里，主要为了获取数据的特征值，那么就是说，在任何一个矩阵计算出来之后，都可以简单化，只要获取矩阵的迹，就可以表示这一块数据的最重要的特征了，这样就可以把很多无关紧要的数据删除掉，达到简化数据，提高处理速度。</p>
<p>CH需要达到的目的：<strong>用尽量少的类别聚类尽量多的样本，同时获得较好的聚类效果。</strong></p>
<h4 id="聚类算法算法优化"><a href="#聚类算法算法优化" class="headerlink" title="聚类算法算法优化"></a>聚类算法算法优化</h4><p><strong>k-means算法小结</strong></p>
<p><strong>优点：</strong></p>
<p> 1.原理简单（靠近中心点），实现容易</p>
<p> 2.聚类效果中上（依赖K的选择）</p>
<p> 3.空间复杂度o(N)，时间复杂度o(I<em>K</em>N)</p>
<blockquote>
<p>N为样本点个数，K为中心点个数，I为迭代次数</p>
</blockquote>
<p><strong>缺点：</strong></p>
<p> 1.对离群点，噪声敏感 （中心点易偏移）</p>
<p> 2.很难发现大小差别很大的簇及进行增量计算</p>
<p> 3.结果不一定是全局最优，只能保证局部最优（与K的个数及初值选取有关）</p>
<h5 id="1-Canopy算法配合初始聚类"><a href="#1-Canopy算法配合初始聚类" class="headerlink" title="1.Canopy算法配合初始聚类"></a>1.Canopy算法配合初始聚类</h5><h5 id="2-K-means"><a href="#2-K-means" class="headerlink" title="2.K-means++"></a>2.K-means++</h5><h5 id="3-二分k-means"><a href="#3-二分k-means" class="headerlink" title="3.二分k-means"></a>3.二分k-means</h5><h5 id="4-k-medoids（k-中心聚类算法）"><a href="#4-k-medoids（k-中心聚类算法）" class="headerlink" title="4.k-medoids（k-中心聚类算法）"></a>4.k-medoids（k-中心聚类算法）</h5><h5 id="5-Kernel-k-means"><a href="#5-Kernel-k-means" class="headerlink" title="5.Kernel k-means"></a>5.Kernel k-means</h5><h5 id="6-ISODATA"><a href="#6-ISODATA" class="headerlink" title="6.ISODATA"></a>6.ISODATA</h5><h5 id="7-Mini-Batch-K-Means"><a href="#7-Mini-Batch-K-Means" class="headerlink" title="7.Mini Batch K-Means"></a>7.Mini Batch K-Means</h5><h4 id="案例-探究用户对物品类别的喜好细分"><a href="#案例-探究用户对物品类别的喜好细分" class="headerlink" title="案例-探究用户对物品类别的喜好细分"></a>案例-探究用户对物品类别的喜好细分</h4><h4 id="拓展-算法选择指导"><a href="#拓展-算法选择指导" class="headerlink" title="拓展-算法选择指导"></a>拓展-算法选择指导</h4><p><strong>关于在计算的过程中，如何选择合适的算法进行计算，可以参考scikit learn官方给的指导意见：</strong></p>
<p> <img src="/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/scikit-learn%E7%AE%97%E6%B3%95%E9%80%89%E6%8B%A9%E8%B7%AF%E5%BE%84%E5%9B%BE.png" alt="scikit-learn算法选择路径图" style="zoom:33%;"> </p>
]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>机器</tag>
        <tag>学习机器学习算法</tag>
        <tag>聚类算法</tag>
        <tag>无监督学习算法</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐算法</title>
    <url>/2020/01/15/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h4 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h4><ul>
<li>了解推荐模型构建流程</li>
<li>理解协同过滤原理</li>
<li>记忆相似度计算方法</li>
<li>应用杰卡德相似度实现简单协同过滤推荐案例</li>
</ul>
<a id="more"></a>]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐算法</tag>
        <tag>协同过滤推荐算法</tag>
      </tags>
  </entry>
  <entry>
    <title>推荐系统</title>
    <url>/2020/01/15/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<h4 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h4><ul>
<li>了解推荐系统概念及产生背景</li>
<li>理解推荐系统工作原理及作用</li>
<li>了解推荐系统与web项目区别</li>
<li>了解推荐系统要素</li>
<li>理解推荐系统架构</li>
</ul>
<a id="more"></a>
<h4 id="推荐系统概念"><a href="#推荐系统概念" class="headerlink" title="推荐系统概念"></a>推荐系统概念</h4><ol>
<li><h5 id="什么是推荐系统"><a href="#什么是推荐系统" class="headerlink" title="什么是推荐系统"></a>什么是推荐系统</h5><p> 没有明确需求的用户访问了我们的服务, 且服务的物品对用户构成了信息过载，系统通过一定的规则对物品进行排序，并将排在前面的物品展示给用户，这样的系统就是推荐系统 。</p>
</li>
<li><h5 id="信息过载-amp-用户需求不明确"><a href="#信息过载-amp-用户需求不明确" class="headerlink" title="信息过载 &amp; 用户需求不明确"></a>信息过载 &amp; 用户需求不明确</h5><ul>
<li>分类⽬录（1990s）：覆盖少量热门⽹站。典型应用：Hao123 Yahoo</li>
<li>搜索引擎（2000s）：通过搜索词明确需求。典型应用：Google Baidu</li>
<li>推荐系统（2010s）：不需要⽤户提供明确的需求，通过分析⽤ 户的历史⾏为给⽤户的兴趣进⾏建模，从⽽主动给⽤户推荐能 够满⾜他们兴趣和需求的信息。</li>
</ul>
</li>
<li><h5 id="推荐系统与搜索引擎"><a href="#推荐系统与搜索引擎" class="headerlink" title="推荐系统与搜索引擎"></a>推荐系统与搜索引擎</h5><p>| 搜索     | 推荐     |          |<br>| ———— | ———— | ———— |<br>| 行为方式 | 主动     | 被动     |<br>| 意图     | 明确     | 模糊     |<br>| 个性化   | 弱       | 强       |<br>| 流量分布 | 马太效应 | 长尾效应 |<br>| 目标     | 快速满足 | 持续服务 |<br>| 评估指标 | 简明     | 复杂     |</p>
</li>
</ol>
<h4 id="推荐系统的工作原理"><a href="#推荐系统的工作原理" class="headerlink" title="推荐系统的工作原理"></a>推荐系统的工作原理</h4><ul>
<li><strong>社会化推荐</strong> 例如：向朋友咨询，社会化推荐，让好友给自己推荐物品</li>
<li><strong>基于内容的推荐</strong> 例如：打开搜索引擎，输入自己喜欢的演员的名字，然后看看返回结果中还有什么电影是自己没看过的</li>
<li><strong>基于流行度的推荐</strong> 例如：查看票房排行榜</li>
<li><strong>基于协同过滤的推荐</strong> 例如：找到和自己历史兴趣相似的用户，看看他们最近在看什么电影</li>
</ul>
<h4 id="推荐系统的作用"><a href="#推荐系统的作用" class="headerlink" title="推荐系统的作用"></a>推荐系统的作用</h4><ul>
<li>高效连接用户和物品</li>
<li>提高用户停留时间和用户活跃程度</li>
<li>有效的帮助产品实现其商业价值</li>
</ul>
<h4 id="推荐系统的应用场景"><a href="#推荐系统的应用场景" class="headerlink" title="推荐系统的应用场景"></a>推荐系统的应用场景</h4><ul>
<li>头条</li>
<li>淘宝京东</li>
<li>抖音</li>
</ul>
<h4 id="推荐系统和Web项目的区别"><a href="#推荐系统和Web项目的区别" class="headerlink" title="推荐系统和Web项目的区别"></a>推荐系统和Web项目的区别</h4><ul>
<li>通过信息过滤实现目标提升 V.S. 稳定的信息流通系统<ul>
<li>web项目: 处理复杂业务逻辑，处理高并发，为用户构建一个稳定的信息流通服务</li>
<li>推荐系统: 追求指标增长， 留存率/阅读时间/GMV (Gross Merchandise Volume电商网站成交金额)/视频网站VV (Video View)</li>
</ul>
</li>
<li>确定 V.S. 不确定思维<ul>
<li>web项目: 对结果有确定预期</li>
<li>推荐系统: 结果是概率问题</li>
</ul>
</li>
</ul>
<h4 id="推荐系统要素"><a href="#推荐系统要素" class="headerlink" title="推荐系统要素"></a>推荐系统要素</h4><ul>
<li>UI 和 UE(前端界面)</li>
<li>数据 (Lambda架构)</li>
<li>业务知识</li>
<li>算法</li>
</ul>
<h4 id="推荐系统架构"><a href="#推荐系统架构" class="headerlink" title="推荐系统架构"></a>推荐系统架构</h4><ol>
<li><h5 id="推荐系统整体架构"><a href="#推荐系统整体架构" class="headerlink" title="推荐系统整体架构"></a>推荐系统整体架构</h5></li>
<li><h5 id="大数据Lambda架构"><a href="#大数据Lambda架构" class="headerlink" title="大数据Lambda架构"></a>大数据Lambda架构</h5></li>
<li><h5 id="推荐算法架构"><a href="#推荐算法架构" class="headerlink" title="推荐算法架构"></a>推荐算法架构</h5></li>
</ol>
]]></content>
      <categories>
        <category>推荐系统</category>
      </categories>
      <tags>
        <tag>推荐系统简介</tag>
        <tag>推荐系统架构</tag>
      </tags>
  </entry>
  <entry>
    <title>五大常用算法</title>
    <url>/2020/01/13/%E4%BA%94%E5%A4%A7%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>据说有人归纳了计算机的五大常用算法，它们是贪婪算法，动态规划算法，分治算法，回溯算法以及分支限界算法。这五个算法是有很多应用场景的，最优化问题大多可以利用这些算法解决。算法的本质就是解决问题。当数据量比较小时，其实根本就不需要什么算法，写一些for循环完全就可以很快速的搞定了，但是当数据量比较大，场景比较复杂的时候，算法就尤为重要了，本文先归纳这几个算法及应用场景，随后在细细品味。</p>
<a id="more"></a>
<h4 id="穷举法"><a href="#穷举法" class="headerlink" title="穷举法"></a>穷举法</h4><h5 id="1-定义"><a href="#1-定义" class="headerlink" title="1.定义"></a>1.定义</h5><p>穷举法也叫枚举法， 在进行归纳推理时，如果逐个考察了某类事件的所有可能情况，因而得出一般结论，那么这结论是可靠的，这种归纳方法叫做枚举法。枚举法是利用计算机运算速度快、精确度高的特点，对要解决问题的所有可能情况，一个不漏地进行检验，从中找出符合要求的答案，因此枚举法是通过牺牲时间来换取答案的全面性 。穷举法属于暴力破解法， 暴力破解法，就是把所有条件，相关情况统统考虑进去，让计算机进行检索，指导得出与之所有条件符合的结果 。</p>
<h5 id="2-基本思想"><a href="#2-基本思想" class="headerlink" title="2.基本思想"></a>2.基本思想</h5><ol>
<li>确定枚举对象、枚举范围和判定条件</li>
<li>枚举可能的解，验证是否是问题的解</li>
</ol>
<h5 id="3-应用实例"><a href="#3-应用实例" class="headerlink" title="3.应用实例"></a>3.应用实例</h5><ol>
<li>百钱买鸡问题</li>
<li>鸡兔同笼问题</li>
<li>搬砖块问题</li>
<li>猜数字</li>
<li>韩信点兵 </li>
</ol>
<h4 id="贪婪算法"><a href="#贪婪算法" class="headerlink" title="贪婪算法"></a>贪婪算法</h4><h5 id="1-定义-1"><a href="#1-定义-1" class="headerlink" title="1.定义"></a>1.定义</h5><p>贪婪算法(贪心算法)是指在对问题进行求解时，在每一步选择中都采取最好或者最优(即最有利)的选择，从而希望能够导致结果是最好或者最优的算法。贪婪算法所得到的结果往往不是最优的结果(有时候会是最优解)，但是都是相对近似(接近)最优解的结果。</p>
<h5 id="2-基本思想-1"><a href="#2-基本思想-1" class="headerlink" title="2.基本思想"></a>2.基本思想</h5><ol>
<li><p>建立数学模型来描述问题</p>
</li>
<li><p>把求解的问题分成若干个子问题</p>
</li>
<li><p>对每一子问题求解，得到子问题的局部最优解</p>
</li>
<li><p>把子问题对应的局部最优解合成原来整个问题的一个近似最优解</p>
</li>
</ol>
<h5 id="3-应用实例-1"><a href="#3-应用实例-1" class="headerlink" title="3.应用实例"></a>3.应用实例</h5><ol>
<li>钱币找零问题 </li>
<li>区间调度问题</li>
<li>背包问题 </li>
<li>均分纸牌 </li>
<li>最大整数</li>
</ol>
<h4 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h4><h5 id="1-定义-2"><a href="#1-定义-2" class="headerlink" title="1.定义"></a>1.定义</h5><p> 动态规划过程是：每次决策依赖于当前状态，又随即引起状态的转移。一个决策序列就是在变化的状态中产生出来的，所以，这种多阶段最优化决策解决问题的过程就称为动态规划。 </p>
<h5 id="2-基本思想-2"><a href="#2-基本思想-2" class="headerlink" title="2.基本思想"></a>2.基本思想</h5><ol>
<li>将待求解的问题分解为若干个子问题（阶段）</li>
<li>按顺序求解子阶段，前一子问题的解，为后一子问题的求解提供了有用的信息</li>
<li>在求解任一子问题时，列出各种可能的局部解，通过决策保留那些有可能达到最优的局部解，丢弃其他局部解</li>
<li>依次解决各子问题，最后一个子问题就是初始问题的解。 </li>
</ol>
<h5 id="3-应用实例-2"><a href="#3-应用实例-2" class="headerlink" title="3.应用实例"></a>3.应用实例</h5><ol>
<li>数字三角形问题</li>
<li>找零钱问题</li>
<li>走方格问题</li>
<li>最长公共序列数</li>
</ol>
<h4 id="分治算法"><a href="#分治算法" class="headerlink" title="分治算法"></a>分治算法</h4><h5 id="1-定义-3"><a href="#1-定义-3" class="headerlink" title="1.定义"></a>1.定义</h5><p> 分治算法的基本思想是将一个规模为N的问题分解为K个规模较小的子问题，这些子问题相互独立且与原问题性质相同。求出子问题的解，就可得到原问题的解。即一种分目标完成程序算法，简单问题可用二分法完成。 </p>
<h5 id="2-基本思想-3"><a href="#2-基本思想-3" class="headerlink" title="2.基本思想"></a>2.基本思想</h5><ol>
<li>先把问题分解成几个子问题</li>
<li>求出这几个子问题的解法</li>
<li>再找到合适的方法，把它们组合成求整个问题的解法。</li>
<li>如果这些子问题还较大，难以解决，可以再把它们分成几个更小的子问题</li>
<li>以此类推，直至可以直接求出解为止</li>
</ol>
<h5 id="3-应用实例-3"><a href="#3-应用实例-3" class="headerlink" title="3.应用实例"></a>3.应用实例</h5><ol>
<li>找出伪币</li>
<li>二分搜索</li>
<li>汉诺塔</li>
<li>归并排序</li>
<li>快速排序</li>
<li>大整数乘法</li>
</ol>
<h4 id="回溯算法"><a href="#回溯算法" class="headerlink" title="回溯算法"></a>回溯算法</h4><h5 id="1-定义-4"><a href="#1-定义-4" class="headerlink" title="1.定义"></a>1.定义</h5><p> 回溯算法实际上一个类似枚举的搜索尝试过程，主要是在搜索尝试过程中寻找问题的解，当发现已不满足求解条件时，就“回溯”返回，尝试别的路径。回溯法是一种选优搜索法，按选优条件向前搜索，以达到目标。但当探索到某一步时，发现原先选择并不优或达不到目标，就退回一步重新选择，这种走不通就退回再走的技术为回溯法，而满足回溯条件的某个状态的点称为“回溯点”。许多复杂的，规模较大的问题都可以使用回溯法，有“通用解题方法”的美称。 </p>
<h5 id="2-基本思想-4"><a href="#2-基本思想-4" class="headerlink" title="2.基本思想"></a>2.基本思想</h5><p> 从一条路往前走，能进则进，不能进则退回来，换一条路再试 </p>
<h5 id="3-应用实例-4"><a href="#3-应用实例-4" class="headerlink" title="3.应用实例"></a>3.应用实例</h5><ol>
<li>八皇后问题</li>
<li>图的着色问题 </li>
<li>装载问题 </li>
<li>批处理作业调度问题 </li>
<li>背包问题 </li>
<li>最大团问题 </li>
</ol>
<h4 id="分支限界算法"><a href="#分支限界算法" class="headerlink" title="分支限界算法"></a>分支限界算法</h4><h5 id="1-定义-5"><a href="#1-定义-5" class="headerlink" title="1.定义"></a>1.定义</h5><p>分支限界算法是按照广度优先的方式对解空间树（状态空间树）进行搜索，从而求得最优解的算法。</p>
<h5 id="2-基本思想-5"><a href="#2-基本思想-5" class="headerlink" title="2.基本思想"></a>2.基本思想</h5><p>在搜索的过程中，采用<strong>限界函数</strong>（bound function）估算所有子节点的目标函数的可能取值，从而选择使目标函数取极值（极大值或者极小值）的节点作为扩展结点（如果限界值没有超过目前的最优解，则剪枝）进行下一步搜索（重复 BFS -&gt; 计算所有子节点限界 -&gt; 选择最优子节点作为扩展结点的过程），从而不断调整搜索的方向，尽快找到问题的最优解。分支限界的思想类似于：图的广度优先搜索，树的层序遍历。</p>
<h5 id="3-应用实例-5"><a href="#3-应用实例-5" class="headerlink" title="3.应用实例"></a>3.应用实例</h5><ol>
<li>单源最短路径问题</li>
<li>装载问题</li>
<li>布线问题</li>
<li>0-1背包问题</li>
<li>最大团问题</li>
<li>旅行售货员问题</li>
</ol>
<h4 id="总结说明"><a href="#总结说明" class="headerlink" title="总结说明"></a>总结说明</h4><p>对于一个应用实例可能会有多种算法解决，算法是一种解决问题的思想，任意一个算法绝对不是一两篇文章可以讲清楚的。当然也不是通过一两道题目可以完全学会。学习算法的关键是<strong>用算法的思想去想问题，去解决实际问题</strong>，多刷题是养成算法思维解决问题的基础。</p>
<h4 id="学习算法方法"><a href="#学习算法方法" class="headerlink" title="学习算法方法"></a>学习算法方法</h4><h5 id="1-书籍"><a href="#1-书籍" class="headerlink" title="1.书籍"></a>1.书籍</h5><ol>
<li>数据结构</li>
<li>数据结构与算法分析 </li>
<li>算法导论</li>
</ol>
<h5 id="2-刷题"><a href="#2-刷题" class="headerlink" title="2.刷题"></a>2.刷题</h5><ol>
<li>牛客</li>
<li>LeeCode</li>
</ol>
<h5 id="3-在线视频课程"><a href="#3-在线视频课程" class="headerlink" title="3.在线视频课程"></a>3.在线视频课程</h5><ol>
<li>慕课</li>
<li>网易云课程（强烈推荐）</li>
</ol>
<h5 id="4-可视化工具"><a href="#4-可视化工具" class="headerlink" title="4.可视化工具"></a>4.可视化工具</h5><ul>
<li><a href="https://visualgo.net/" target="_blank" rel="noopener">visualgo 网址 </a> </li>
</ul>
<p>最重要的是耐心，自律，有毅力，坚持。</p>
<p>作为才入门的程序员，我已经沉浸在知识海洋中无法自拔，深深感受到了自己的渺小。</p>
]]></content>
      <categories>
        <category>计算机常用算法</category>
      </categories>
      <tags>
        <tag>计算机常用算法</tag>
        <tag>算法思想</tag>
      </tags>
  </entry>
  <entry>
    <title>集成学习</title>
    <url>/2020/01/13/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h4 id="什么是集成学习"><a href="#什么是集成学习" class="headerlink" title="什么是集成学习"></a>什么是集成学习</h4><p> 集成学习通过建立几个模型来解决单一预测问题。它的工作原理是<strong>生成多个分类器/模型</strong>，各自独立地学习和作出预测。<strong>这些预测最后结合成组合预测，因此优于任何一个单分类的做出预测。</strong> </p>
<a id="more"></a>
<p> <img src="/2020/01/13/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/ensemble1.png" alt="image-20190214154128155" style="zoom: 25%;"></p>
<h4 id="机器学习的两个核心任务"><a href="#机器学习的两个核心任务" class="headerlink" title="机器学习的两个核心任务"></a>机器学习的两个核心任务</h4><ul>
<li>任务一：<strong>如何优化训练数据</strong> —&gt; 主要用于<strong>解决欠拟合问题</strong></li>
<li>任务二：<strong>如何提升泛化性能</strong> —&gt; 主要用于<strong>解决过拟合问题</strong></li>
</ul>
<h4 id="集成学习中boosting和Bagging"><a href="#集成学习中boosting和Bagging" class="headerlink" title="集成学习中boosting和Bagging"></a>集成学习中boosting和Bagging</h4><p> <img src="/2020/01/13/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/ensemble3.png" alt="image-20190214155522366" style="zoom:25%;"> </p>
<p><strong>只要单分类器的表现不太差，集成学习的结果总是要好于单分类器的</strong></p>
<h4 id="Bagging集成原理"><a href="#Bagging集成原理" class="headerlink" title="Bagging集成原理"></a>Bagging集成原理</h4><p> 目标：把下面的圈和方块进行分类 </p>
<p> <img src="/2020/01/13/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/bagging1.png" alt="image-20190214160011298" style="zoom: 33%;"> </p>
<p>实现过程：</p>
<p>1.采样不同数据集</p>
<p> <img src="/2020/01/13/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/bagging2.png" alt="image-20190214160047813" style="zoom:33%;"></p>
<p> 2.训练分类器 </p>
<p> <img src="/2020/01/13/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/bagging3.png" alt="image-20190214160135609" style="zoom:33%;"> </p>
<ol>
<li><p>平权投票，获取最终结果 </p>
<p><img src="/2020/01/13/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/bagging4.png" alt="image-20190214160208609" style="zoom:33%;"></p>
<p>4.主要实现过程小结 </p>
<p><img src="/2020/01/13/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/bagging5.png" alt="image-20190214160248137" style="zoom:50%;"> </p>
</li>
</ol>
<h4 id="随机森林构造过程"><a href="#随机森林构造过程" class="headerlink" title="随机森林构造过程"></a>随机森林构造过程</h4><p>在机器学习中，<strong>随机森林是一个包含多个决策树的分类器</strong>，并且其输出的类别是由个别树输出的类别的众数而定。</p>
<p><strong>随机森林</strong> <strong>= Bagging +</strong> <strong>决策树</strong></p>
<p> <img src="/2020/01/13/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/tf1.png" alt="image-20190214160355272" style="zoom:33%;"></p>
<h4 id="随机森林api介绍"><a href="#随机森林api介绍" class="headerlink" title="随机森林api介绍"></a>随机森林api介绍</h4><figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">sklearn.ensemble.RandomForestClassifier(<span class="attribute">n_estimators</span>=10, <span class="attribute">criterion</span>=’gini’, <span class="attribute">max_depth</span>=None, <span class="attribute">bootstrap</span>=<span class="literal">True</span>, <span class="attribute">random_state</span>=None, <span class="attribute">min_samples_split</span>=2)</span><br><span class="line">	n_estimators：integer，optional（default = 10）森林里的树木数量120,200,300,500,800,1200</span><br><span class="line">	Criterion：string，可选（default =“gini”）分割特征的测量方法</span><br><span class="line">	max_depth：integer或None，可选（默认=无）树的最大深度 5,8,15,25,30</span><br><span class="line">	<span class="attribute">max_features</span>=<span class="string">"auto”,每个决策树的最大特征数量</span></span><br><span class="line"><span class="string">		If "</span>auto", then <span class="attribute">max_features</span>=sqrt(n_features).</span><br><span class="line">		<span class="keyword">If</span> <span class="string">"sqrt"</span>, then <span class="attribute">max_features</span>=sqrt(n_features)(same as <span class="string">"auto"</span>).</span><br><span class="line">		<span class="keyword">If</span> <span class="string">"log2"</span>, then <span class="attribute">max_features</span>=log2(n_features).</span><br><span class="line">		<span class="keyword">If</span> None, then <span class="attribute">max_features</span>=n_features.</span><br><span class="line">	bootstrap：boolean，optional（default = <span class="literal">True</span>）是否在构建树时使用放回抽样</span><br><span class="line">	min_samples_split:节点划分最少样本数</span><br><span class="line">	min_samples_leaf:叶子节点的最小样本数</span><br><span class="line">超参数：n_estimator, max_depth, min_samples_split,min_samples_leaf</span><br></pre></td></tr></table></figure>
<h4 id="随机森林预测案例"><a href="#随机森林预测案例" class="headerlink" title="随机森林预测案例"></a>随机森林预测案例</h4><ul>
<li>实例化随机森林</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 随机森林去进行预测</span></span><br><span class="line">rf = RandomForestClassifier()</span><br></pre></td></tr></table></figure>
<ul>
<li>定义超参数的选择列表</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">param = &#123;<span class="string">"n_estimators"</span>: [<span class="number">120</span>,<span class="number">200</span>,<span class="number">300</span>,<span class="number">500</span>,<span class="number">800</span>,<span class="number">1200</span>], <span class="string">"max_depth"</span>: [<span class="number">5</span>, <span class="number">8</span>, <span class="number">15</span>, <span class="number">25</span>, <span class="number">30</span>]&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>使用GridSearchCV进行网格搜索</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 超参数调优</span></span><br><span class="line">gc = GridSearchCV(rf, param_grid=param, cv=<span class="number">2</span>)</span><br><span class="line">gc.fit(x_train, y_train)</span><br><span class="line">print(<span class="string">"随机森林预测的准确率为："</span>, gc.score(x_test, y_test))</span><br></pre></td></tr></table></figure>
<blockquote>
<p>注意</p>
<ul>
<li>随机森林的建立过程</li>
<li>树的深度、树的个数等需要进行超参数调优</li>
</ul>
</blockquote>
<h4 id="bagging集成优点"><a href="#bagging集成优点" class="headerlink" title="bagging集成优点"></a>bagging集成优点</h4><p><strong>Bagging + 决策树/线性回归/逻辑回归/深度学习… = bagging集成学习方法</strong></p>
<p>经过上面方式组成的集成学习方法:</p>
<ol>
<li><strong>均可在原有算法上提高约2%左右的泛化正确率</strong></li>
<li><strong>简单, 方便, 通用</strong></li>
</ol>
<h4 id="boosting集成原理"><a href="#boosting集成原理" class="headerlink" title="boosting集成原理"></a>boosting集成原理</h4><p> <img src="/2020/01/13/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/boosting1.png" alt="image-20190214160534929" style="zoom:33%;"> </p>
<p><strong>随着学习的积累从弱到强</strong></p>
<p><strong>简而言之：每新加入一个弱学习器，整体能力就会得到提升</strong></p>
<p>代表算法：Adaboost，GBDT，XGBoost</p>
<h4 id="boosting实现过程"><a href="#boosting实现过程" class="headerlink" title="boosting实现过程"></a>boosting实现过程</h4><p>1.<strong>训练第一个学习器</strong></p>
<p> <img src="/2020/01/13/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/boosting2.png" alt="image-20190214160657608" style="zoom:33%;"></p>
<p> 2.<strong>调整数据分布</strong></p>
<p> <img src="/2020/01/13/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/boosting3.png" alt="image-20190214160727582" style="zoom:33%;"></p>
<p> 3.<strong>训练第二个学习器</strong></p>
<p> <img src="/2020/01/13/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/boostin4.png" alt="image-20190214160805813" style="zoom:33%;"></p>
<p> 4.<strong>再次调整数据分布</strong></p>
<p> <img src="/2020/01/13/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/boosting5.png" alt="image-20190214160900951" style="zoom:33%;"></p>
<p> 5.<strong>依次训练学习器，调整数据分布</strong></p>
<p> <img src="/2020/01/13/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/boosting6.png" alt="image-20190214160951473" style="zoom:33%;"></p>
<p> 6.<strong>整体过程实现</strong></p>
<p> <img src="/2020/01/13/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/boosting7.png" alt="image-20190214161215163" style="zoom:33%;"> </p>
<blockquote>
<p>关键点：<br>如何确认投票权重？<br>如何调整数据分布？</p>
</blockquote>
<p> <img src="/2020/01/13/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/boosting8.png" alt="image-20190214161243306" style="zoom:33%;"></p>
<p> <img src="/2020/01/13/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/boosting9.png" alt="image-20190214161305414" style="zoom:33%;">  </p>
<p><img src="/2020/01/13/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/boosting10.png" alt="image-20190214161432444" style="zoom:33%;"> </p>
<h4 id="bagging与boosting比较"><a href="#bagging与boosting比较" class="headerlink" title="bagging与boosting比较"></a>bagging与boosting比较</h4><blockquote>
<p>区别一：数据方面</p>
<p>​    Bagging：对数据进行采样训练；</p>
<p>​    Boosting：根据前一轮学习结果调整数据的重要性。</p>
<p>区别二：投票方面</p>
<p>​    Bagging：所有学习器平权投票；</p>
<p>​    Boosting：对学习器进行加权投票。</p>
<p>区别三：学习顺序</p>
<p>​    Bagging的学习是并行的，每个学习器没有依赖关系；</p>
<p>​    Boosting学习是串行，学习有先后顺序。</p>
<p>区别四：主要作用</p>
<p>​    Bagging主要用于提高泛化性能（解决过拟合，也可以说降低方差）</p>
<p>​    Boosting主要用于提高训练精度 （解决欠拟合，也可以说降低偏差）</p>
</blockquote>
<h4 id="boostingAPI介绍"><a href="#boostingAPI介绍" class="headerlink" title="boostingAPI介绍"></a>boostingAPI介绍</h4><figure class="highlight elixir"><table><tr><td class="code"><pre><span class="line">from sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line">api链接<span class="symbol">:https</span><span class="symbol">://scikit-learn</span>.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html<span class="comment">#sklearn.ensemble.AdaBoostClassifier</span></span><br></pre></td></tr></table></figure>
<h4 id="梯度提升决策树-GBDT"><a href="#梯度提升决策树-GBDT" class="headerlink" title="梯度提升决策树 GBDT"></a>梯度提升决策树 GBDT</h4><ol>
<li><h5 id="GBDT定义"><a href="#GBDT定义" class="headerlink" title="GBDT定义"></a>GBDT定义</h5><p>梯度提升决策树(GBDT Gradient Boosting Decision Tree) <strong>是一种迭代的决策树算法</strong>，<strong>该算法由多棵决策树组成，所有树的结论累加起来做最终答案。</strong>它在被提出之初就被认为是泛化能力（generalization)较强的算法。近些年更因为被用于搜索排序的机器学习模型而引起大家关注。</p>
<p><strong>GBDT = 梯度下降 + Boosting + 决策树</strong></p>
</li>
<li><h5 id="GBDT执行流程"><a href="#GBDT执行流程" class="headerlink" title="GBDT执行流程"></a>GBDT执行流程</h5><p> <img src="/2020/01/13/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/gbdt4.png" alt="image-20190307231055168"></p>
<p> 如果上式中的$h_i(x)=$决策树模型,则上式就变为:</p>
<p><strong>GBDT = 梯度下降 + Boosting + 决策树</strong></p>
</li>
<li><h5 id="GBDT案例"><a href="#GBDT案例" class="headerlink" title="GBDT案例"></a>GBDT案例</h5><p> 预测编号5的身高: </p>
<p>| 编号 | 年龄(岁) | 体重(KG) | 身高(M) |<br>| —— | ———— | ———— | ———- |<br>| 1    | 5        | 20       | 1.1     |<br>| 2    | 7        | 30       | 1.3     |<br>| 3    | 21       | 70       | 1.7     |<br>| 4    | 30       | 60       | 1.8     |<br>| 5    | 25       | 65       | ?       |</p>
<p>第一步：计算损失函数,并求出第一个预测值:</p>
<p> <img src="/2020/01/13/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/gbdt5.png" alt="image-20190307231938147" style="zoom:33%;"> </p>
<p> 第二步：求解划分点 </p>
<p> <img src="/2020/01/13/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/gbdt6.png" alt="image-20190307232225237" style="zoom:33%;"></p>
<p> 得出:年龄21为划分点的方差=0.01+0.0025=0.0125</p>
<p>第三步：通过调整后目标值,求解得出h1(x)</p>
<p> <img src="/2020/01/13/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/gbdt8.png" alt="image-20190307232510232" style="zoom:33%;"></p>
<p>  第四步：求解h2(x) </p>
<p> <img src="/2020/01/13/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/gbdt9.png" alt="image-20190307232816506" style="zoom:33%;"></p>
<p> 得出结果:</p>
<p><img src="/2020/01/13/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/gbdt10.png" alt="image-20190307232909127" style="zoom:33%;"> 编号5身高 = 1.475 + 0.03 + 0.275 = 1.78</p>
</li>
<li><h5 id="GBDT主要执行思想"><a href="#GBDT主要执行思想" class="headerlink" title="GBDT主要执行思想"></a>GBDT主要执行思想</h5><p>1.使用梯度下降法优化代价函数；</p>
<p>2.使用一层决策树作为弱学习器，负梯度作为目标值；</p>
<p>3.利用boosting思想进行集成。</p>
</li>
</ol>
<h4 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h4><p> <strong>XGBoost= 二阶泰勒展开+boosting+决策树+正则化</strong> </p>
<p><strong>Boosting</strong>：XGBoost使用Boosting提升思想对多个弱学习器进行迭代式学习</p>
<p><strong>二阶泰勒展开</strong>：每一轮学习中，XGBoost对损失函数进行二阶泰勒展开，使用一阶和二阶梯度进行优化。</p>
<p><strong>决策树</strong>：在每一轮学习中，XGBoost使用决策树算法作为弱学习进行优化。</p>
<p><strong>正则化</strong>：在优化过程中XGBoost为防止过拟合，在损失函数中加入惩罚项，限制决策树的叶子节点个数以及决策树叶子节点的值。</p>
<p><img src="/2020/01/13/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/taylor.png" alt="image-20190307234553645" style="zoom:33%;"> </p>
<p>泰勒展开越多，计算结果越精确</p>
]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>机器学习算法</tag>
        <tag>机器学习</tag>
        <tag>监督学习算法</tag>
        <tag>集成学习</tag>
        <tag>欠拟合过拟合</tag>
        <tag>bagging集成</tag>
        <tag>boosting集成</tag>
      </tags>
  </entry>
  <entry>
    <title>特征工程-特征提取</title>
    <url>/2020/01/11/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/</url>
    <content><![CDATA[<h4 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h4><ul>
<li><p>定义</p>
<p><strong>将任意数据（如文本或图像）转换为可用于机器学习的数字特征</strong></p>
<blockquote>
<p>注：特征值化是为了计算机更好的去理解数据</p>
</blockquote>
<ul>
<li>特征提取分类:<ul>
<li>字典特征提取(特征离散化)</li>
<li>文本特征提取</li>
<li>图像特征提取（深度学习将介绍）</li>
</ul>
</li>
</ul>
</li>
</ul>
<a id="more"></a>
<ul>
<li><p>特征提取API</p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">sklearn</span><span class="selector-class">.feature_extraction</span></span><br></pre></td></tr></table></figure>
</li>
</ul>
<h4 id="字典特征提取"><a href="#字典特征提取" class="headerlink" title="字典特征提取"></a>字典特征提取</h4><p><strong>作用：对字典数据进行特征值化</strong></p>
<ul>
<li>sklearn.feature_extraction.DictVectorizer(sparse=True,…)<ul>
<li>DictVectorizer.fit_transform(X)<ul>
<li>X：字典或者包含字典的迭代器返回值</li>
<li>返回sparse矩阵</li>
</ul>
</li>
<li>DictVectorizer.get_feature_names() 返回类别名称</li>
</ul>
</li>
</ul>
<p><strong>应用：</strong></p>
<ul>
<li>实例化类DictVectorizer</li>
<li>调用fit_transform方法输入数据并转换（注意返回格式）</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dict_demo</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    对字典类型的数据进行特征抽取</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    data = [&#123;<span class="string">'city'</span>: <span class="string">'北京'</span>,<span class="string">'temperature'</span>:<span class="number">100</span>&#125;, </span><br><span class="line">            &#123;<span class="string">'city'</span>: <span class="string">'上海'</span>,<span class="string">'temperature'</span>:<span class="number">60</span>&#125;,</span><br><span class="line">            &#123;<span class="string">'city'</span>: <span class="string">'深圳'</span>,<span class="string">'temperature'</span>:<span class="number">30</span>&#125;]</span><br><span class="line">    <span class="comment"># 1、实例化一个转换器类</span></span><br><span class="line">    transfer = DictVectorizer(sparse=<span class="literal">False</span>)</span><br><span class="line">    <span class="comment"># 2、调用fit_transform</span></span><br><span class="line">    data = transfer.fit_transform(data)</span><br><span class="line">    print(<span class="string">"返回的结果:\n"</span>, data)</span><br><span class="line">    <span class="comment"># 打印特征名字</span></span><br><span class="line">    print(<span class="string">"特征名字：\n"</span>, transfer.get_feature_names())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Nonepython</span><br></pre></td></tr></table></figure>
<p> 注意观察没有加上sparse=False参数的结果 </p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">返回的结果:</span><br><span class="line">   (<span class="number">0</span>, <span class="number">1</span>)    <span class="number">1.0</span></span><br><span class="line">  (<span class="number">0</span>, <span class="number">3</span>)    <span class="number">100.0</span></span><br><span class="line">  (<span class="number">1</span>, <span class="number">0</span>)    <span class="number">1.0</span></span><br><span class="line">  (<span class="number">1</span>, <span class="number">3</span>)    <span class="number">60.0</span></span><br><span class="line">  (<span class="number">2</span>, <span class="number">2</span>)    <span class="number">1.0</span></span><br><span class="line">  (<span class="number">2</span>, <span class="number">3</span>)    <span class="number">30.0</span></span><br><span class="line">特征名字：</span><br><span class="line"> [<span class="string">'city=上海'</span>, <span class="string">'city=北京'</span>, <span class="string">'city=深圳'</span>, <span class="string">'temperature'</span>]</span><br></pre></td></tr></table></figure>
<p> 这个结果并不是我们想要看到的，所以加上参数，得到想要的结果： </p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">返回的结果:</span><br><span class="line"> [[   <span class="number">0.</span>    <span class="number">1.</span>    <span class="number">0.</span>  <span class="number">100.</span>]</span><br><span class="line"> [   <span class="number">1.</span>    <span class="number">0.</span>    <span class="number">0.</span>   <span class="number">60.</span>]</span><br><span class="line"> [   <span class="number">0.</span>    <span class="number">0.</span>    <span class="number">1.</span>   <span class="number">30.</span>]]</span><br><span class="line">特征名字：</span><br><span class="line"> [<span class="string">'city=上海'</span>, <span class="string">'city=北京'</span>, <span class="string">'city=深圳'</span>, <span class="string">'temperature'</span>]</span><br></pre></td></tr></table></figure>
<p>之前在学习pandas中的离散化的时候，也实现了类似的效果。我们把这个处理数据的技巧叫做”one-hot“编码</p>
<p> <strong>对于特征当中存在类别信息的我们都会做one-hot编码处理</strong> 。</p>
<h4 id="文本特征提取"><a href="#文本特征提取" class="headerlink" title="文本特征提取"></a>文本特征提取</h4><p><strong>作用：对文本数据进行特征值化</strong></p>
<ul>
<li><strong>sklearn.feature_extraction.text.CountVectorizer(stop_words=[])</strong><ul>
<li>返回词频矩阵</li>
<li>CountVectorizer.fit_transform(X)<ul>
<li>X:文本或者包含文本字符串的可迭代对象</li>
<li>返回值:返回sparse矩阵</li>
</ul>
</li>
<li>CountVectorizer.get_feature_names() 返回值:单词列表</li>
</ul>
</li>
<li><strong>sklearn.feature_extraction.text.TfidfVectorizer</strong></li>
</ul>
<p><strong>应用：</strong></p>
<ul>
<li>实例化类CountVectorizer</li>
<li>调用fit_transform方法输入数据并转换 （注意返回格式，利用toarray()进行sparse矩阵转换array数组）</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">text_count_demo</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    对文本进行特征抽取，countvetorizer</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    data = [<span class="string">"life is short,i like like python"</span>, <span class="string">"life is too long,i dislike python"</span>]</span><br><span class="line">    <span class="comment"># 1、实例化一个转换器类</span></span><br><span class="line">    <span class="comment"># transfer = CountVectorizer(sparse=False) # 注意,没有sparse这个参数</span></span><br><span class="line">    transfer = CountVectorizer()</span><br><span class="line">    <span class="comment"># 2、调用fit_transform</span></span><br><span class="line">    data = transfer.fit_transform(data)</span><br><span class="line">    print(<span class="string">"文本特征抽取的结果：\n"</span>, data.toarray())</span><br><span class="line">    print(<span class="string">"返回特征名字：\n"</span>, transfer.get_feature_names())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<p>返回结果：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">文本特征抽取的结果：</span><br><span class="line"> [[<span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span>]]</span><br><span class="line">返回特征名字：</span><br><span class="line"> [<span class="string">'dislike'</span>, <span class="string">'is'</span>, <span class="string">'life'</span>, <span class="string">'like'</span>, <span class="string">'long'</span>, <span class="string">'python'</span>, <span class="string">'short'</span>, <span class="string">'too'</span>]</span><br></pre></td></tr></table></figure>
<p>不支持单个中文字，中文未分词，所以我们要对中文进行分词处理 </p>
<h4 id="jieba分词处理"><a href="#jieba分词处理" class="headerlink" title="jieba分词处理"></a>jieba分词处理</h4><ul>
<li>jieba.cut()：返回词语组成的生成器</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 需要安装下jieba库</span></span><br><span class="line">pip3 install jieba</span><br></pre></td></tr></table></figure>
<p><strong>案例</strong>：</p>
<ul>
<li>准备句子，利用jieba.cut进行分词</li>
<li>实例化CountVectorizer</li>
<li>将分词结果变成字符串当作fit_transform的输入值</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cut_word</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    对中文进行分词</span></span><br><span class="line"><span class="string">    "我爱北京天安门"————&gt;"我 爱 北京 天安门"</span></span><br><span class="line"><span class="string">    :param text:</span></span><br><span class="line"><span class="string">    :return: text</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 用结巴对中文字符串进行分词</span></span><br><span class="line">    text = <span class="string">" "</span>.join(list(jieba.cut(text)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">text_chinese_count_demo2</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    对中文进行特征抽取</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    data = [<span class="string">"一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。"</span>,</span><br><span class="line">            <span class="string">"我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。"</span>,</span><br><span class="line">            <span class="string">"如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"</span>]</span><br><span class="line">    <span class="comment"># 将原始数据转换成分好词的形式</span></span><br><span class="line">    text_list = []</span><br><span class="line">    <span class="keyword">for</span> sent <span class="keyword">in</span> data:</span><br><span class="line">        text_list.append(cut_word(sent))</span><br><span class="line">    print(text_list)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1、实例化一个转换器类</span></span><br><span class="line">    <span class="comment"># transfer = CountVectorizer(sparse=False)</span></span><br><span class="line">    transfer = CountVectorizer()</span><br><span class="line">    <span class="comment"># 2、调用fit_transform</span></span><br><span class="line">    data = transfer.fit_transform(text_list)</span><br><span class="line">    print(<span class="string">"文本特征抽取的结果：\n"</span>, data.toarray())</span><br><span class="line">    print(<span class="string">"返回特征名字：\n"</span>, transfer.get_feature_names())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<p> 返回结果： </p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">Building prefix dict <span class="keyword">from</span> the <span class="keyword">default</span> <span class="built_in">dictionary</span> ...</span><br><span class="line">Dumping model to file cache /var/folders/mz/tzf2l3sx4rgg6qpglfb035_r0000gn/T/jieba.cache</span><br><span class="line">Loading model cost <span class="number">1.032</span> seconds.</span><br><span class="line">[<span class="string">'一种 还是 一种 今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。'</span>, <span class="string">'我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。'</span>, <span class="string">'如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。'</span>]</span><br><span class="line">Prefix dict has been built succesfully.</span><br><span class="line">文本特征抽取的结果：</span><br><span class="line"> [[<span class="number">2</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">2</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">2</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">3</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">4</span> <span class="number">3</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">2</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]]</span><br><span class="line">返回特征名字：</span><br><span class="line"> [<span class="string">'一种'</span>, <span class="string">'不会'</span>, <span class="string">'不要'</span>, <span class="string">'之前'</span>, <span class="string">'了解'</span>, <span class="string">'事物'</span>, <span class="string">'今天'</span>, <span class="string">'光是在'</span>, <span class="string">'几百万年'</span>, <span class="string">'发出'</span>, <span class="string">'取决于'</span>, <span class="string">'只用'</span>, <span class="string">'后天'</span>, <span class="string">'含义'</span>, <span class="string">'大部分'</span>, <span class="string">'如何'</span>, <span class="string">'如果'</span>, <span class="string">'宇宙'</span>, <span class="string">'我们'</span>, <span class="string">'所以'</span>, <span class="string">'放弃'</span>, <span class="string">'方式'</span>, <span class="string">'明天'</span>, <span class="string">'星系'</span>, <span class="string">'晚上'</span>, <span class="string">'某样'</span>, <span class="string">'残酷'</span>, <span class="string">'每个'</span>, <span class="string">'看到'</span>, <span class="string">'真正'</span>, <span class="string">'秘密'</span>, <span class="string">'绝对'</span>, <span class="string">'美好'</span>, <span class="string">'联系'</span>, <span class="string">'过去'</span>, <span class="string">'还是'</span>, <span class="string">'这样'</span>]</span><br></pre></td></tr></table></figure>
<h4 id="Tf-idf文本特征提取"><a href="#Tf-idf文本特征提取" class="headerlink" title="Tf-idf文本特征提取"></a>Tf-idf文本特征提取</h4><ul>
<li>TF-IDF的主要思想是：如果<strong>某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现</strong>，则认为此词或者短语具有很好的类别区分能力，能代表文章的主题，适合用来分类。</li>
<li><strong>TF-IDF作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。</strong>提取文章的标签（主题）。</li>
</ul>
<h5 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h5><ul>
<li>词频（term frequency，tf）指的是某一个给定的词语在该文件中出现的频率</li>
<li><p>逆向文档频率（inverse document frequency，idf）是一个词语普遍重要性的度量。某一特定词语的idf，可以<strong>由总文件数目除以包含该词语之文件的数目，再将得到的商取以10为底的对数得到</strong></p>
<p><img src="/2020/01/11/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/tfidf%E5%85%AC%E5%BC%8F.png" alt="tfidfå…¬å¼"></p>
<p>最终得出结果可以理解为重要程度。 </p>
</li>
</ul>
<p><strong>tfidf越大越能代表文章的主题</strong></p>
<figure class="highlight lsl"><table><tr><td class="code"><pre><span class="line">举例：</span><br><span class="line">假如一篇文章的总词语数是<span class="number">100</span>个，而词语<span class="string">"非常"</span>出现了<span class="number">5</span>次，那么<span class="string">"非常"</span>一词在该文件中的词频就是<span class="number">5</span>/<span class="number">100</span>=<span class="number">0.05</span>。</span><br><span class="line">而计算文件频率（IDF）的方法是以文件集的文件总数，除以出现<span class="string">"非常"</span>一词的文件数。</span><br><span class="line">所以，如果<span class="string">"非常"</span>一词在<span class="number">1</span>,<span class="number">0000</span>份文件出现过，而文件总数是<span class="number">10</span>,<span class="number">000</span>,<span class="number">000</span>份的话，</span><br><span class="line">其逆向文件频率就是lg（<span class="number">10</span>,<span class="number">000</span>,<span class="number">000</span> / <span class="number">1</span>,<span class="number">0000</span>）=<span class="number">3</span>。</span><br><span class="line">最后<span class="string">"非常"</span>对于这篇文档的tf-idf的分数为<span class="number">0.05</span> * <span class="number">3</span>=<span class="number">0.15</span></span><br></pre></td></tr></table></figure>
<h5 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cut_word</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    对中文进行分词</span></span><br><span class="line"><span class="string">    "我爱北京天安门"————&gt;"我 爱 北京 天安门"</span></span><br><span class="line"><span class="string">    :param text:</span></span><br><span class="line"><span class="string">    :return: text</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 用结巴对中文字符串进行分词</span></span><br><span class="line">    text = <span class="string">" "</span>.join(list(jieba.cut(text)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">text_chinese_tfidf_demo</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    对中文进行特征抽取</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    data = [<span class="string">"一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。"</span>,</span><br><span class="line">            <span class="string">"我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。"</span>,</span><br><span class="line">            <span class="string">"如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"</span>]</span><br><span class="line">    <span class="comment"># 将原始数据转换成分好词的形式</span></span><br><span class="line">    text_list = []</span><br><span class="line">    <span class="keyword">for</span> sent <span class="keyword">in</span> data:</span><br><span class="line">        text_list.append(cut_word(sent))</span><br><span class="line">    print(text_list)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1、实例化一个转换器类</span></span><br><span class="line">    <span class="comment"># transfer = CountVectorizer(sparse=False)</span></span><br><span class="line">    <span class="comment"># 剔除'一种', '不会', '不要'这些词没有说明代表意义，减少计算</span></span><br><span class="line">    transfer = TfidfVectorizer(stop_words=[<span class="string">'一种'</span>, <span class="string">'不会'</span>, <span class="string">'不要'</span>])</span><br><span class="line">    <span class="comment"># 2、调用fit_transform</span></span><br><span class="line">    data = transfer.fit_transform(text_list)</span><br><span class="line">    print(<span class="string">"文本特征抽取的结果：\n"</span>, data.toarray())</span><br><span class="line">    print(<span class="string">"返回特征名字：\n"</span>, transfer.get_feature_names())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure>
<p>结果：</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">Building prefix dict <span class="keyword">from</span> the <span class="keyword">default</span> <span class="built_in">dictionary</span> ...</span><br><span class="line">Loading model <span class="keyword">from</span> cache /var/folders/mz/tzf2l3sx4rgg6qpglfb035_r0000gn/T/jieba.cache</span><br><span class="line">Loading model cost <span class="number">0.856</span> seconds.</span><br><span class="line">Prefix dict has been built succesfully.</span><br><span class="line">[<span class="string">'一种 还是 一种 今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。'</span>, <span class="string">'我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。'</span>, <span class="string">'如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。'</span>]</span><br><span class="line">文本特征抽取的结果：</span><br><span class="line"> [[ <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.43643578</span>  <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span></span><br><span class="line">   <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.21821789</span>  <span class="number">0.</span>          <span class="number">0.21821789</span>  <span class="number">0.</span>          <span class="number">0.</span></span><br><span class="line">   <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.21821789</span>  <span class="number">0.21821789</span>  <span class="number">0.</span>          <span class="number">0.43643578</span></span><br><span class="line">   <span class="number">0.</span>          <span class="number">0.21821789</span>  <span class="number">0.</span>          <span class="number">0.43643578</span>  <span class="number">0.21821789</span>  <span class="number">0.</span>          <span class="number">0.</span></span><br><span class="line">   <span class="number">0.</span>          <span class="number">0.21821789</span>  <span class="number">0.21821789</span>  <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.21821789</span></span><br><span class="line">   <span class="number">0.</span>        ]</span><br><span class="line"> [ <span class="number">0.2410822</span>   <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.2410822</span>   <span class="number">0.2410822</span></span><br><span class="line">   <span class="number">0.2410822</span>   <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span></span><br><span class="line">   <span class="number">0.</span>          <span class="number">0.2410822</span>   <span class="number">0.55004769</span>  <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span></span><br><span class="line">   <span class="number">0.2410822</span>   <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.48216441</span></span><br><span class="line">   <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.2410822</span></span><br><span class="line">   <span class="number">0.</span>          <span class="number">0.2410822</span> ]</span><br><span class="line"> [ <span class="number">0.</span>          <span class="number">0.644003</span>    <span class="number">0.48300225</span>  <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span></span><br><span class="line">   <span class="number">0.16100075</span>  <span class="number">0.16100075</span>  <span class="number">0.</span>          <span class="number">0.16100075</span>  <span class="number">0.</span>          <span class="number">0.16100075</span></span><br><span class="line">   <span class="number">0.16100075</span>  <span class="number">0.</span>          <span class="number">0.12244522</span>  <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.16100075</span></span><br><span class="line">   <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.16100075</span>  <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span></span><br><span class="line">   <span class="number">0.3220015</span>   <span class="number">0.16100075</span>  <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.16100075</span>  <span class="number">0.</span>          <span class="number">0.</span></span><br><span class="line">   <span class="number">0.</span>        ]]</span><br><span class="line">返回特征名字：</span><br><span class="line">[<span class="string">'之前'</span>, <span class="string">'了解'</span>, <span class="string">'事物'</span>, <span class="string">'今天'</span>, <span class="string">'光是在'</span>, <span class="string">'几百万年'</span>, <span class="string">'发出'</span>, <span class="string">'取决于'</span>, <span class="string">'只用'</span>, <span class="string">'后天'</span>, <span class="string">'含义'</span>, <span class="string">'大部分'</span>, <span class="string">'如何'</span>, <span class="string">'如果'</span>, <span class="string">'宇宙'</span>, <span class="string">'我们'</span>, <span class="string">'所以'</span>, <span class="string">'放弃'</span>, <span class="string">'方式'</span>, <span class="string">'明天'</span>, <span class="string">'星系'</span>, <span class="string">'晚上'</span>, <span class="string">'某样'</span>, <span class="string">'残酷'</span>, <span class="string">'每个'</span>, <span class="string">'看到'</span>, <span class="string">'真正'</span>, <span class="string">'秘密'</span>, <span class="string">'绝对'</span>, <span class="string">'美好'</span>, <span class="string">'联系'</span>, <span class="string">'过去'</span>, <span class="string">'还是'</span>, <span class="string">'这样'</span>]</span><br></pre></td></tr></table></figure>
<h5 id="Tf-idf的重要性"><a href="#Tf-idf的重要性" class="headerlink" title="Tf-idf的重要性"></a>Tf-idf的重要性</h5><p> <strong>分类机器学习算法进行文章分类中前期数据处理方式</strong> </p>
<p>举例：</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">文章库中  <span class="number">1000</span>篇文章</span><br><span class="line">的  在 <span class="number">1000</span> 篇文章中都出现过</span><br><span class="line">python  在<span class="number">100</span> 文章中出现过</span><br><span class="line">java       在 <span class="number">100</span> 文章中出现过</span><br><span class="line"></span><br><span class="line">idf(的)  = lg(<span class="number">1000</span>/<span class="number">1000</span>)  = <span class="number">0</span></span><br><span class="line">idf(python) = lg(<span class="number">1000</span>/<span class="number">100</span>) = <span class="number">1</span></span><br><span class="line">idf(java) = lg(<span class="number">1000</span>/<span class="number">100</span>) = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">文章<span class="number">1</span> python  出现了 <span class="number">100</span>次，java也出现了<span class="number">10</span>次， 文章<span class="number">1</span>中一共有一千个词</span><br><span class="line">文章<span class="number">2</span> python  出现了 <span class="number">10</span>次，java也出现了<span class="number">100</span>次， 文章<span class="number">2</span>中一共有一千个词</span><br><span class="line"></span><br><span class="line">tf(python , 文章<span class="number">1</span>) = <span class="number">100</span> /<span class="number">1000</span> = <span class="number">0.1</span></span><br><span class="line">tf(java, 文章<span class="number">1</span>) = <span class="number">10</span>/<span class="number">1000</span> = <span class="number">0.01</span></span><br><span class="line">tf(python , 文章<span class="number">2</span>) = <span class="number">10</span> /<span class="number">1000</span> = <span class="number">00.1</span></span><br><span class="line">tf(java, 文章<span class="number">2</span>) = <span class="number">100</span>/<span class="number">1000</span> = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">tfidf(python , 文章<span class="number">1</span>) = <span class="number">0.1</span> * <span class="number">1</span>  = <span class="number">0.1</span></span><br><span class="line">tfidf(java, 文章<span class="number">1</span>) = <span class="number">0.01</span></span><br><span class="line">tfidf(的, 文章<span class="number">1</span>) = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">tfidf(python , 文章<span class="number">2</span>)  = <span class="number">00.1</span></span><br><span class="line">tfidf(java, 文章<span class="number">2</span>)  = <span class="number">0.1</span></span><br><span class="line">tfidf(的, 文章<span class="number">2</span>) = <span class="number">0</span></span><br><span class="line">相对而言，Java代表文章<span class="number">2</span>的主题，python代表文章<span class="number">1</span>的主题</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>特征工程</tag>
        <tag>特征提取</tag>
        <tag>jieba分词处理</tag>
        <tag>词袋模型</tag>
      </tags>
  </entry>
  <entry>
    <title>决策树算法案例</title>
    <url>/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E6%A1%88%E4%BE%8B/</url>
    <content><![CDATA[<h4 id="决策树算法API"><a href="#决策树算法API" class="headerlink" title="决策树算法API"></a>决策树算法API</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">sklearn</span>.<span class="title">tree</span>.<span class="title">DecisionTreeClassifier</span><span class="params">(criterion=’gini’, max_depth=None,random_state=None)</span></span></span><br></pre></td></tr></table></figure>
<a id="more"></a>
<ul>
<li>criterion</li>
<li><p>特征选择标准</p>
<ul>
<li>“gini”或者”entropy”，前者代表基尼系数，后者代表信息增益。一默认”gini”，即CART算法。</li>
</ul>
</li>
<li><p>min_samples_split</p>
</li>
<li><p>内部节点再划分所需最小样本数</p>
<ul>
<li>这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。我之前的一个项目例子，有大概10万样本，建立决策树时，我选择了min_samples_split=10。可以作为参考。</li>
</ul>
</li>
<li><p>min_samples_leaf</p>
</li>
<li><p>叶子节点最少样本数</p>
<ul>
<li>这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1，可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。之前的10万样本项目使用min_samples_leaf的值为5，仅供参考。</li>
</ul>
</li>
<li><p>max_depth</p>
</li>
<li><p>决策树最大深度</p>
<ul>
<li>决策树的最大深度，默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间</li>
</ul>
</li>
<li><p>random_state</p>
</li>
<li>随机数种子</li>
</ul>
<h4 id="案例-泰坦尼克号乘客生存预测"><a href="#案例-泰坦尼克号乘客生存预测" class="headerlink" title="案例-泰坦尼克号乘客生存预测"></a>案例-泰坦尼克号乘客生存预测</h4><h5 id="案例背景"><a href="#案例背景" class="headerlink" title="案例背景"></a>案例背景</h5><p>泰坦尼克号沉没是历史上最臭名昭着的沉船之一。1912年4月15日，在她的处女航中，泰坦尼克号在与冰山相撞后沉没，在2224名乘客和机组人员中造成1502人死亡。这场耸人听闻的悲剧震惊了国际社会，并为船舶制定了更好的安全规定。 造成海难失事的原因之一是乘客和机组人员没有足够的救生艇。尽管幸存下沉有一些运气因素，但有些人比其他人更容易生存，例如妇女，儿童和上流社会。 在这个案例中，我们要求您完成对哪些人可能存活的分析。特别是，我们要求您运用机器学习工具来预测哪些乘客幸免于悲剧。</p>
<p>案例：<a href="https://www.kaggle.com/c/titanic/overview" target="_blank" rel="noopener">https://www.kaggle.com/c/titanic/overview</a></p>
<p>我们提取到的数据集中的特征包括票的类别，是否存活，乘坐班次，年龄，登陆home.dest，房间，船和性别等。</p>
<p>数据：<a href="http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt" target="_blank" rel="noopener">http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt</a></p>
<p>经过观察数据得到:</p>
<ul>
<li><strong>1 乘坐班是指乘客班（1，2，3），是社会经济阶层的代表。</strong></li>
<li><strong>2 其中age数据存在缺失。</strong></li>
</ul>
<h5 id="步骤分析"><a href="#步骤分析" class="headerlink" title="步骤分析"></a>步骤分析</h5><ul>
<li><p>1.获取数据</p>
</li>
<li><p>2.数据基本处理</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="number">2.1</span> 确定特征值,目标值</span><br><span class="line"><span class="number">2.2</span> 缺失值处理</span><br><span class="line"><span class="number">2.3</span> 数据集划分</span><br></pre></td></tr></table></figure>
</li>
<li><p>3.特征工程(字典特征抽取)</p>
</li>
<li><p>4.机器学习(决策树)</p>
</li>
<li><p>5.模型评估</p>
</li>
</ul>
<h5 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入模块</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier, export_graphviz</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.获取数据</span></span><br><span class="line">taitan = pd.read_csv(<span class="string">"http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt"</span>)</span><br><span class="line">taitan.describe()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.数据基本处理</span></span><br><span class="line"><span class="comment"># 2.1 确定特征值,目标值</span></span><br><span class="line">x = taitan[[<span class="string">"pclass"</span>, <span class="string">"age"</span>, <span class="string">"sex"</span>]]</span><br><span class="line">y = taitan[<span class="string">"survived"</span>]</span><br><span class="line"><span class="comment"># 2.2 缺失值处理</span></span><br><span class="line"><span class="comment"># inplace:True:会修改原数据，False:不替换修改原数据，生成新的对象</span></span><br><span class="line">x[<span class="string">'age'</span>].fillna(x[<span class="string">'age'</span>].mean(), inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 2.3 数据集划分</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=<span class="number">22</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.特征工程(字典特征抽取)</span></span><br><span class="line"><span class="comment"># 特征中出现类别符号，需要进行one-hot编码处理(DictVectorizer)</span></span><br><span class="line"><span class="comment"># x.to_dict(orient="records") 需要将数组特征转换成字典数据</span></span><br><span class="line"><span class="comment"># 对于x转换成字典数据x.to_dict(orient="records")</span></span><br><span class="line">transfer = DictVectorizer(sparse=<span class="literal">False</span>)</span><br><span class="line">x_train = transfer.fit_transform(x_train.to_dict(orient=<span class="string">"records"</span>))</span><br><span class="line">x_test = transfer.fit_transform(x_test.to_dict(orient=<span class="string">"records"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.机器学习(决策树)</span></span><br><span class="line">estimator = DecisionTreeClassifier(criterion=<span class="string">"entropy"</span>, max_depth=<span class="number">5</span>)</span><br><span class="line">estimator.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.模型评估</span></span><br><span class="line"><span class="comment"># 精确率</span></span><br><span class="line">estimator.score(x_test, y_test)</span><br><span class="line"><span class="comment"># 预测值</span></span><br><span class="line">estimator.predict(x_test)</span><br></pre></td></tr></table></figure>
<h4 id="决策树可视化"><a href="#决策树可视化" class="headerlink" title="决策树可视化"></a>决策树可视化</h4><h5 id="保存树的结构到dot文件"><a href="#保存树的结构到dot文件" class="headerlink" title="保存树的结构到dot文件"></a>保存树的结构到dot文件</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># sklearn.tree.export_graphviz() 该函数能够导出DOT格式</span></span><br><span class="line"><span class="comment"># tree.export_graphviz(estimator,out_file='tree.dot’,feature_names=[‘’,’’])</span></span><br><span class="line"></span><br><span class="line">export_graphviz(estimator, out_file=<span class="string">"./data/tree.dot"</span>, feature_names=[<span class="string">'age'</span>, <span class="string">'pclass=1st'</span>, <span class="string">'pclass=2nd'</span>, <span class="string">'pclass=3rd'</span>, <span class="string">'女性'</span>, <span class="string">'男性'</span>])</span><br></pre></td></tr></table></figure>
<h5 id="网站显示结构"><a href="#网站显示结构" class="headerlink" title="网站显示结构"></a>网站显示结构</h5><figure class="highlight dts"><table><tr><td class="code"><pre><span class="line"><span class="symbol">http:</span><span class="comment">//webgraphviz.com/</span></span><br></pre></td></tr></table></figure>
<h5 id="结果显示"><a href="#结果显示" class="headerlink" title="结果显示"></a>结果显示</h5><p><img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E6%A1%88%E4%BE%8B/%E6%A0%91%E7%BB%93%E6%9E%84.png" alt="img"> </p>
<h5 id="决策树优缺点"><a href="#决策树优缺点" class="headerlink" title="决策树优缺点"></a>决策树优缺点</h5><ul>
<li>优点：简单的理解和解释，树木可视化。</li>
<li>缺点：<strong>决策树学习者可以创建不能很好地推广数据的过于复杂的树，容易发生过拟合。</strong></li>
<li>改进：<ul>
<li>剪枝cart算法</li>
<li><strong>随机森林</strong>（集成学习的一种）</li>
</ul>
</li>
</ul>
<p><strong>注：企业重要决策，由于决策树很好的分析能力，在决策过程应用较多， 可以选择特征</strong></p>
]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>机器学习算法</tag>
        <tag>机器学习</tag>
        <tag>决策树可视化</tag>
        <tag>监督学习算法</tag>
      </tags>
  </entry>
  <entry>
    <title>决策树算法</title>
    <url>/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/</url>
    <content><![CDATA[<h4 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h4><p>决策树思想的来源非常朴素，程序设计中的条件分支结构就是if-else结构，最早的决策树就是利用这类结构分割数据的一种分类学习方法。</p>
<p><strong>决策树：是一种树形结构，其中每个内部节点表示一个属性上的判断，每个分支代表一个判断结果的输出，最后每个叶节点代表一种分类结果，本质是一颗由多个判断节点组成的树</strong>。</p>
<a id="more"></a>
<h4 id="熵的概念"><a href="#熵的概念" class="headerlink" title="熵的概念"></a>熵的概念</h4><p> 物理学上，<strong>熵 Entropy</strong> 是“混乱”程度的量度。 </p>
<p> <img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/%E7%86%B5%E7%9A%84%E4%BB%8B%E7%BB%8D.png" alt="img"> </p>
<p> <strong>系统越有序，熵值越低；系统越混乱或者分散，熵值越高</strong>。 </p>
<ul>
<li><h5 id="信息理论"><a href="#信息理论" class="headerlink" title="信息理论"></a>信息理论</h5><ol>
<li><p><strong>从信息的完整性上进行的描述：</strong></p>
<p> 当<strong>系统的有序状态一致时</strong>，数据越集中的地方熵值越小，数据越分散的地方熵值越大。 </p>
</li>
<li><p><strong>从信息的有序性上进行的描述：</strong></p>
<p>  当<strong>数据量一致时</strong>，<strong>系统越有序，熵值越低；系统越混乱或者分散，熵值越高</strong>。 </p>
</li>
</ol>
</li>
</ul>
<p>“<strong>信息熵</strong>“ (information entropy)是度量样本集合纯度最常用的一种指标。</p>
<p>假定当前样本集合 D 中第 k 类样本所占的比例为$p_k(k = 1, 2,. . . , |y|)$ ，</p>
<p>$p_k=\frac{C^k}{D}$, D为样本的所有数量，$C^k$为第$k$类样本的数量。</p>
<p>则 D的信息熵定义为(（log是以2为底）:</p>
<p> <img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/%E5%85%AC%E5%BC%8F1.png" alt="image-20190701180157939"> </p>
<p> 其中：Ent(D) 的值越小，则 D 的纯度越高. </p>
<ul>
<li><h5 id="举例说明"><a href="#举例说明" class="headerlink" title="举例说明"></a>举例说明</h5></li>
</ul>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">假设我们没有看世界杯的比赛，但是想知道哪支球队会是冠军，</span><br><span class="line">我们只能猜测某支球队是或不是冠军，然后观众用对或不对来回答，</span><br><span class="line">我们想要猜测次数尽可能少，你会用什么方法？</span><br><span class="line"></span><br><span class="line">答案：</span><br><span class="line">二分法：</span><br><span class="line">假如有 <span class="number">16</span> 支球队，分别编号，先问是否在 <span class="number">1</span><span class="number">-8</span> 之间，如果是就继续问是否在 <span class="number">1</span><span class="number">-4</span> 之间，</span><br><span class="line">以此类推，直到最后判断出冠军球队是哪支。</span><br><span class="line">如果球队数量是 <span class="number">16</span>，我们需要问 <span class="number">4</span> 次来得到最后的答案。那么世界冠军这条消息的信息熵就是 <span class="number">4</span>。</span><br><span class="line"></span><br><span class="line">那么信息熵等于<span class="number">4</span>，是如何进行计算的呢？</span><br><span class="line">Ent(D) = -（p1 * logp1 + p2 * logp2 + ... + p16 * logp16），</span><br><span class="line">其中 p1, ..., p16 分别是这 <span class="number">16</span> 支球队夺冠的概率。</span><br><span class="line">当每支球队夺冠概率相等都是 <span class="number">1</span>/<span class="number">16</span> 的时：Ent(D) = -（<span class="number">16</span> * <span class="number">1</span>/<span class="number">16</span> * log1/<span class="number">16</span>） = <span class="number">4</span></span><br><span class="line">每个事件概率相同时，熵最大，这件事越不确定。</span><br><span class="line"></span><br><span class="line">篮球比赛里，有<span class="number">4</span>个球队 &#123;A,B,C,D&#125; ，获胜概率分别为&#123;<span class="number">1</span>/<span class="number">2</span>, <span class="number">1</span>/<span class="number">4</span>, <span class="number">1</span>/<span class="number">8</span>, <span class="number">1</span>/<span class="number">8</span>&#125;，求Ent(D)</span><br><span class="line">Ent(D)=<span class="number">7</span>/<span class="number">4</span></span><br></pre></td></tr></table></figure>
<h4 id="决策树的划分依据"><a href="#决策树的划分依据" class="headerlink" title="决策树的划分依据"></a>决策树的划分依据</h4><h5 id="1-信息增益"><a href="#1-信息增益" class="headerlink" title="1.信息增益"></a>1.信息增益</h5><p> <strong>信息增益：</strong>以某特征划分数据集前后的熵的差值。熵可以表示样本集合的不确定性，熵越大，样本的不确定性就越大。因此可以<strong>使用划分前后集合熵的差值来衡量使用当前特征对于样本集合D划分效果的好坏</strong>。 </p>
<p> <strong>信息增益 = entroy(前) - entroy(后)</strong> </p>
<figure class="highlight tp"><table><tr><td class="code"><pre><span class="line">注意：信息增益表示得知特征<span class="keyword">X</span>的信息而使得类<span class="keyword">Y</span>的信息熵减少的程度</span><br></pre></td></tr></table></figure>
<ul>
<li><p>定义与公式</p>
<p> 假定离散属性a有 V 个可能的取值：</p>
<p> <img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/%E5%8F%96%E5%80%BC.png" alt="image-20190712164631503"> </p>
<p>若使用a来对样本集 $D$ 进行划分，则会产生 V 个分支结点，其中第$v$个分支结点包含了 $D$ 中所有在属性$a$上取值为$a^v$的样本，记为$D^v$. 我们可根据前面给出的信息熵公式计算出$D^v$的信息熵，再考虑到不同的分支结点所包含的样本数不同，给分支结点赋予权重$\frac{|D^v|}{|D|}$</p>
<p>即样本数越多的分支结点的影响越大，于是可计算出用属性a对样本集 D 进行划分所获得的”信息增益” (information gain)</p>
<p>其中：</p>
<p>特征$a$对训练数据集D的信息增益$Gain(D,a)$，定义为<strong>集合D的信息熵$Ent(D)$</strong>与<strong>给定特征$a$条件下$D$的信息条件熵$Ent(D|a)$</strong>之差，即公式为：</p>
<p> <img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/%E5%85%AC%E5%BC%8F2.png" alt="image-20190701180230634"></p>
<p>  信息熵的计算： </p>
<p> <img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/%E5%85%AC%E5%BC%8F3.png" alt="image-20190701180248293"> </p>
<p> 条件熵的计算： </p>
<p> <img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/%E5%85%AC%E5%BC%8F4.png" alt="image-20190701180307869"></p>
<p>其中：</p>
<p>$D^v$ 表示$a$属性中第$v$个分支节点包含的样本数</p>
<p>$C^{kv}$ 表示$a$属性中第$v$个分支节点包含的样本数中，第$k$个类别下包含的样本数</p>
<p>一般而言，信息增益越大，则意味着<strong>使用属性 $a$ 来进行划分所获得的”纯度提升”越大</strong>。因此，我们可用信息增益来进行决策树的划分属性选择，著名的 <strong>ID3 决策树学习算法</strong> [Quinlan， 1986] 就是以<strong>信息增益为准则</strong>来选择划分属性。 </p>
</li>
<li><p>举例说明</p>
<p>如下图，第一列为论坛号码，第二列为性别，第三列为活跃度，最后一列用户是否流失。</p>
<p>我们要解决一个问题：<strong>性别和活跃度两个特征，哪个对用户流失影响更大</strong>？</p>
<p> <img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/entropy_example1.png" alt="image-20190214123146695"> </p>
<p>通过计算信息增益可以解决这个问题，统计上右表信息</p>
<p>其中Positive为正样本（已流失），Negative为负样本（未流失），下面的数值为不同划分下对应的人数。</p>
<p>可得到三个熵：</p>
<p><strong>a.计算类别信息熵</strong></p>
<p>整体熵：</p>
<p> <img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/%E5%85%AC%E5%BC%8F5.png" alt="image-20190701180330245"> </p>
<p> <strong>b.计算性别属性的信息熵(a=”性别”)</strong> </p>
<p> <img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/%E6%80%A7%E5%88%AB%E4%BF%A1%E6%81%AF%E7%86%B5.png" alt="image-20190701175350303"> </p>
<p> <strong>c.计算性别的信息增益(a=”性别”)</strong> </p>
<p> <img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A1.png" alt="image-20190701173018397"> </p>
<p> <strong>b.计算活跃度属性的信息熵(a=”活跃度”)</strong> </p>
<p> <img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/%E6%B4%BB%E8%B7%83%E4%BF%A1%E6%81%AF%E7%86%B5.png" alt="image-20190701175748466"> </p>
<p> <strong>c.计算活跃度的信息增益(a=”活跃度”)</strong> </p>
<p> <img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A2.png" alt="image-20190701173116116"> </p>
<p> <strong>活跃度的信息增益比性别的信息增益大，也就是说，活跃度对用户流失的影响比性别大。</strong>在做特征选择或者数据分析的时候，我们应该重点考察活跃度这个指标。 </p>
</li>
</ul>
<h5 id="2-信息增益率"><a href="#2-信息增益率" class="headerlink" title="2.信息增益率"></a>2.信息增益率</h5><p> 在上面的介绍中，我们有意忽略了”编号”这一列.若把”编号”也作为一个候选划分属性，则根据信息增益公式可计算出它的信息增益为 0.9182，远大于其他候选划分属性。 </p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">计算每个属性的信息熵过程中,我们发现,该属性的值为<span class="number">0</span>, 也就是其信息增益为<span class="number">0.9182</span>. 但是很明显这么分类,最后出现的结果不具有泛化效果.无法对新样本进行有效预测.</span><br></pre></td></tr></table></figure>
<p> 实际上，<strong>信息增益准则对可取值数目较多的属性有所偏好</strong>，为减少这种偏好可能带来的不利影响，著名的 <strong>C4.5 决策树算法 [Quinlan， 1993J 不直接使用信息增益，而是使用”增益率” (gain ratio) 来选择最优划分属性</strong>。</p>
<p> <strong>增益率：</strong>增益率是用前面的信息增益$Gain(D, a)$和属性$a$对应的”固有值”(intrinsic value) [Quinlan , 1993J的比值来共同定义的。 </p>
<p> <img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/%E5%85%AC%E5%BC%8F6.png" alt="image-20190701180359267"></p>
<figure class="highlight gcode"><table><tr><td class="code"><pre><span class="line">属性 a 的可能取值数目越多<span class="comment">(即 V 越大)</span>，则 IV<span class="comment">(a)</span> 的值通常会越大.</span><br></pre></td></tr></table></figure>
<p> <strong>上个案例中</strong></p>
<figure class="highlight gcode"><table><tr><td class="code"><pre><span class="line">a.计算类别信息熵</span><br><span class="line">b.计算性别属性的信息熵<span class="comment">(性别、活跃度)</span></span><br><span class="line">c.计算活跃度的信息增益<span class="comment">(性别、活跃度)</span></span><br></pre></td></tr></table></figure>
<p> <strong>d.计算属性分裂信息度量</strong> </p>
<p> 用分裂信息度量来考虑某种属性进行分裂时分支的数量信息和尺寸信息，我们把这些信息称为属性的内在信息（instrisic information）。信息增益率用信息增益/内在信息，会导致属性的重要性随着内在信息的增大而减小<strong>（也就是说，如果这个属性本身不确定性就很大，那我就越不倾向于选取它）</strong>，这样算是对单纯用信息增益有所补偿。 </p>
<p> <img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A3.png" alt="image-20190701173713129"> </p>
<p> <strong>e.计算信息增益率</strong> </p>
<p> <img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A5.png" alt="image-20190701173741987"> </p>
<p>活跃度的信息增益率更高一些，所以在构建决策树的时候，优先选择</p>
<p>通过这种方式，在选取节点的过程中，我们可以降低取值较多的属性的选取偏好。</p>
<p><strong>案例二</strong></p>
<p>如下图，第一列为天气，第二列为温度，第三列为湿度，第四列为风速，最后一列该活动是否进行。</p>
<p>我们要解决：<strong>根据下面表格数据，判断在对应天气下，活动是否会进行</strong>？</p>
<p> <img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/image-20190520110721291.png" alt="image-20190520110721291"> </p>
<p> <img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/image-20190522125059373.png" alt="image-20190522125059373"> </p>
<p> 该数据集有四个属性，属性集合A={ 天气，温度，湿度，风速}， 类别标签有两个，类别集合L={进行，取消}。 </p>
<p> <strong>a.计算类别信息熵</strong> </p>
<p> 类别信息熵表示的是所有样本中各种类别出现的不确定性之和。根据熵的概念，熵越大，不确定性就越大，把事情搞清楚所需要的信息量就越多。 </p>
<script type="math/tex; mode=display">Ent(D)=-\frac{9}{14}log_2\frac{9}{14}-\frac{5}{14}log_2\frac{5}{14}=0.940</script><p><strong>b.计算每个属性的信息熵</strong></p>
<p>每个属性的信息熵相当于一种条件熵。他表示的是在某种属性的条件下，各种类别出现的不确定性之和。属性的信息熵越大，表示这个属性中拥有的样本类别越不“纯”。 <img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/%E5%85%AC%E5%BC%8F7.png" alt="image-20190701180511460"> </p>
<p><strong>c.计算信息增益</strong></p>
<p>信息增益的 = 熵 - 条件熵，在这里就是 类别信息熵 - 属性信息熵，它表示的是信息不确定性减少的程度。如果一个属性的信息增益越大，就表示用这个属性进行样本划分可以更好的减少划分后样本的不确定性，当然，选择该属性就可以更快更好地完成我们的分类目标。</p>
<p> <strong>信息增益就是ID3算法的特征选择指标。</strong></p>
<p>  <img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/%E7%BB%BC%E5%90%88%E5%9F%BA%E5%B0%BC%E5%A2%9E%E7%9B%8A.png" alt="image-20190701175852913"> </p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">假设我们把上面表格<span class="number">1</span>的数据前面添加一列为<span class="string">"编号"</span>,取值(<span class="number">1</span>-<span class="number">-14</span>). 若把<span class="string">"编号"</span>也作为一个候选划分属性,则根据前面步骤: 计算每个属性的信息熵过程中,我们发现,该属性的值为<span class="number">0</span>, 也就是其信息增益为<span class="number">0.940</span>. 但是很明显这么分类,最后出现的结果不具有泛化效果.此时根据信息增益就无法选择出有效分类特征。所以，C4<span class="number">.5</span>选择使用信息增益率对ID3进行改进。</span><br></pre></td></tr></table></figure>
<p> <strong>d.计算属性分裂信息度量</strong></p>
<p>  用分裂信息度量来考虑某种属性进行分裂时分支的数量信息和尺寸信息，我们把这些信息称为属性的内在信息（instrisic information）。信息增益率用信息增益/内在信息，会导致属性的重要性随着内在信息的增大而减小<strong>（也就是说，如果这个属性本身不确定性就很大，那我就越不倾向于选取它）</strong>，这样算是对单纯用信息增益有所补偿。 </p>
<p> <img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E7%8E%871.png" alt="image-20190701173339182"> </p>
<p> <strong>e.计算信息增益率</strong> </p>
<p> <img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E7%8E%872.png" alt="image-20190701173403157"> </p>
<p> 天气的信息增益率最高，选择天气为分裂属性。发现分裂了之后，天气是“阴”的条件下，类别是”纯“的，所以把它定义为叶子节点，选择不“纯”的结点继续分裂。 </p>
<p> <img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/image-20190520121422401.png" alt="image-20190520121422401"> </p>
<p>在子结点当中重复过程1~5，直到所有的叶子结点足够”纯”。</p>
<p>现在我们来总结一下C4.5的算法流程</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span>(当前节点<span class="string">"不纯"</span>)：</span><br><span class="line">    <span class="number">1.</span>计算当前节点的类别熵(以类别取值计算)</span><br><span class="line">    <span class="number">2.</span>计算当前阶段的属性熵(按照属性取值吓得类别取值计算)</span><br><span class="line">    <span class="number">3.</span>计算信息增益</span><br><span class="line">    <span class="number">4.</span>计算各个属性的分裂信息度量</span><br><span class="line">    <span class="number">5.</span>计算各个属性的信息增益率</span><br><span class="line">end <span class="keyword">while</span></span><br><span class="line">当前阶段设置为叶子节点</span><br></pre></td></tr></table></figure>
<h5 id="3-基尼值和基尼指数"><a href="#3-基尼值和基尼指数" class="headerlink" title="3.基尼值和基尼指数"></a>3.基尼值和基尼指数</h5><p> CART 决策树 [Breiman et al., 1984] 使用”基尼指数” (Gini index)来选择划分属性。</p>
<figure class="highlight armasm"><table><tr><td class="code"><pre><span class="line"><span class="symbol">CART</span> 是Classification <span class="keyword">and </span>Regression Tree的简称，这是一种著名的决策树学习算法,分类和回归任务都可用。</span><br></pre></td></tr></table></figure>
<p> <strong>基尼值Gini（D）：</strong>从数据集D中随机抽取两个样本，其类别标记不一致的概率。<strong>故，Gini（D）值越小，数据集D的纯度越高。</strong> </p>
<p> 数据集 D 的纯度可用基尼值来度量: </p>
<p> <img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/%E5%85%AC%E5%BC%8F9.png" alt="image-20190701180551894"> </p>
<p> $*p_k=\frac{C^k}{D}$, D为样本的所有数量，$C^k$为第k类样本的数量。 </p>
<p> <strong>基尼指数Gini_index（D）：</strong>一般，选择使划分后基尼系数最小的属性作为最优化分属性。 </p>
<p> <img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/%E5%85%AC%E5%BC%8F11.png" alt="image-20190701180610582"> </p>
<p><strong>案例</strong></p>
<p> 请根据下图列表，按照基尼指数的划分依据，做出决策树。 </p>
<div class="table-container">
<table>
<thead>
<tr>
<th>序号</th>
<th>是否有房</th>
<th>婚姻状况</th>
<th>年收入</th>
<th>是否拖欠贷款</th>
</tr>
</thead>
<tbody>
<tr>
<td>1</td>
<td>yes</td>
<td>single</td>
<td>125k</td>
<td>no</td>
</tr>
<tr>
<td>2</td>
<td>no</td>
<td>married</td>
<td>100k</td>
<td>no</td>
</tr>
<tr>
<td>3</td>
<td>no</td>
<td>single</td>
<td>70k</td>
<td>no</td>
</tr>
<tr>
<td>4</td>
<td>yes</td>
<td>married</td>
<td>120k</td>
<td>no</td>
</tr>
<tr>
<td>5</td>
<td>no</td>
<td>divorced</td>
<td>95k</td>
<td>yes</td>
</tr>
<tr>
<td>6</td>
<td>no</td>
<td>married</td>
<td>60k</td>
<td>no</td>
</tr>
<tr>
<td>7</td>
<td>yes</td>
<td>divorced</td>
<td>220k</td>
<td>no</td>
</tr>
<tr>
<td>8</td>
<td>no</td>
<td>single</td>
<td>85k</td>
<td>yes</td>
</tr>
<tr>
<td>9</td>
<td>no</td>
<td>married</td>
<td>75k</td>
<td>no</td>
</tr>
<tr>
<td>10</td>
<td>No</td>
<td>Single</td>
<td>90k</td>
<td>Yes</td>
</tr>
</tbody>
</table>
</div>
<ol>
<li><p>对数据集非序列标号属性{是否有房，婚姻状况，年收入}分别计算它们的Gini指数，<strong>取Gini指数最小的属性作为决策树的根节点属性。</strong> </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">第一次大循环</span><br></pre></td></tr></table></figure>
</li>
<li><p>根节点的Gini值为： </p>
<p><img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/%E5%85%AC%E5%BC%8F12.png" alt="image-20190701180659409"> </p>
</li>
<li><p>当根据是否有房来进行划分时，Gini指数计算过程为： </p>
<p><img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E7%8E%874.png" alt="image-20190701173439230"></p>
<p><img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/gain_example4.png" alt="image-20190214131000602">  </p>
</li>
<li><p>若按婚姻状况属性来划分，属性婚姻状况有三个可能的取值{married，single，divorced}，分别计算划分后的Gini系数增益。 </p>
<p><img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/%E5%85%AC%E5%BC%8F13.png" alt="image-20190701180730086"></p>
<p>对比计算结果，根据婚姻状况属性来划分根节点时取Gini指数最小的分组作为划分结果，即:</p>
<p>{married} | {single,divorced} </p>
</li>
<li><p>同理可得年收入Gini： </p>
<p>对于年收入属性为数值型属性，首先需要对数据按升序排序，然后从小到大依次用相邻值的中间值作为分隔将样本划分为两组。例如当面对年收入为60和70这两个值时，我们算得其中间值为65。以中间值65作为分割点求出Gini指数。 </p>
<p><img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/image-20190521160430905.png" alt="image-20190521160430905"> </p>
<p><img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/%E5%85%AC%E5%BC%8F14.png" alt="image-20190701180802159"> </p>
<p>根据计算知道，三个属性划分根节点的指数最小的有两个：年收入属性和婚姻状况，他们的指数都为0.3。此时，选取首先出现的属性【married】作为第一次划分。 </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">第二次大循环</span><br></pre></td></tr></table></figure>
</li>
<li><p>接下来，采用同样的方法，分别计算剩下属性，其中根节点的Gini系数为（此时是否拖欠贷款的各有3个records） </p>
<p><img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/%E5%85%AC%E5%BC%8F15.png" alt="image-20190701180829379"> </p>
</li>
<li><p>对于是否有房属性，可得： </p>
<p><img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/%E5%85%AC%E5%BC%8F16.png" alt="image-20190701180847171"> </p>
</li>
<li><p>对于年收入属性则有： </p>
<p><img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/image-20190521171602023.png" alt="image-20190521171602023"> </p>
<p>经过如上流程，构建的决策树，如下图： </p>
<p><img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/gini_result.png" alt="image-20190214132127391"></p>
</li>
</ol>
<p>总结一下CART的算法流程 </p>
<figure class="highlight ada"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span>(当前节点<span class="string">"不纯"</span>)：</span><br><span class="line">    <span class="number">1</span>.遍历每个变量的每一种分割方式，找到最好的分割点</span><br><span class="line">    <span class="number">2</span>.分割成两个节点N1和N2</span><br><span class="line"><span class="keyword">end</span> <span class="keyword">while</span></span><br><span class="line">每个节点足够“纯”为止</span><br></pre></td></tr></table></figure>
<h4 id="常见决策树的启发函数比较"><a href="#常见决策树的启发函数比较" class="headerlink" title="常见决策树的启发函数比较"></a>常见决策树的启发函数比较</h4><p> <img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/%E5%85%AC%E5%BC%8F17.png" alt="image-20190701180919817"></p>
<div class="table-container">
<table>
<thead>
<tr>
<th>名称</th>
<th>提出时间</th>
<th>分支方式</th>
<th>备注</th>
</tr>
</thead>
<tbody>
<tr>
<td>ID3</td>
<td>1975</td>
<td>信息增益</td>
<td>ID3只能对离散属性的数据集构成决策树</td>
</tr>
<tr>
<td>C4.5</td>
<td>1993</td>
<td>信息增益率</td>
<td>优化后解决了ID3分支过程中总喜欢偏向选择值较多的 属性</td>
</tr>
<tr>
<td>CART</td>
<td>1984</td>
<td>Gini系数</td>
<td>可以进行分类和回归，可以处理离散属性，也可以处理连续属性</td>
</tr>
</tbody>
</table>
</div>
<h5 id="ID3-算法"><a href="#ID3-算法" class="headerlink" title="ID3 算法"></a>ID3 算法</h5><p><strong>存在的缺点</strong></p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"> (<span class="number">1</span>) ID3算法在选择根节点和各内部节点中的分支属性时，采用信息增益作为评价标准。信息增益的缺点是倾向于选择取值较多的属性，在有些情况下这类属性可能不会提供太多有价值的信息.</span><br><span class="line">(<span class="number">2</span>) ID3算法只能对描述属性为离散型属性的数据集构造决策树。</span><br></pre></td></tr></table></figure>
<h5 id="C4-5算法"><a href="#C4-5算法" class="headerlink" title="C4.5算法"></a>C4.5算法</h5><p><strong>1.用信息增益率来选择属性</strong></p>
<p>克服了用信息增益来选择属性时偏向选择值多的属性的不足。</p>
<p><strong>2.采用了一种后剪枝方法</strong></p>
<p>避免树的高度无节制的增长，避免过度拟合数据</p>
<p><strong>3.对于缺失值的处理</strong></p>
<p>在某些情况下，可供使用的数据可能缺少某些属性的值。假如〈x，c(x)〉是样本集S中的一个训练实例，但是其属性A的值A(x)未知。处理缺少属性值的一种策略是赋给它结点n所对应的训练实例中该属性的最常见值；</p>
<p>另外一种更复杂的策略是为A的每个可能值赋予一个概率。</p>
<p>例如，给定一个布尔属性A，如果结点n包含6个已知A=1和4个A=0的实例，那么A(x)=1的概率是0.6，而A(x)=0的概率是0.4。于是，实例x的$60\%$被分配到A=1的分支，$40\%$被分配到另一个分支。</p>
<p><strong>C4.5就是使用这种方法处理缺少的属性值</strong>。</p>
<p><strong>C4.5算法的优缺点</strong></p>
<p> 优点：产生的分类规则易于理解，准确率较高。</p>
<p> 缺点： 在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。此外，C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。</p>
<h5 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h5><p> CART算法相比C4.5算法的分类方法，采用了简化的二叉树模型，同时特征选择采用了近似的基尼系数来简化计算。 </p>
<p> <strong>C4.5不一定是二叉树，但CART一定是二叉树</strong> </p>
<h5 id="多变量决策树-multi-variate-decision-tree"><a href="#多变量决策树-multi-variate-decision-tree" class="headerlink" title="多变量决策树(multi-variate decision tree)"></a>多变量决策树(multi-variate decision tree)</h5><p> 同时，无论是ID3, C4.5还是CART,在做特征选择的时候都是选择最优的一个特征来做分类决策，但是大多数，<strong>分类决策不应该是由某一个特征决定的，而是应该由一组特征决定的。</strong>这样决策得到的决策树更加准确。这个决策树叫做多变量决策树(multi-variate decision tree)。在选择最优特征的时候，多变量决策树不是选择某一个最优特征，而是选择最优的一个特征线性组合来做决策。这个算法的代表是OC1 （了解一下）。</p>
<p> 如果样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习里面的随机森林之类的方法解决。 </p>
<h4 id="决策树变量的两种类型"><a href="#决策树变量的两种类型" class="headerlink" title="决策树变量的两种类型"></a>决策树变量的两种类型</h4><ol>
<li>数字型（Numeric）：变量类型是整数或浮点数，如前面例子中的“年收入”。用“&gt;=”，“&gt;”,“&lt;”或“&lt;=”作为分割条件（排序后，利用已有的分割情况，可以优化分割算法的时间复杂度）。</li>
<li>名称型（Nominal）：类似编程语言中的枚举类型，变量只能从有限的选项中选取，比如前面例子中的“婚姻情况”，只能是“单身”，“已婚”或“离婚”，使用“=”来分割。</li>
</ol>
<h4 id="如何评估分割点的好坏"><a href="#如何评估分割点的好坏" class="headerlink" title="如何评估分割点的好坏"></a>如何评估分割点的好坏</h4><p>如果一个分割点可以将当前的所有节点分为两类，使得每一类都很“纯”，也就是同一类的记录较多，那么就是一个好分割点。</p>
<p>比如上面的例子，“拥有房产”，可以将记录分成了两类，“是”的节点全部都可以偿还债务，非常“纯”；“否”的节点，可以偿还贷款和无法偿还贷款的人都有，不是很“纯”，但是两个节点加起来的纯度之和与原始节点的纯度之差最大，所以按照这种方法分割。</p>
<p>构建决策树采用贪心算法，只考虑当前纯度差最大的情况作为分割点。</p>
<h4 id="cart剪枝"><a href="#cart剪枝" class="headerlink" title="cart剪枝"></a>cart剪枝</h4><h5 id="为什么要剪枝"><a href="#为什么要剪枝" class="headerlink" title="为什么要剪枝"></a>为什么要剪枝</h5><p> <img src="/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/cart.png" alt="image-20190214142450219"> </p>
<ul>
<li><strong>图形描述</strong><ul>
<li>横轴表示在决策树创建过程中树的结点总数，纵轴表示决策树的预测精度。</li>
<li>实线显示的是决策树在训练集上的精度，虚线显示的则是在一个独立的测试集上测量出来的精度。</li>
<li>随着树的增长，在训练样集上的精度是单调上升的， 然而在独立的测试样例上测出的精度先上升后下降。</li>
</ul>
</li>
<li><strong>出现这种情况的原因：</strong><ul>
<li>原因1：噪声、样本冲突，即错误的样本数据。</li>
<li>原因2：特征即属性不能完全作为分类标准。</li>
<li>原因3：巧合的规律性，数据量不够大。</li>
</ul>
</li>
</ul>
<h5 id="常用的减枝方法"><a href="#常用的减枝方法" class="headerlink" title="常用的减枝方法"></a>常用的减枝方法</h5><ol>
<li><p><strong>预剪枝</strong></p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="number">1</span>）每一个结点所包含的最小样本数目，例如<span class="number">10</span>，则该结点总样本数小于<span class="number">10</span>时，则不再分；</span><br><span class="line"><span class="number">2</span>）指定树的高度或者深度，例如树的最大深度为<span class="number">4</span>；</span><br><span class="line"><span class="number">3</span>）指定结点的熵小于某个值，不再划分。随着树的增长， 在训练样集上的精度是单调上升的， 然而在独立的测试样例上测出的精度先上升后下降。</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong>后剪枝</strong></p>
<p>把一棵树，构建完成之后，再进行从下往上的剪枝 </p>
</li>
</ol>
]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>机器学习算法</tag>
        <tag>机器学习</tag>
        <tag>监督学习算法</tag>
        <tag>决策树分类原理</tag>
      </tags>
  </entry>
  <entry>
    <title>TCP协议</title>
    <url>/2020/01/09/TCP%E5%8D%8F%E8%AE%AE/</url>
    <content><![CDATA[<h4 id="网络应用程序之间的通信流程"><a href="#网络应用程序之间的通信流程" class="headerlink" title="网络应用程序之间的通信流程"></a>网络应用程序之间的通信流程</h4><p> 通过 IP 地址能够找到对应的设备，然后再通过端口号找到对应的端口，再通过端口把数据传输给应用程序，<strong>这里要注意，数据不能随便发送，在发送之前还需要选择一个对应的传输协议，保证程序之间按照指定的传输规则进行数据的通信，</strong> 而这个传输协议就是TCP协议。</p>
 <a id="more"></a>
<h4 id="TCP-的概念"><a href="#TCP-的概念" class="headerlink" title="TCP 的概念"></a>TCP 的概念</h4><p>TCP 的英文全拼(Transmission Control Protocol)简称<strong>传输控制协议</strong>，它是一种<strong>面向连接的、可靠的、基于字节流的传输层通信协议</strong>。</p>
<p><strong>面向连接的效果图:</strong></p>
<p> <img src="/2020/01/09/TCP%E5%8D%8F%E8%AE%AE/%E9%9D%A2%E5%90%91%E8%BF%9E%E6%8E%A5.png" alt="面向连接"> </p>
<p><strong>TCP 通信步骤:</strong></p>
<ol>
<li>创建连接</li>
<li>传输数据</li>
<li>关闭连接</li>
</ol>
<h4 id="TCP-的特点"><a href="#TCP-的特点" class="headerlink" title="TCP 的特点"></a>TCP 的特点</h4><ol>
<li>面向连接<ul>
<li>通信双方必须先建立好连接才能进行数据的传输，数据传输完成后，双方必须断开此连接，以释放系统资源。双方间的数据传输都可以通过这一个连接进行，完成数据交换后，双方必须断开此连接，以释放系统资源。<strong>这种连接是一对一的，因此TCP不适用于广播的应用程序，基于广播的应用程序请使用UDP协议</strong></li>
</ul>
</li>
<li>可靠传输<ul>
<li>TCP 采用发送应答机制：通过TCP这种方式发送的每一个报文段都必须得到接收方的应答才认为这个TCP报文传送成功。</li>
<li>超时重传：指定时间内没有应答会重新发送。</li>
<li>错误校验：传输数据必须与接收数据一致，否则传输失败。</li>
<li>流量控制和阻塞管理：流量控制用来避免发送端发送过快而使得接收方来不及接收。</li>
</ul>
</li>
<li>TCP 是一个<strong>稳定、可靠的传输协议，常用于对数据进行准确无误的传输，比如: 文件下载，浏览器上网</strong>。 </li>
</ol>
<h4 id="TCP在建立连接时的三次握手"><a href="#TCP在建立连接时的三次握手" class="headerlink" title="TCP在建立连接时的三次握手"></a>TCP在建立连接时的三次握手</h4><figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">TCP在建立连接时需要通过三次握手过程来完成。</span><br><span class="line">  	原因：TCP 协议为了实现可靠传输， 通信双方需要判断自己已经发送的数据包是否都被接收方收到， 如果没收到， 就需要重发。 为了实现这个需求， 很自然地就会引出序号（sequence number） 和确认号（acknowledgement number） 的使用。发送方在发送数据包（假设大小为 <span class="number">10</span> byte）时， 同时送上一个序号( 假设为 <span class="number">500</span>)，那么接收方收到这个数据包以后， 就可以回复一个确认号（<span class="number">510</span> = <span class="number">500</span> + <span class="number">10</span>） 告诉发送方 “我已经收到了你的数据包， 你可以发送下一个数据包， 序号从 <span class="number">510</span> 开始” 。这样发送方就可以知道哪些数据被接收到，哪些数据没被接收到， 需要重发。</span><br></pre></td></tr></table></figure>
<ul>
<li>第一次握手：Client将标志位SYN置为1，随机产生一个值seq=J，并将该数据包发送给Server，Client进入SYN_SENT状态，等待Server确认。</li>
<li>第二次握手：Server收到数据包后由标志位SYN=1知道Client请求建立连接，Server将标志位SYN和ACK都置为1，ack (number )=J+1，随机产生一个值seq=K，并将该数据包发送给Client以确认连接请求，Server进入SYN_RCVD状态。</li>
<li>第三次握手：Client收到确认后，检查ack是否为J+1，ACK是否为1，如果正确则将标志位ACK置为1，ack=K+1，并将该数据包发送给Server，Server检查ack是否为K+1，ACK是否为1，如果正确则连接建立成功，Client和Server进入ESTABLISHED状态，完成三次握手，随后Client与Server之间可以开始传输数据了。</li>
</ul>
<h4 id="TCP在断开连接时的四次挥手"><a href="#TCP在断开连接时的四次挥手" class="headerlink" title="TCP在断开连接时的四次挥手"></a>TCP在断开连接时的四次挥手</h4><p>TCP在断开连接时，需要通过四次挥手的过程来完成。</p>
<ul>
<li>第一次挥手：Client发送一个FIN，用来关闭Client到Server的数据传送。</li>
<li>第二次挥手：Server收到FIN后，发送一个ACK给Client，确认序号为收到序号+1。</li>
<li>第三次挥手：Server发送一个FIN，用来关闭Server到Client的数据传送。</li>
<li>第四次挥手：Client收到FIN后，接着发送一个ACK给Server，确认序号为收到序号+1。</li>
</ul>
<h4 id="TCP-网络应用程序开发流程"><a href="#TCP-网络应用程序开发流程" class="headerlink" title="TCP 网络应用程序开发流程"></a>TCP 网络应用程序开发流程</h4><p>TCP 网络应用程序开发分为:</p>
<ul>
<li>TCP 客户端程序开发</li>
<li>TCP 服务端程序开发</li>
</ul>
<p><strong>说明:</strong></p>
<p>客户端程序是指运行在<strong>用户设备上的程序</strong> 服务端程序是指运行在<strong>服务器设备上的程序</strong>，专门为客户端提供数据服务。</p>
<h5 id="TCP-客户端程序开发流程的介绍"><a href="#TCP-客户端程序开发流程的介绍" class="headerlink" title="TCP 客户端程序开发流程的介绍"></a>TCP 客户端程序开发流程的介绍</h5><p> <img src="/2020/01/09/TCP%E5%8D%8F%E8%AE%AE/tcp%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E7%9A%84%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B.png" alt="TCP客户端程序开发流程"> </p>
<p><strong>步骤说明:</strong></p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span> 创建客户端套接字对象</span><br><span class="line"><span class="number">2.</span> 和服务端套接字建立连接</span><br><span class="line"><span class="number">3.</span> 发送数据</span><br><span class="line"><span class="number">4.</span> 接收数据</span><br><span class="line"><span class="number">5.</span> 关闭客户端套接字</span><br></pre></td></tr></table></figure>
<h5 id="TCP-服务端程序开发流程的介绍"><a href="#TCP-服务端程序开发流程的介绍" class="headerlink" title="TCP 服务端程序开发流程的介绍"></a>TCP 服务端程序开发流程的介绍</h5><p> <img src="/2020/01/09/TCP%E5%8D%8F%E8%AE%AE/tcp%E7%BD%91%E7%BB%9C%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E7%9A%84%E5%BC%80%E5%8F%91%E6%B5%81%E7%A8%8B-1578661899128.png" alt="TCP客户端程序开发流程"> </p>
<p><strong>步骤说明:</strong></p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span> 创建服务端端套接字对象</span><br><span class="line"><span class="number">2.</span> 绑定端口号</span><br><span class="line"><span class="number">3.</span> 设置监听</span><br><span class="line"><span class="number">4.</span> 等待接受客户端的连接请求</span><br><span class="line"><span class="number">5.</span> 接收数据</span><br><span class="line"><span class="number">6.</span> 发送数据</span><br><span class="line"><span class="number">7.</span> 关闭套接字</span><br></pre></td></tr></table></figure>
<p><strong>主动发起建立连接请求的</strong>是客户端程序</p>
<p><strong>等待接受连接请求的</strong>是服务端程序</p>
<h4 id="socket-介绍"><a href="#socket-介绍" class="headerlink" title="socket 介绍"></a>socket 介绍</h4><p> socket (简称 套接字) 是<strong>进程之间通信一个工具</strong>，好比现实生活中的<strong>插座</strong>，所有的家用电器要想工作都是基于插座进行，<strong>进程之间想要进行网络通信需要基于这个 socket</strong>。 </p>
<p>socket 的作用：负责<strong>进程之间的网络数据传输</strong>，好比数据的搬运工。 </p>
<h4 id="TCP-客户端程序开发API"><a href="#TCP-客户端程序开发API" class="headerlink" title="TCP 客户端程序开发API"></a>TCP 客户端程序开发API</h4><p><strong>socket 类的介绍</strong>：</p>
<p>1.导入 socket 模块</p>
<figure class="highlight elm"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br></pre></td></tr></table></figure>
<p>2.创建客户端 socket 对象</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">socket.socket(AddressFamily, Type)</span><br><span class="line">参数说明:</span><br><span class="line">- AddressFamily 表示IP地址类型, 分为IPv4和IPv6</span><br><span class="line">-<span class="built_in"> Type </span>表示传输协议类型</span><br><span class="line">方法说明:</span><br><span class="line">- connect((host, port)) 表示和服务端套接字建立连接, host是服务器ip地址，port是应用程序的端口号</span><br><span class="line">- send(data) 表示发送数据，data是二进制数据</span><br><span class="line">- recv(buffersize) 表示接收数据, buffersize是每次接收数据的长度</span><br></pre></td></tr></table></figure>
<p>3.创建服务端 socket 对象</p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">socket.socket(AddressFamily, Type)</span><br><span class="line">参数说明:</span><br><span class="line">- AddressFamily 表示IP地址类型, 分为IPv4和IPv6</span><br><span class="line">-<span class="built_in"> Type </span>表示传输协议类型</span><br><span class="line">方法说明:</span><br><span class="line">- bind((host, port)) 表示绑定端口号, host 是<span class="built_in"> ip </span>地址，port 是端口号，ip 地址一般不指定，表示本机的任何一个ip地址都可以。</span><br><span class="line">- listen (backlog) 表示设置监听，backlog参数表示最大等待建立连接的个数。</span><br><span class="line">- accept() 表示等待接受客户端的连接请求</span><br><span class="line">- send(data) 表示发送数据，data 是二进制数据</span><br><span class="line">- recv(buffersize) 表示接收数据, buffersize 是每次接收数据的长度</span><br></pre></td></tr></table></figure>
<h4 id="TCP-客户端程序开发示例代码"><a href="#TCP-客户端程序开发示例代码" class="headerlink" title="TCP 客户端程序开发示例代码"></a>TCP 客户端程序开发示例代码</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 创建tcp客户端套接字</span></span><br><span class="line">    <span class="comment"># 1. AF_INET：表示ipv4</span></span><br><span class="line">    <span class="comment"># 2. SOCK_STREAM: tcp传输协议</span></span><br><span class="line">    tcp_client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)</span><br><span class="line">    <span class="comment"># 和服务端应用程序建立连接</span></span><br><span class="line">    tcp_client_socket.connect((<span class="string">"192.168.131.62"</span>, <span class="number">8080</span>))</span><br><span class="line">    <span class="comment"># 代码执行到此，说明连接建立成功</span></span><br><span class="line">    <span class="comment"># 准备发送的数据</span></span><br><span class="line">    send_data = <span class="string">"你好服务端，我是客户端小黑!"</span>.encode(<span class="string">"gbk"</span>)</span><br><span class="line">    <span class="comment"># 发送数据</span></span><br><span class="line">    tcp_client_socket.send(send_data)</span><br><span class="line">    <span class="comment"># 接收数据, 这次接收的数据最大字节数是1024</span></span><br><span class="line">    recv_data = tcp_client_socket.recv(<span class="number">1024</span>)</span><br><span class="line">    <span class="comment"># 返回的直接是服务端程序发送的二进制数据</span></span><br><span class="line">    print(recv_data)</span><br><span class="line">    <span class="comment"># 对数据进行解码</span></span><br><span class="line">    recv_content = recv_data.decode(<span class="string">"gbk"</span>)</span><br><span class="line">    print(<span class="string">"接收服务端的数据为:"</span>, recv_content)</span><br><span class="line">    <span class="comment"># 关闭套接字</span></span><br><span class="line">    tcp_client_socket.close()</span><br></pre></td></tr></table></figure>
<h4 id="案例-多任务版TCP服务端程序开发"><a href="#案例-多任务版TCP服务端程序开发" class="headerlink" title="案例-多任务版TCP服务端程序开发"></a>案例-多任务版TCP服务端程序开发</h4><h5 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h5><p>目前我们开发的TCP服务端程序只能服务于一个客户端，如何开发一个多任务版的TCP服务端程序能够服务于多个客户端呢?</p>
<p>完成多任务，可以使用<strong>线程</strong>，比进程更加节省内存资源。</p>
<h5 id="具体实现步骤"><a href="#具体实现步骤" class="headerlink" title="具体实现步骤"></a>具体实现步骤</h5><ol>
<li>编写一个TCP服务端程序，循环等待接受客户端的连接请求</li>
<li>当客户端和服务端建立连接成功，创建子线程，使用子线程专门处理客户端的请求，防止主线程阻塞</li>
<li>把创建的子线程设置成为守护主线程，防止主线程无法退出。</li>
</ol>
<h5 id="多任务版TCP服务端程序的示例代码"><a href="#多任务版TCP服务端程序的示例代码" class="headerlink" title="多任务版TCP服务端程序的示例代码"></a>多任务版TCP服务端程序的示例代码</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理客户端的请求操作</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handle_client_request</span><span class="params">(service_client_socket, ip_port)</span>:</span></span><br><span class="line">    <span class="comment"># 循环接收客户端发送的数据</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># 接收客户端发送的数据</span></span><br><span class="line">        recv_data = service_client_socket.recv(<span class="number">1024</span>)</span><br><span class="line">        <span class="comment"># 容器类型判断是否有数据可以直接使用if语句进行判断，如果容器类型里面有数据表示条件成立，否则条件失败</span></span><br><span class="line">        <span class="comment"># 容器类型: 列表、字典、元组、字符串、set、range、二进制数据</span></span><br><span class="line">        <span class="keyword">if</span> recv_data:</span><br><span class="line">            print(recv_data.decode(<span class="string">"gbk"</span>), ip_port)</span><br><span class="line">            <span class="comment"># 回复</span></span><br><span class="line">            service_client_socket.send(<span class="string">"ok，问题正在处理中..."</span>.encode(<span class="string">"gbk"</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"客户端下线了:"</span>, ip_port)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># 终止和客户端进行通信</span></span><br><span class="line">    service_client_socket.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 创建tcp服务端套接字</span></span><br><span class="line">    tcp_server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)</span><br><span class="line">    <span class="comment"># 设置端口号复用，让程序退出端口号立即释放</span></span><br><span class="line">    tcp_server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, <span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 绑定端口号</span></span><br><span class="line">    tcp_server_socket.bind((<span class="string">""</span>, <span class="number">9090</span>))</span><br><span class="line">    <span class="comment"># 设置监听, listen后的套接字是被动套接字，只负责接收客户端的连接请求</span></span><br><span class="line">    tcp_server_socket.listen(<span class="number">128</span>)</span><br><span class="line">    <span class="comment"># 循环等待接收客户端的连接请求</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># 等待接收客户端的连接请求</span></span><br><span class="line">        service_client_socket, ip_port = tcp_server_socket.accept()</span><br><span class="line">        print(<span class="string">"客户端连接成功:"</span>, ip_port)</span><br><span class="line">        <span class="comment"># 当客户端和服务端建立连接成功以后，需要创建一个子线程，不同子线程负责接收不同客户端的消息</span></span><br><span class="line">        sub_thread = threading.Thread(target=handle_client_request, args=(service_client_socket, ip_port))</span><br><span class="line">        <span class="comment"># 设置守护主线程</span></span><br><span class="line">        sub_thread.setDaemon(<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 启动子线程</span></span><br><span class="line">        sub_thread.start()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># tcp服务端套接字可以不需要关闭，因为服务端程序需要一直运行</span></span><br><span class="line">    <span class="comment"># tcp_server_socket.close()</span></span><br></pre></td></tr></table></figure>
<p><strong>说明:</strong></p>
<p>当客户端和服务端建立连接后，<strong>服务端程序退出后端口号不会立即释放，需要等待大概1-2分钟。</strong></p>
<p>解决办法有两种:</p>
<ol>
<li>更换服务端端口号</li>
<li>设置端口号复用(推荐大家使用)，也就是说让服务端程序退出后端口号立即释放。</li>
</ol>
<p>设置端口号复用的代码如下:</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 参数1: 表示当前套接字</span></span><br><span class="line"><span class="comment"># 参数2: 设置端口号复用选项</span></span><br><span class="line"><span class="comment"># 参数3: 设置端口号复用选项对应的值</span></span><br><span class="line">tcp_server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, <span class="literal">True</span>)</span><br></pre></td></tr></table></figure>
<h4 id="socket之send和recv原理剖析"><a href="#socket之send和recv原理剖析" class="headerlink" title="socket之send和recv原理剖析"></a>socket之send和recv原理剖析</h4><h5 id="认识TCP-socket的发送和接收缓冲区"><a href="#认识TCP-socket的发送和接收缓冲区" class="headerlink" title="认识TCP socket的发送和接收缓冲区"></a>认识TCP socket的发送和接收缓冲区</h5><p>当创建一个TCP socket对象的时候会有一个<strong>发送缓冲区</strong>和一个<strong>接收缓冲区</strong>，<strong>这个发送和接收缓冲区指的就是内存中的一片空间。</strong></p>
<h5 id="send原理剖析"><a href="#send原理剖析" class="headerlink" title="send原理剖析"></a>send原理剖析</h5><p>send是不是直接把数据发给服务端?</p>
<p>不是，要想发数据，必须得<strong>通过网卡发送数据</strong>，应用程序是无法直接通过网卡发送数据的，它需要调用操作系统接口，也就是说，应用程序把发送的数据先写入到<strong>发送缓冲区</strong>(内存中的一片空间)，再<strong>由操作系统控制网卡把发送缓冲区的数据发送给服务端网卡</strong> 。</p>
<h5 id="recv原理剖析"><a href="#recv原理剖析" class="headerlink" title="recv原理剖析"></a>recv原理剖析</h5><p>recv是不是直接从客户端接收数据?</p>
<p>不是，<strong>应用软件是无法直接通过网卡接收数据的</strong>，它需要调用操作系统接口，<strong>由操作系统通过网卡接收数据</strong>，把接收的数据<strong>写入到接收缓冲区</strong>(内存中的一片空间），应用程序<strong>再从接收缓存区获取客户端发送的数据</strong>。</p>
<h5 id="send和recv原理剖析图"><a href="#send和recv原理剖析图" class="headerlink" title="send和recv原理剖析图"></a>send和recv原理剖析图</h5><p> <img src="/2020/01/09/TCP%E5%8D%8F%E8%AE%AE/send%E5%92%8Crecv%E5%8E%9F%E7%90%86.png" alt="send和recv原理剖析图"> </p>
<ul>
<li>发送数据是发送到发送缓冲区</li>
<li>接收数据是从接收缓冲区 获取</li>
</ul>
]]></content>
      <categories>
        <category>网络编程</category>
      </categories>
      <tags>
        <tag>传输层通信协议</tag>
        <tag>TCP协议</tag>
        <tag>TCP应用程序开发</tag>
      </tags>
  </entry>
  <entry>
    <title>逻辑回归</title>
    <url>/2020/01/09/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<h4 id="什么是逻辑回归"><a href="#什么是逻辑回归" class="headerlink" title="什么是逻辑回归"></a>什么是逻辑回归</h4><p> 逻辑回归（Logistic Regression）是机器学习中的<strong>一种分类模型</strong>，逻辑回归是一种分类算法，虽然名字中带有回归。由于算法的简单和高效，在实际中应用非常广泛。 </p>
<a id="more"></a>
<h4 id="逻辑回归的应用场景"><a href="#逻辑回归的应用场景" class="headerlink" title="逻辑回归的应用场景"></a>逻辑回归的应用场景</h4><ul>
<li>广告点击率</li>
<li>是否为垃圾邮件</li>
<li>是否患病</li>
<li>金融诈骗</li>
<li><p>虚假账号</p>
<p>看到上面的例子，我们可以发现其中的特点，那就是都属于两个类别之间的判断。逻辑回归就是解决二分类问题的利器 。</p>
</li>
</ul>
<h4 id="逻辑回归的原理"><a href="#逻辑回归的原理" class="headerlink" title="逻辑回归的原理"></a>逻辑回归的原理</h4><ol>
<li><h5 id="输入"><a href="#输入" class="headerlink" title="输入"></a>输入</h5><p><img src="/2020/01/09/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E8%BE%93%E5%85%A5.png" alt="é€&quot;è¾‘å›žå½’è¾“å…¥"></p>
<p>  <strong>逻辑回归的输入就是一个线性回归的结果</strong>。 </p>
</li>
<li><h5 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h5><ul>
<li><p><strong>sigmoid函数</strong> </p>
<p><img src="/2020/01/09/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/1578552295823.png" alt="1578552295823"></p>
</li>
<li><p>判断标准</p>
<ol>
<li>回归的结果输入到sigmoid函数当中</li>
<li><p>输出结果：[0, 1]区间中的一个概率值，默认为0.5为阈值。</p>
<p><img src="/2020/01/09/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/sigmoid%E5%9B%BE%E5%83%8F.png" alt="img"></p>
</li>
</ol>
<p>逻辑回归最终的分类是通过属于某个类别的概率值来判断是否属于某个类别，并且这个类别默认标记为1(正例),另外的一个类别会标记为0(反例)。（方便损失计算） </p>
</li>
</ul>
</li>
</ol>
<h4 id="逻辑损失以及优化"><a href="#逻辑损失以及优化" class="headerlink" title="逻辑损失以及优化"></a>逻辑损失以及优化</h4><p> 逻辑回归的损失，称之为<strong>对数似然损失</strong>，公式如下： </p>
<ul>
<li><p>分开类别：</p>
<p><img src="/2020/01/09/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/%E5%8D%95%E4%B8%AA%E5%AF%B9%E6%95%B0%E4%BC%BC%E7%84%B6%E6%8D%9F%E5%A4%B1.png" alt="å•ä¸ªå¯¹æ•°ä¼¼ç„¶æŸå¤±"></p>
<p>其中y为真实值，$h_\theta(x)$为预测值  </p>
<p><img src="/2020/01/09/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/log%E5%9B%BE%E5%83%8F.png" alt="image-20190221142055367"></p>
<p>无论何时，我们都希望<strong>损失函数值，越小越好</strong></p>
<p>分情况讨论，对应的损失函数值：</p>
<ul>
<li><strong>当y=1时，我们希望$h_\theta(x)$值越大越好；</strong></li>
<li><strong>当y=0时，我们希望$h_\theta(x)$值越小越好</strong></li>
</ul>
</li>
<li><p>综合完整损失函数</p>
<p> <img src="/2020/01/09/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/%E5%AE%8C%E6%95%B4%E5%AF%B9%E6%95%B0%E4%BC%BC%E7%84%B6%E6%8D%9F%E5%A4%B1.png" alt="å®Œæ•´å¯¹æ•°ä¼¼ç„¶æŸå¤±"></p>
</li>
<li><p><strong>优化</strong></p>
<p>同样使用梯度下降优化算法，去减少损失函数的值。这样去更新逻辑回归前面对应算法的权重参数，<strong>提升原本属于1类别的概率，降低原本是0类别的概率。</strong>  </p>
</li>
</ul>
<h4 id="逻辑回归API"><a href="#逻辑回归API" class="headerlink" title="逻辑回归API"></a>逻辑回归API</h4><p><strong>sklearn.linear_model.LogisticRegression(solver=’liblinear’, penalty=‘l2’, C = 1.0)</strong></p>
<ul>
<li>solver可选参数:{‘liblinear’, ‘sag’, ‘saga’,’newton-cg’, ‘lbfgs’}，<ul>
<li>默认: ‘liblinear’；用于优化问题的算法。</li>
<li>对于小数据集来说，“liblinear”是个不错的选择，而“sag”和’saga’对于大型数据集会更快。</li>
<li>对于多类问题，只有’newton-cg’， ‘sag’， ‘saga’和’lbfgs’可以处理多项损失;“liblinear”仅限于“one-versus-rest”分类。</li>
</ul>
</li>
<li>penalty：正则化的种类</li>
<li><p>C：正则化力度</p>
<p><strong>LogisticRegression方法相当于 SGDClassifier(loss=”log”, penalty=” “),SGDClassifier实现了一个普通的随机梯度下降学习。而使用LogisticRegression(实现了SAG)。</strong> </p>
</li>
</ul>
<h4 id="案例-癌症分类预测"><a href="#案例-癌症分类预测" class="headerlink" title="案例-癌症分类预测"></a>案例-癌症分类预测</h4><ol>
<li><h5 id="数据介绍"><a href="#数据介绍" class="headerlink" title="数据介绍"></a>数据介绍</h5><p> <img src="/2020/01/09/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/%E7%99%8C%E7%97%87%E6%95%B0%E6%8D%AE.png" alt="ç™Œç—‡æ•°æ®"></p>
<p>  原始数据的下载地址：<a href="https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/" target="_blank" rel="noopener">https://archive.ics.uci.edu/ml/machine-learning-databases/</a> </p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">数据描述</span><br><span class="line"></span><br><span class="line">（<span class="number">1</span>）<span class="number">699</span>条样本，共<span class="number">11</span>列数据，第一列用语检索的id，后<span class="number">9</span>列分别是与肿瘤相关的医学特征，最后一列表示肿瘤类型的数值。</span><br><span class="line">（<span class="number">2</span>）包含<span class="number">16</span>个缺失值，用”?”标出。</span><br></pre></td></tr></table></figure>
</li>
<li><h5 id="案例分析"><a href="#案例分析" class="headerlink" title="案例分析"></a>案例分析</h5><figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span>获取数据</span><br><span class="line"><span class="number">2.</span>基本数据处理</span><br><span class="line"><span class="number">2.1</span> 缺失值处理</span><br><span class="line"><span class="number">2.2</span> 确定特征值,目标值</span><br><span class="line"><span class="number">2.3</span> 分割数据</span><br><span class="line"><span class="number">3.</span>特征工程(标准化)</span><br><span class="line"><span class="number">4.</span>机器学习(逻辑回归)</span><br><span class="line"><span class="number">5.</span>模型评估</span><br></pre></td></tr></table></figure>
</li>
<li><h5 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> ssl</span><br><span class="line">ssl._create_default_https_context = ssl._create_unverified_context</span><br></pre></td></tr></table></figure>
<p><strong>1. 获取数据</strong></p>
<figure class="highlight kotlin"><table><tr><td class="code"><pre><span class="line">names = [<span class="string">'Sample code number'</span>, <span class="string">'Clump Thickness'</span>, <span class="string">'Uniformity of Cell Size'</span>, <span class="string">'Uniformity of Cell Shape'</span>,</span><br><span class="line">                   <span class="string">'Marginal Adhesion'</span>, <span class="string">'Single Epithelial Cell Size'</span>, <span class="string">'Bare Nuclei'</span>, <span class="string">'Bland Chromatin'</span>,</span><br><span class="line">                   <span class="string">'Normal Nucleoli'</span>, <span class="string">'Mitoses'</span>, <span class="string">'Class'</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">data</span> = pd.read_csv(<span class="string">"https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data"</span>,</span><br><span class="line">                  names=names)</span><br><span class="line"><span class="keyword">data</span>.head()</span><br></pre></td></tr></table></figure>
<p><strong>2. 基本数据处理</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 2.1 缺失值处理</span></span><br><span class="line">data = data.replace(to_replace=<span class="string">"?"</span>, value=np.NaN)</span><br><span class="line">data = data.dropna()</span><br><span class="line"><span class="comment"># 2.2 确定特征值,目标值</span></span><br><span class="line">x = data.iloc[:, <span class="number">1</span>:<span class="number">10</span>]</span><br><span class="line">x.head()</span><br><span class="line">y = data[<span class="string">"Class"</span>]</span><br><span class="line">y.head()</span><br><span class="line"><span class="comment"># 2.3 分割数据</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=<span class="number">22</span>)</span><br></pre></td></tr></table></figure>
<p><strong>3. 特征工程(标准化)</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">transfer = StandardScaler()</span><br><span class="line">x_train = transfer.fit_transform(x_train)</span><br><span class="line">x_test = transfer.transform(x_test)</span><br></pre></td></tr></table></figure>
<p><strong>4. 机器学习(逻辑回归)</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">estimator = LogisticRegression()</span><br><span class="line">estimator.fit(x_train, y_train)</span><br></pre></td></tr></table></figure>
<p><strong>5. 模型评估</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_predict = estimator.predict(x_test)</span><br><span class="line">y_predict</span><br><span class="line">estimator.score(x_test, y_test)</span><br></pre></td></tr></table></figure>
</li>
</ol>
<h4 id="分类评估方法"><a href="#分类评估方法" class="headerlink" title="分类评估方法"></a>分类评估方法</h4><p>在很多分类场景当中我们不一定只关注预测的准确率。</p>
<p>比如以这个癌症举例子，我们并不关注预测的准确率，而是关注在所有的样本当中，癌症患者有没有被全部预测（检测）出来。**</p>
<h5 id="精确率与召回率"><a href="#精确率与召回率" class="headerlink" title="精确率与召回率"></a>精确率与召回率</h5><ul>
<li><p><strong>准确率 （对不对） </strong></p>
</li>
<li><p>（TP+TN）/(TP+TN+FN+FP) </p>
</li>
<li><p><strong>混淆矩阵</strong></p>
<p> 在分类任务下，预测结果(Predicted Condition)与正确标记(True Condition)之间存在四种不同的组合，构成混淆矩阵(适用于多分类) </p>
<p> <img src="/2020/01/09/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/%E6%B7%B7%E6%B7%86%E7%9F%A9%E9%98%B5.png" alt="image-20190321103913068"></p>
</li>
<li><p><strong>精确率(Precision)与召回率(Recall)</strong></p>
<ul>
<li><p>精确率 —— 查的准不准 ：预测结果为正例样本中真实为正例的比例 </p>
</li>
<li><p>TP/(TP+FP) </p>
</li>
<li><p>P -预测为正，N-预测为负</p>
</li>
<li><p>T-预测对了， F-预测错了</p>
<p> <img src="/2020/01/09/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/confusion_matrix1.png" alt="image-20190321103930761"> </p>
</li>
<li><p>召回率  ——  查的全不全 ：真实为正例的样本中预测结果为正例的比例（查得全，对正样本的区分能力） </p>
</li>
<li><p>TP/(TP+FN) </p>
<p><img src="/2020/01/09/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/confusion_matrix2.png" alt="image-20190321103947092"> </p>
</li>
</ul>
</li>
<li><p><strong>F1-score</strong></p>
<ul>
<li><p>还有其他的评估标准，F1-score，反映了模型的稳健型 </p>
<p><img src="/2020/01/09/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/f1_score.png" alt="image-20190321104006686"></p>
</li>
</ul>
</li>
<li><p><strong>分类评估报告API</strong></p>
<ul>
<li>sklearn.metrics.classification_report(y_true, y_pred, labels=[], target_names=None )<ul>
<li>y_true：真实目标值</li>
<li>y_pred：估计器预测目标值</li>
<li>labels:指定类别对应的数字</li>
<li>target_names：目标类别名称</li>
<li>return：每个类别精确率与召回率</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ret = classification_report(y_test, y_predict, labels=(<span class="number">2</span>,<span class="number">4</span>), target_names=(<span class="string">"良性"</span>, <span class="string">"恶性"</span>))</span><br><span class="line">print(ret)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<h5 id="ROC曲线与AUC指标"><a href="#ROC曲线与AUC指标" class="headerlink" title="ROC曲线与AUC指标"></a>ROC曲线与AUC指标</h5><p><strong>假设这样一个情况，如果99个样本癌症，1个样本非癌症，不管怎样我全都预测正例(默认癌症为正例),准确率就为99%但是这样效果并不好，这就是样本不均衡下的评估问题</strong></p>
<p>问题：<strong>如何衡量样本不均衡下的评估</strong>？</p>
<ul>
<li><p><strong>TPR与FPR</strong></p>
<ul>
<li>TPR = TP / (TP + FN)    —— 正类的召回率<ul>
<li>所有真实类别为1的样本中，预测类别为1的比例</li>
</ul>
</li>
<li>FPR = FP / (FP + TN)    —— 1-负类的召回率<ul>
<li>所有真实类别为0的样本中，预测类别为1的比例</li>
</ul>
</li>
</ul>
</li>
<li><p><strong>ROC曲线</strong></p>
<p> ROC曲线的横轴就是FPRate，纵轴就是TPRate，当二者相等时，表示的意义则是：对于不论真实类别是1还是0的样本，分类器预测为1的概率是相等的，此时AUC为0.5 </p>
<p> <img src="/2020/01/09/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/ROC.png" alt="ROC"> </p>
</li>
<li><p><strong>AUC指标</strong></p>
<ul>
<li>AUC的概率意义是随机取一对正负样本，正样本得分大于负样本得分的概率</li>
<li>AUC的范围在[0, 1]之间，并且越接近1越好，越接近0.5属于乱猜</li>
<li><strong>AUC=1，完美分类器，采用这个预测模型时，不管设定什么阈值都能得出完美预测。绝大多数预测的场合，不存在完美分类器。</strong></li>
<li>$0.5&lt;AUC&lt;1$ <strong>优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。</strong> </li>
<li>这个指标主要用于评价不平衡的二分类问题 </li>
</ul>
</li>
<li><p><strong>AUC计算API</strong></p>
</li>
<li><p><strong>from sklearn.metrics import roc_auc_score</strong></p>
</li>
<li><p>sklearn.metrics.roc_auc_score(y_true, y_score)</p>
<ul>
<li>计算ROC曲线面积，即AUC值</li>
<li>y_true：每个样本的真实类别，必须为0(反例),1(正例)标记</li>
<li>y_score：预测得分，可以是正类的估计概率、置信值或者分类器方法的返回值</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 0.5~1之间，越接近于1约好</span></span><br><span class="line">y_test = np.where(y_test &gt; <span class="number">2.5</span>, <span class="number">1</span>, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"AUC指标："</span>, roc_auc_score(y_test, y_predict)</span><br></pre></td></tr></table></figure>
<ul>
<li>AUC只能用来评价二分类</li>
<li>AUC非常适合评价样本不平衡中的分类器性能</li>
</ul>
<h4 id="ROC曲线的绘制"><a href="#ROC曲线的绘制" class="headerlink" title="ROC曲线的绘制"></a>ROC曲线的绘制</h4><p>关于ROC曲线的绘制过程，通过以下举例进行说明：</p>
<h5 id="举例说明"><a href="#举例说明" class="headerlink" title="举例说明"></a>举例说明</h5><p>假设有6次展示记录，有两次被点击了，得到一个展示序列（1:1,2:0,3:1,4:0,5:0,6:0），前面的表示序号，后面的表示点击（1）或没有点击（0）。然后在这6次展示的时候都通过model算出了点击的概率序列。</p>
<p>下面看三种情况。</p>
<p>1.如果概率的序列是（1:0.9,2:0.7,3:0.8,4:0.6,5:0.5,6:0.4）：</p>
<p>与原来的序列一起，得到序列（从概率从高到低排）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>1</th>
<th>1</th>
<th>0</th>
<th>0</th>
<th>0</th>
<th>0</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.9</td>
<td>0.8</td>
<td>0.7</td>
<td>0.6</td>
<td>0.5</td>
<td>0.4</td>
</tr>
</tbody>
</table>
</div>
<p>绘制的步骤是：</p>
<p>1）把概率序列从高到低排序，得到顺序（1:0.9,3:0.8,2:0.7,4:0.6,5:0.5,6:0.4）；</p>
<p>2）从概率最大开始取一个点作为正类，取到点1，计算得到TPR=0.5，FPR=0.0；</p>
<p>3）从概率最大开始，再取一个点作为正类，取到点3，计算得到TPR=1.0，FPR=0.0；</p>
<p>4）再从最大开始取一个点作为正类，取到点2，计算得到TPR=1.0，FPR=0.25;</p>
<p>5）以此类推，得到6对TPR和FPR。</p>
<p>然后把这6对数据组成6个点(0,0.5),(0,1.0),(0.25,1),(0.5,1),(0.75,1),(1.0,1.0)。</p>
<p>这6个点在二维坐标系中能绘出来就是 ROC曲线 。</p>
<p> <img src="/2020/01/09/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/roc1.png" alt="image-20190406170931355"> </p>
<p>2.如果概率的序列是（1:0.9,2:0.8,3:0.7,4:0.6,5:0.5,6:0.4）：</p>
<p>与原来的序列一起，得到序列（从概率从高到低排）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>1</th>
<th>0</th>
<th>1</th>
<th>0</th>
<th>0</th>
<th>0</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.9</td>
<td>0.8</td>
<td>0.7</td>
<td>0.6</td>
<td>0.5</td>
<td>0.4</td>
</tr>
</tbody>
</table>
</div>
<p>绘制的步骤是：</p>
<p>6）把概率序列从高到低排序，得到顺序（1:0.9,2:0.8,3:0.7,4:0.6,5:0.5,6:0.4）；</p>
<p>7）从概率最大开始取一个点作为正类，取到点1，计算得到TPR=0.5，FPR=0.0；</p>
<p>8）从概率最大开始，再取一个点作为正类，取到点2，计算得到TPR=0.5，FPR=0.25；</p>
<p>9）再从最大开始取一个点作为正类，取到点3，计算得到TPR=1.0，FPR=0.25;</p>
<p>10）以此类推，得到6对TPR和FPR。</p>
<p>然后把这6对数据组成6个点(0,0.5),(0.25,0.5),(0.25,1),(0.5,1),(0.75,1),(1.0,1.0)。</p>
<p>这6个点在二维坐标系中能绘出来就是 ROC曲线 。</p>
<p> <img src="/2020/01/09/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/roc2.png" alt="image-20190406171018456"> </p>
<p>3.如果概率的序列是（1:0.4,2:0.6,3:0.5,4:0.7,5:0.8,6:0.9）：</p>
<p>与原来的序列一起，得到序列（从概率从高到低排）</p>
<div class="table-container">
<table>
<thead>
<tr>
<th>1</th>
<th>0</th>
<th>1</th>
<th>0</th>
<th>0</th>
<th>0</th>
</tr>
</thead>
<tbody>
<tr>
<td>0.9</td>
<td>0.8</td>
<td>0.7</td>
<td>0.6</td>
<td>0.5</td>
<td>0.4</td>
</tr>
</tbody>
</table>
</div>
<p>绘制的步骤是：</p>
<p>6）把概率序列从高到低排序，得到顺序（1:0.9,2:0.8,3:0.7,4:0.6,5:0.5,6:0.4）；</p>
<p>7）从概率最大开始取一个点作为正类，取到点1，计算得到TPR=0.5，FPR=0.0；</p>
<p>8）从概率最大开始，再取一个点作为正类，取到点2，计算得到TPR=0.5，FPR=0.25；</p>
<p>9）再从最大开始取一个点作为正类，取到点3，计算得到TPR=1.0，FPR=0.25;</p>
<p>10）以此类推，得到6对TPR和FPR。</p>
<p>然后把这6对数据组成6个点(0,0.5),(0.25,0.5),(0.25,1),(0.5,1),(0.75,1),(1.0,1.0)。</p>
<p>这6个点在二维坐标系中能绘出来就是ROC曲线。</p>
<p> <img src="/2020/01/09/%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92/roc2-1578562672822.png" alt="image-20190406171018456"> </p>
<h5 id="意义解释"><a href="#意义解释" class="headerlink" title="意义解释"></a>意义解释</h5><p>如上图的例子，总共6个点，2个正样本，4个负样本，取一个正样本和一个负样本的情况总共有8种。</p>
<p>上面的第一种情况，从上往下取，无论怎么取，正样本的概率总在负样本之上，所以分对的概率为1，AUC=1。再看那个ROC曲线，它的积分是什么？也是1，ROC曲线的积分与AUC相等。</p>
<p>上面第二种情况，如果取到了样本2和3，那就分错了，其他情况都分对了；所以分对的概率是0.875，AUC=0.875。再看那个ROC曲线，它的积分也是0.875，ROC曲线的积分与AUC相等。</p>
<p>上面的第三种情况，无论怎么取，都是分错的，所以分对的概率是0，AUC=0.0。再看ROC曲线，它的积分也是0.0，ROC曲线的积分与AUC相等。</p>
<p>很牛吧，其实AUC的意思是——Area Under roc Curve，就是ROC曲线的积分，也是ROC曲线下面的面积。</p>
<p>绘制ROC曲线的意义很明显，不断地把可能分错的情况扣除掉，从概率最高往下取的点，每有一个是负样本，就会导致分错排在它下面的所有正样本，所以要把它下面的正样本数扣除掉（1-TPR，剩下的正样本的比例）。总的ROC曲线绘制出来了，AUC就定了，分对的概率也能求出来了。</p>
]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>机器学习算法</tag>
        <tag>机器学习</tag>
        <tag>分类评估方法</tag>
      </tags>
  </entry>
  <entry>
    <title>线性回归</title>
    <url>/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</url>
    <content><![CDATA[<h4 id="什么是线性回归"><a href="#什么是线性回归" class="headerlink" title="什么是线性回归"></a>什么是线性回归</h4><p><strong>定义：</strong> 线性回归(Linear regression)是利用<strong>回归方程(函数)</strong>对<strong>一个或多个自变量(特征值)和因变量(目标值)之间</strong>关系进行建模的一种分析方式。 </p>
<p>特点： 只有一个自变量的情况称为单变量回归，多于一个自变量情况的叫做多元回归 </p>
<a id="more"></a>
<p><img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1578465007318.png" alt="1578465007318"></p>
<p>在机器学习中<strong>特征值与目标值之间建立了一个关系，这个关系可以理解为线性模型</strong>。 </p>
<h4 id="线性回归的特征与目标的关系"><a href="#线性回归的特征与目标的关系" class="headerlink" title="线性回归的特征与目标的关系"></a>线性回归的特征与目标的关系</h4><p> 线性回归当中主要有两种模型，<strong>一种是线性关系，另一种是非线性关系。</strong>在这里我们只能画一个平面更好去理解，所以都用单个特征或两个特征举例子。 </p>
<ul>
<li><p>线性关系</p>
<ul>
<li><p>单变量关系：</p>
<p><img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1578465045629.png" alt="单变量"></p>
</li>
</ul>
</li>
<li><p>多变量线性关系 </p>
<p><img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%A4%9A%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%85%B3%E7%B3%BB.png" alt="å¤šå˜é‡çº¿æ€§å…³ç³&quot;"></p>
<p>注释： 单特征与目标值的关系呈直线关系，两个特征与目标值呈现平面的关系  </p>
</li>
<li><p>非线性关系 </p>
<p><img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%85%B3%E7%B3%BB.png" alt="éžçº¿æ€§å…³ç³&quot;"></p>
</li>
</ul>
<h4 id="线性回归API"><a href="#线性回归API" class="headerlink" title="线性回归API"></a>线性回归API</h4><figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">sklearn</span><span class="selector-class">.linear_model</span><span class="selector-class">.LinearRegression</span>()</span><br><span class="line">	# 基于正规方程</span><br><span class="line">	<span class="selector-tag">LinearRegression</span><span class="selector-class">.coef_</span>：回归系数</span><br></pre></td></tr></table></figure>
<p><strong>小案例：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="comment"># 构造数据集</span></span><br><span class="line"><span class="comment"># 已知学生平时成绩和期末成绩，预测最终成绩</span></span><br><span class="line">x = [[<span class="number">80</span>, <span class="number">86</span>],[<span class="number">82</span>, <span class="number">80</span>],[<span class="number">85</span>, <span class="number">78</span>],[<span class="number">90</span>, <span class="number">90</span>],</span><br><span class="line">     [<span class="number">86</span>, <span class="number">82</span>],[<span class="number">82</span>, <span class="number">90</span>],[<span class="number">78</span>, <span class="number">80</span>],[<span class="number">92</span>, <span class="number">94</span>]]</span><br><span class="line">y = [<span class="number">84.2</span>, <span class="number">80.6</span>, <span class="number">80.1</span>, <span class="number">90</span>, <span class="number">83.2</span>, <span class="number">87.6</span>, <span class="number">79.4</span>, <span class="number">93.4</span>]</span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line"><span class="comment"># 实例化API</span></span><br><span class="line">estimator = LinearRegression()</span><br><span class="line"><span class="comment"># 使用fit方法进行训练</span></span><br><span class="line">estimator.fit(x,y)</span><br><span class="line">print(estimator.coef_)</span><br><span class="line">print(estimator.predict([[<span class="number">100</span>, <span class="number">80</span>]]))</span><br></pre></td></tr></table></figure>
<h4 id="线性回归的损失和优化"><a href="#线性回归的损失和优化" class="headerlink" title="线性回归的损失和优化"></a>线性回归的损失和优化</h4><ol>
<li><p><strong>损失函数</strong></p>
<p>总损失定义为：</p>
<p><img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.png" alt="çº¿æ€§å›žå½’æŸå¤±å‡½æ•°"></p>
<ul>
<li>$y_i$为第i个训练样本的真实值</li>
<li>$h(x_i)$为第i个训练样本特征值组合预测函数</li>
<li>又称<strong>最小二乘法</strong></li>
</ul>
<p>如何去减少这个损失，使我们预测的更加准确些？既然存在了这个损失，我们一直说机器学习有自动学习的功能，在线性回归这里更是能够体现。这里可以通过一些优化方法去优化（其实是数学当中的求导功能）回归的总损失！！！</p>
</li>
<li><p><strong>优化算法</strong></p>
<p><strong>如何去求模型当中的W，使得损失最小？（目的是找到最小损失对应的W值）</strong></p>
<ul>
<li>线性回归经常使用的两种优化算法<ul>
<li>正规方程</li>
<li>梯度下降法</li>
</ul>
</li>
</ul>
</li>
</ol>
<h4 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h4><p> <img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B.png" alt="æ­£è§„æ–¹ç¨‹"></p>
<p> 理解：X为特征值矩阵，y为目标值矩阵。<strong>直接求到最好的结果</strong></p>
<p><strong>缺点：当特征过多过复杂时，求解速度太慢并且得不到结果</strong></p>
<p><strong>正规方程的推导：</strong></p>
<p> 把该损失函数转换成矩阵写法： </p>
<p> <img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1578468182435.png" alt="1578468182435"></p>
<p>其中$y$是真实值矩阵，$X$是特征值矩阵，$w$是权重矩阵</p>
<p>对其求解关于$w$的最小值，起止$y$，$X$ 均已知二次函数直接求导，导数为零的位置，即为最小值。</p>
<p>对损失函数求导：</p>
<p> <img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E6%8E%A8%E5%AF%BC2-3809987.png" alt="image-20190320211408492"></p>
<h4 id="梯度下降-Gradient-Descent"><a href="#梯度下降-Gradient-Descent" class="headerlink" title="梯度下降(Gradient Descent)"></a>梯度下降(Gradient Descent)</h4><p><strong>梯度</strong>：梯度是微积分中一个很重要的概念，<strong>在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率；</strong> <strong>在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向；</strong></p>
<p> <strong>梯度下降（Gradient Descent）公式</strong></p>
<p> <img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%85%AC%E5%BC%8F.png" alt="image-20190709161202497"></p>
<p>  <strong>α是什么含义</strong> </p>
<p> α在梯度下降算法中被称作为<strong>学习率</strong>或者<strong>步长</strong>，意味着我们可以通过α来控制每一步走的距离，以保证不要走太快，错过了最低点。同时也要保证不要走的太慢，导致太阳下山了，还没有走到山下。所以α的选择在梯度下降法中往往是很重要的！α不能太大也不能太小，太小的话，可能导致迟迟走不到最低点，太大的话，会导致错过最低点。</p>
<p> <strong>所以有了梯度下降这样一个优化算法，回归就有了”自动学习”的能力</strong> </p>
<p> <img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BC%98%E5%8C%96%E5%8A%A8%E6%80%81%E5%9B%BE-0716617.gif" alt="çº¿æ€§å›žå½’ä¼˜åŒ–åŠ¨æ€å›¾"> </p>
<h4 id="梯度下降和正规方程的对比"><a href="#梯度下降和正规方程的对比" class="headerlink" title="梯度下降和正规方程的对比"></a>梯度下降和正规方程的对比</h4><div class="table-container">
<table>
<thead>
<tr>
<th>梯度下降</th>
<th>正规方程</th>
</tr>
</thead>
<tbody>
<tr>
<td>需要选择学习率</td>
<td>不需要</td>
</tr>
<tr>
<td>需要迭代求解</td>
<td>一次运算得出</td>
</tr>
<tr>
<td>特征数量较大可以使用</td>
<td>需要计算方程，时间复杂度高O(n3)</td>
</tr>
</tbody>
</table>
</div>
<h4 id="算法选择依据"><a href="#算法选择依据" class="headerlink" title="算法选择依据"></a>算法选择依据</h4><ul>
<li>小规模数据：<ul>
<li>正规方程：<strong>LinearRegression(不能解决拟合问题)</strong></li>
<li>岭回归</li>
</ul>
</li>
<li>大规模数据：<ul>
<li>梯度下降法：<strong>SGDRegressor</strong></li>
</ul>
</li>
</ul>
<h4 id="常见的梯度下降算法"><a href="#常见的梯度下降算法" class="headerlink" title="常见的梯度下降算法"></a>常见的梯度下降算法</h4><figure class="highlight lasso"><table><tr><td class="code"><pre><span class="line">全梯度下降算法(<span class="literal">Full</span> gradient descent）,</span><br><span class="line">随机梯度下降算法（Stochastic gradient descent）,</span><br><span class="line">小批量梯度下降算法（Mini<span class="params">-batch</span> gradient descent）,</span><br><span class="line">随机平均梯度下降算法（Stochastic <span class="keyword">average</span> gradient descent）</span><br></pre></td></tr></table></figure>
<ol>
<li><h5 id="全梯度下降算法（FG）"><a href="#全梯度下降算法（FG）" class="headerlink" title="全梯度下降算法（FG）"></a>全梯度下降算法（FG）</h5><ol>
<li>计算训练集所有样本误差，对其求和再取平均值作为目标函数。</li>
<li>权重向量沿其梯度相反的方向移动，从而使当前目标函数减少得最多。</li>
<li>因为在执行每次更新时，我们需要在整个数据集上计算所有的梯度，所以批梯度下降法的速度会很慢，同时，批梯度下降法无法处理超出内存容量限制的数据集。</li>
<li><p>批梯度下降法同样也不能在线更新模型，即在运行的过程中，不能增加新的样本。</p>
<p><img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/GD%E4%B8%8B%E9%99%8D%E5%85%AC%E5%BC%8F.png" alt="image-20190403165606134"> </p>
</li>
</ol>
</li>
</ol>
<ol>
<li><h5 id="随机梯度下降算法（SG）"><a href="#随机梯度下降算法（SG）" class="headerlink" title="随机梯度下降算法（SG）"></a>随机梯度下降算法（SG）</h5><p>由于FG每迭代更新一次权重都需要计算所有样本误差，而实际问题中经常有上亿的训练样本，故效率偏低，且容易陷入局部最优解，因此提出了随机梯度下降算法。</p>
<p>其每轮计算的目标函数不再是全体样本误差，而仅是单个样本误差，即<strong>每次只代入计算一个样本目标函数的梯度来更新权重，再取下一个样本重复此过程，直到损失函数值停止下降或损失函数值小于某个可以容忍的阈值。</strong></p>
<p>此过程简单，高效，通常可以较好地避免更新迭代收敛到局部最优解。 但是由于，SG每次只使用一个样本迭代，若遇上噪声则容易陷入局部最优解。 </p>
<p> <img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/SG%E4%B8%8B%E9%99%8D%E5%85%AC%E5%BC%8F.png" alt="image-20190403165840513"> </p>
</li>
</ol>
<ol>
<li><h5 id="小批量梯度下降算法（mini-batch）"><a href="#小批量梯度下降算法（mini-batch）" class="headerlink" title="小批量梯度下降算法（mini-batch）"></a>小批量梯度下降算法（mini-batch）</h5><p>小批量梯度下降算法是FG和SG的折中方案,在一定程度上兼顾了以上两种方法的优点。</p>
<p><strong>每次从训练样本集上随机抽取一个小样本集，在抽出来的小样本集上采用FG迭代更新权重。</strong></p>
<p>被抽出的小样本集所含样本点的个数称为batch_size，通常设置为2的幂次方，更有利于GPU加速处理。</p>
<p>特别的，若batch_size=1，则变成了SG；若batch_size=n，则变成了FG.其迭代形式为</p>
<p> <img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/mini-batch%E4%B8%8B%E9%99%8D%E5%85%AC%E5%BC%8F.png" alt="image-20190403170347851"></p>
</li>
</ol>
<ol>
<li><h5 id="随机平均梯度下降算法（SAG）"><a href="#随机平均梯度下降算法（SAG）" class="headerlink" title="随机平均梯度下降算法（SAG）"></a>随机平均梯度下降算法（SAG）</h5><p>在SG方法中，虽然避开了运算成本大的问题，但对于大数据训练而言，SG效果常不尽如人意，因为每一轮梯度更新都完全与上一轮的数据和梯度无关。</p>
<p><strong>随机平均梯度算法克服了这个问题，在内存中为每一个样本都维护一个旧的梯度，随机选择第i个样本来更新此样本的梯度，其他样本的梯度保持不变，然后求得所有梯度的平均值，进而更新了参数。</strong></p>
<p>如此，每一轮更新仅需计算一个样本的梯度，计算成本等同于SG，但收敛速度快得多。</p>
</li>
</ol>
<h4 id="梯度下降法算法比较"><a href="#梯度下降法算法比较" class="headerlink" title="梯度下降法算法比较"></a>梯度下降法算法比较</h4><ul>
<li>全梯度下降算法(Full gradient descent）,</li>
<li>随机梯度下降算法（Stochastic gradient descent）,</li>
<li>小批量梯度下降算法（Mini-batch gradient descent）,</li>
<li>随机平均梯度下降算法（Stochastic average gradient descent）</li>
</ul>
<ol>
<li><p>FG方法由于它每轮更新都要使用全体数据集，故花费的时间成本最多，内存存储最大。</p>
</li>
<li><p>SAG在训练初期表现不佳，优化速度较慢。这是因为我们常将初始梯度设为0，而SAG每轮梯度更新都结合了上一轮梯度值。</p>
</li>
<li>综合考虑迭代次数和运行时间，SG表现性能都很好，能在训练初期快速摆脱初始梯度值，快速将平均损失函数降到很低。但要注意，在使用SG方法时要慎重选择步长，否则容易错过最优解。 </li>
<li>mini-batch结合了SG的“胆大”和FG的“心细”，从6幅图像来看，它的表现也正好居于SG和FG二者之间。在目前的机器学习领域，mini-batch是使用最多的梯度下降算法，正是因为它避开了FG运算效率低成本大和SG收敛效果不稳定的缺点。 </li>
</ol>
<h4 id="基于梯度下降法API"><a href="#基于梯度下降法API" class="headerlink" title="基于梯度下降法API"></a>基于梯度下降法API</h4><p>正规方程</p>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">sklearn.linear_model.<span class="constructor">LinearRegression(<span class="params">fit_intercept</span>=True)</span></span><br><span class="line">	通过正规方程优化</span><br><span class="line">	参数</span><br><span class="line">		fit_intercept：是否计算偏置</span><br><span class="line">	属性</span><br><span class="line">		<span class="module-access"><span class="module"><span class="identifier">LinearRegression</span>.</span></span>coef_：回归系数</span><br><span class="line">		<span class="module-access"><span class="module"><span class="identifier">LinearRegression</span>.</span></span>intercept_：偏置</span><br></pre></td></tr></table></figure>
<p><strong>随机梯度下降法</strong></p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">sklearn.linear_model.SGDRegressor(<span class="attribute">loss</span>=<span class="string">"squared_loss"</span>, <span class="attribute">fit_intercept</span>=<span class="literal">True</span>, learning_rate =<span class="string">'invscaling'</span>, <span class="attribute">eta0</span>=0.01)</span><br><span class="line">	SGDRegressor类实现了随机梯度下降学习，它支持不同的loss函数和正则化惩罚项来拟合线性回归模型。</span><br><span class="line">	参数：</span><br><span class="line">	loss:损失类型</span><br><span class="line">		<span class="attribute">loss</span>=”squared_loss”: 普通最小二乘法</span><br><span class="line">	fit_intercept：是否计算偏置</span><br><span class="line">	learning_rate : string, optional</span><br><span class="line">		学习率填充</span><br><span class="line">		<span class="string">'constant'</span>: eta = eta0</span><br><span class="line">		<span class="string">'optimal'</span>: eta = 1.0 / (alpha * (t + t0) [default]</span><br><span class="line">		<span class="string">'invscaling'</span>: eta = eta0 / pow(t, power_t)</span><br><span class="line">			<span class="attribute">power_t</span>=0.25:存在父类当中</span><br><span class="line">	对于一个常数值的学习率来说，可以使用<span class="attribute">learning_rate</span>=’constant’ ，并使用eta0来指定学习率。</span><br><span class="line">	属性：</span><br><span class="line">		SGDRegressor.coef_：回归系数</span><br><span class="line">		SGDRegressor.intercept_：偏置</span><br></pre></td></tr></table></figure>
<h4 id="欠拟合和过拟合"><a href="#欠拟合和过拟合" class="headerlink" title="欠拟合和过拟合"></a>欠拟合和过拟合</h4><ol>
<li><h5 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h5><ul>
<li>过拟合：一个假设<strong>在训练数据上能够获得比其他假设更好的拟合， 但是在测试数据集上却不能很好地拟合数据</strong>，此时认为这个假设出现了过拟合的现象。(模型过于复杂)</li>
<li>欠拟合：一个假设<strong>在训练数据上不能获得更好的拟合，并且在测试数据集上也不能很好地拟合数据</strong>，此时认为这个假设出现了欠拟合的现象。(模型过于简单)</li>
</ul>
</li>
<li><h5 id="原因以及解决办法"><a href="#原因以及解决办法" class="headerlink" title="原因以及解决办法"></a>原因以及解决办法</h5><ul>
<li><p>欠拟合原因以及解决办法</p>
<p>原因：学习到数据的特征过少</p>
<p>解决办法：继续学习</p>
<p><strong>1）添加其他特征项，</strong>有时候我们模型出现欠拟合的时候是因为特征项不够导致的，可以添加其他特征项来很好地解决。例如，“组合”、“泛化”、“相关性”三类特征是特征添加的重要手段，无论在什么场景，都可以照葫芦画瓢，总会得到意想不到的效果。除上面的特征之外，“上下文特征”、“平台特征”等等，都可以作为特征添加的首选项。</p>
<p><strong>2）添加多项式特征</strong>，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强。</p>
</li>
<li><p>过拟合原因以及解决办法</p>
<p>原因：原始特征过多，存在一些嘈杂特征， 模型过于复杂是因为模型尝试去兼顾各个测试数据点</p>
<p>解决办法：</p>
<p>1）重新清洗数据，导致过拟合的一个原因也有可能是数据不纯导致的，如果出现了过拟合就需要我们重新清洗数据。</p>
<p>2）增大数据的训练量，还有一个原因就是我们用于训练的数据量太小导致的，训练数据占总数据的比例过小。</p>
<p><strong>3）正则化</strong></p>
<p>4）减少特征维度，防止<strong>维灾难</strong></p>
</li>
</ul>
</li>
<li><p>练一练 ：尝试不看后面的字回答出来是解决过拟合的措施，还是解决欠拟合的措施？</p>
<p><strong>过拟合和欠拟合的解决方案</strong></p>
<ul>
<li>获取更多样本 过 </li>
<li>减少特征个数 过 </li>
<li>增加特征个数 欠 </li>
<li>使用高次项的特征 欠 </li>
<li>增加正则化的惩罚系数 过 </li>
<li>减少正则化的惩罚系数 欠   </li>
</ul>
<p><strong>解决过拟合和欠拟合的措施</strong> </p>
<p>过拟合解决方案： </p>
<ul>
<li>获取更多样本</li>
<li>减少特征个数 </li>
<li>增加正则化的惩罚系数</li>
</ul>
<p>欠拟合解决方案:</p>
<ul>
<li>增加特征个数 </li>
<li>使用高次项的特征 </li>
<li>减少正则化的惩罚系数 </li>
</ul>
</li>
</ol>
<h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><p> 在解决回归过拟合中，我们选择正则化。但是对于其他机器学习算法如分类算法来说也会出现这样的问题，除了一些算法本身作用之外（决策树、神经网络），我们更多的也是去自己做特征选择，包括之前说的删除、合并一些特征 。</p>
<p> <img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A8%A1%E5%9E%8B%E5%A4%8D%E6%9D%82.png" alt="æ¨¡åž‹å¤æ‚"></p>
<p>  <strong>如何解决？</strong> </p>
<p> <img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%AD%A3%E5%88%99%E5%8C%96.png" alt="æ­£åˆ™åŒ–"></p>
<p><strong>在学习的时候，数据提供的特征有些影响模型复杂度或者这个特征的数据点异常较多，所以算法在学习的时候尽量减少这个特征的影响（甚至删除某个特征的影响），这就是正则化</strong></p>
<p>注：调整时候，算法并不知道某个特征影响，而是去调整参数得出优化的结果</p>
<h4 id="正则化类别"><a href="#正则化类别" class="headerlink" title="正则化类别"></a>正则化类别</h4><ul>
<li>L2正则化<ul>
<li>作用：可以使得其中一些W的都很小，都接近于0，削弱某个特征的影响</li>
<li>优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象</li>
<li>Ridge回归</li>
</ul>
</li>
<li>L1正则化<ul>
<li>作用：可以使得其中一些W的值直接为0，删除这个特征的影响</li>
<li>LASSO回归</li>
</ul>
</li>
</ul>
<h4 id="正则化线性模型"><a href="#正则化线性模型" class="headerlink" title="正则化线性模型"></a>正则化线性模型</h4><ul>
<li><h5 id="岭回归（Ridge-Regression）"><a href="#岭回归（Ridge-Regression）" class="headerlink" title="岭回归（Ridge Regression）"></a>岭回归（Ridge Regression）</h5><p> 岭回归是线性回归的正则化版本，即在原来的线性回归的 cost function 中添加正则项（regularization term）: </p>
<p> <img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%B2%AD%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B1.png" alt="image-20190404180643584"> </p>
<p> 以达到在拟合数据的同时，使模型权重尽可能小的目的,岭回归代价函数: </p>
<p> <img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%B2%AD%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B2.png" alt="image-20190404180757307"> </p>
<p> α=0：岭回归退化为线性回归。 </p>
</li>
<li><h5 id="Lasso-Regression-Lasso-回归"><a href="#Lasso-Regression-Lasso-回归" class="headerlink" title="Lasso Regression(Lasso 回归)"></a>Lasso Regression(Lasso 回归)</h5><p>Lasso 回归是线性回归的另一种正则化版本，正则项为权值向量的ℓ1范数。</p>
<p>Lasso回归的代价函数 ：</p>
<p> <img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/lasso%E5%9B%9E%E5%BD%921.png" alt="image-20190404181600245"> </p>
<p>【注意 】</p>
<ul>
<li>Lasso Regression 的代价函数在 θi=0处是不可导的.</li>
<li>解决方法：在θi=0处用一个次梯度向量(subgradient vector)代替梯度，如下式</li>
<li><p>Lasso Regression 的次梯度向量</p>
<p><img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/lasso%E5%9B%9E%E5%BD%922.png" alt="image-20190404181709725"></p>
</li>
</ul>
<p>Lasso Regression 有一个很重要的性质是：倾向于完全消除不重要的权重。</p>
<p>例如：当α 取值相对较大时，高阶多项式退化为二次甚至是线性：高阶多项式特征的权重被置为0。</p>
<p>也就是说，Lasso Regression 能够自动进行特征选择，并输出一个稀疏模型（只有少数特征的权重是非零的）。</p>
</li>
</ul>
<ul>
<li><h5 id="Elastic-Net-弹性网络"><a href="#Elastic-Net-弹性网络" class="headerlink" title="Elastic Net (弹性网络)"></a>Elastic Net (弹性网络)</h5><p>弹性网络在岭回归和Lasso回归中进行了折中，通过 <strong>混合比(mix ratio) r</strong> 进行控制， 是前两个内容的综合 ：</p>
<ul>
<li>r=0：弹性网络变为岭回归</li>
<li>r=1：弹性网络便为Lasso回归</li>
</ul>
<p>弹性网络的代价函数 ：</p>
<p> <img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/elastic_net.png" alt="image-20190406110447953"></p>
</li>
</ul>
<ul>
<li><h5 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a>Early Stopping</h5><p>Early Stopping 也是正则化迭代学习的方法之一。</p>
<p>其做法为：在验证错误率达到最小值的时候停止训练。</p>
</li>
<li><h5 id="选择正则化方法"><a href="#选择正则化方法" class="headerlink" title="选择正则化方法"></a>选择正则化方法</h5><p> 一般来说，我们应避免使用<strong>朴素线性回归</strong>，而应对模型进行一定的正则化处理 。</p>
<ul>
<li>常用：岭回归</li>
<li>假设只有少部分特征是有用的：<ul>
<li>弹性网络</li>
<li>Lasso</li>
<li>一般来说，弹性网络的使用更为广泛。因为在特征维度高于训练样本数，或者特征是强相关的情况下，Lasso回归的表现不太稳定。</li>
</ul>
</li>
</ul>
</li>
<li><h5 id="岭回归API"><a href="#岭回归API" class="headerlink" title="岭回归API"></a>岭回归API</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge, ElasticNet, Lasso</span><br></pre></td></tr></table></figure>
<ul>
<li><p><strong>Ridge</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sklearn.linear_model.Ridge(alpha=<span class="number">1.0</span>, fit_intercept=<span class="literal">True</span>,solver=<span class="string">"auto"</span>, normalize=<span class="literal">False</span>)</span><br><span class="line">	具有l2正则化的线性回归</span><br><span class="line">	alpha:正则化力度，也叫 λ</span><br><span class="line">		λ取值：<span class="number">0</span>~<span class="number">1</span> <span class="number">1</span>~<span class="number">10</span></span><br><span class="line">	solver:会根据数据自动选择优化方法</span><br><span class="line">		sag:如果数据集、特征都比较大，选择该随机梯度下降优化</span><br><span class="line">	normalize:数据是否进行标准化</span><br><span class="line">		normalize=<span class="literal">False</span>:可以在fit之前调用preprocessing.StandardScaler标准化数据</span><br><span class="line">	Ridge.coef_:回归权重</span><br><span class="line">	Ridge.intercept_:回归偏置</span><br><span class="line">Ridge方法相当于SGDRegressor(penalty=<span class="string">'l2'</span>, loss=<span class="string">"squared_loss"</span>),</span><br><span class="line">只不过SGDRegressor实现了一个普通的随机梯度下降学习，推荐使用Ridge(实现了SAG)</span><br><span class="line">sklearn.linear_model.RidgeCV(_BaseRidgeCV, RegressorMixin)</span><br><span class="line">	具有l2正则化的线性回归，可以进行交叉验证</span><br><span class="line">	coef_:回归系数</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
</ul>
<h4 id="案例-波士顿房价预测"><a href="#案例-波士顿房价预测" class="headerlink" title="案例-波士顿房价预测"></a>案例-波士顿房价预测</h4><p><strong>数据介绍：</strong></p>
<p><img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%88%BF%E4%BB%B7%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D.png" alt="æˆ¿ä&quot;·æ•°æ®é›†ä&quot;‹ç&quot;"></p>
<p><img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%B1%9E%E6%80%A7.png" alt="å±žæ€§"></p>
<p> <strong>案例分析：</strong></p>
<p>回归当中的数据大小不一致，是否会导致结果影响较大。所以需要做标准化处理。</p>
<ul>
<li>数据分割与标准化处理</li>
<li>回归预测</li>
<li>线性回归的算法效果评估</li>
</ul>
<p><strong>回归性能评估：</strong></p>
<p> 均方误差(Mean Squared Error)MSE)评价机制： </p>
<p><img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1578471165671.png" alt="MSE"></p>
<p>$ y_i$为预测值，$\overline{y}$ 为真实值 </p>
<figure class="highlight dts"><table><tr><td class="code"><pre><span class="line">sklearn.metrics.mean_squared_error(y_true, y_pred)</span><br><span class="line">	均方误差回归损失</span><br><span class="line"><span class="symbol">	y_true:</span>真实值</span><br><span class="line"><span class="symbol">	y_pred:</span>预测值</span><br><span class="line"><span class="symbol">	return:</span>浮点数结果</span><br></pre></td></tr></table></figure>
<p> <strong>代码实现：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression, SGDRegressor, Ridge, RidgeCV</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_model1</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""线性回归：正规方程"""</span></span><br><span class="line">    <span class="comment"># 1.获取数据</span></span><br><span class="line">    boston = load_boston()</span><br><span class="line">    <span class="comment"># print(boston)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2.数据基本处理</span></span><br><span class="line">    <span class="comment"># 2.1分割数据</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target,test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3.特征工程</span></span><br><span class="line">    transfer = StandardScaler()</span><br><span class="line">    x_train = transfer.fit_transform(x_train)</span><br><span class="line">    x_test = transfer.fit_transform(x_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4.机器学习-线性回归</span></span><br><span class="line">    estimator = LinearRegression()</span><br><span class="line">    estimator.fit(x_train, y_train)</span><br><span class="line">    print(<span class="string">"模型的偏置是：&#123;&#125;"</span>.format(estimator.intercept_))</span><br><span class="line">    print(<span class="string">"模型的系数是：&#123;&#125;"</span>.format(estimator.coef_))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5.模型评估</span></span><br><span class="line">    <span class="comment"># 5.1预测值</span></span><br><span class="line">    y_pre = estimator.predict(x_test)</span><br><span class="line">    <span class="comment"># print("预测值是：\n", y_pre)</span></span><br><span class="line">    <span class="comment"># 5.2均方误差</span></span><br><span class="line">    ret = mean_squared_error(y_test, y_pre)</span><br><span class="line">    print(<span class="string">"均方误差是："</span>, ret)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_model2</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""线性回归：梯度下降法"""</span></span><br><span class="line">    <span class="comment"># 1.获取数据</span></span><br><span class="line">    boston = load_boston()</span><br><span class="line">    <span class="comment"># print(boston)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2.数据基本处理</span></span><br><span class="line">    <span class="comment"># 2.1分割数据</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3.特征工程</span></span><br><span class="line">    transfer = StandardScaler()</span><br><span class="line">    x_train = transfer.fit_transform(x_train)</span><br><span class="line">    x_test = transfer.fit_transform(x_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4.机器学习-线性回归</span></span><br><span class="line">    estimator = SGDRegressor(max_iter=<span class="number">1000</span>)</span><br><span class="line">    <span class="comment"># estimator = SGDRegressor(max_iter=1000, learning_rate="constant", eta0=1)</span></span><br><span class="line">    estimator.fit(x_train, y_train)</span><br><span class="line">    print(<span class="string">"模型的偏置是：&#123;&#125;"</span>.format(estimator.intercept_))</span><br><span class="line">    print(<span class="string">"模型的系数是：&#123;&#125;"</span>.format(estimator.coef_))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5.模型评估</span></span><br><span class="line">    <span class="comment"># 5.1预测值</span></span><br><span class="line">    y_pre = estimator.predict(x_test)</span><br><span class="line">    <span class="comment"># print("预测值是：\n", y_pre)</span></span><br><span class="line">    <span class="comment"># 5.2均方误差</span></span><br><span class="line">    ret = mean_squared_error(y_test, y_pre)</span><br><span class="line">    print(<span class="string">"均方误差是："</span>, ret)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_model3</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""线性回归：岭回归"""</span></span><br><span class="line">    <span class="comment"># 1.获取数据</span></span><br><span class="line">    boston = load_boston()</span><br><span class="line">    <span class="comment"># print(boston)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2.数据基本处理</span></span><br><span class="line">    <span class="comment"># 2.1分割数据</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3.特征工程</span></span><br><span class="line">    transfer = StandardScaler()</span><br><span class="line">    x_train = transfer.fit_transform(x_train)</span><br><span class="line">    x_test = transfer.fit_transform(x_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4.机器学习-线性回归</span></span><br><span class="line">    <span class="comment"># estimator = Ridge(alpha=1.0)</span></span><br><span class="line">    estimator = RidgeCV()</span><br><span class="line">    estimator.fit(x_train, y_train)</span><br><span class="line">    print(<span class="string">"模型的偏置是：&#123;&#125;"</span>.format(estimator.intercept_))</span><br><span class="line">    print(<span class="string">"模型的系数是：&#123;&#125;"</span>.format(estimator.coef_))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5.模型评估</span></span><br><span class="line">    <span class="comment"># 5.1预测值</span></span><br><span class="line">    y_pre = estimator.predict(x_test)</span><br><span class="line">    <span class="comment"># print("预测值是：\n", y_pre)</span></span><br><span class="line">    <span class="comment"># 5.2均方误差</span></span><br><span class="line">    ret = mean_squared_error(y_test, y_pre)</span><br><span class="line">    print(<span class="string">"均方误差是："</span>, ret)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    linear_model1()</span><br><span class="line">    linear_model2()</span><br><span class="line">    linear_model3()</span><br></pre></td></tr></table></figure>
<h4 id="模型的保存和加载"><a href="#模型的保存和加载" class="headerlink" title="模型的保存和加载"></a>模型的保存和加载</h4><ul>
<li><h5 id="sklearn模型的保存和加载API"><a href="#sklearn模型的保存和加载API" class="headerlink" title="sklearn模型的保存和加载API"></a>sklearn模型的保存和加载API</h5></li>
<li><h5 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h5><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.externals <span class="keyword">import</span> joblib</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge, RidgeCV</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dump_load</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""模型保存和加载"""</span></span><br><span class="line">    <span class="comment"># 1.获取数据</span></span><br><span class="line">    boston = load_boston()</span><br><span class="line">    <span class="comment"># print(boston)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2.数据基本处理</span></span><br><span class="line">    <span class="comment"># 2.1分割数据</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=<span class="number">22</span>,test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3.特征工程</span></span><br><span class="line">    transfer = StandardScaler()</span><br><span class="line">    x_train = transfer.fit_transform(x_train)</span><br><span class="line">    x_test = transfer.fit_transform(x_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4.机器学习-线性回归</span></span><br><span class="line">    <span class="comment"># 4.1 模型训练</span></span><br><span class="line">    <span class="comment"># estimator = Ridge(alpha=1.0)</span></span><br><span class="line">    <span class="comment"># estimator = RidgeCV()</span></span><br><span class="line">    <span class="comment"># estimator.fit(x_train, y_train)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># # 4.2 模型保存</span></span><br><span class="line">    <span class="comment"># joblib.dump(estimator, "./data/test.pkl")</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4.3 模型加载</span></span><br><span class="line">    estimator = joblib.load(<span class="string">"./data/test.pkl"</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"模型的偏置是：&#123;&#125;"</span>.format(estimator.intercept_))</span><br><span class="line">    print(<span class="string">"模型的系数是：&#123;&#125;"</span>.format(estimator.coef_))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5.模型评估</span></span><br><span class="line">    <span class="comment"># 5.1预测值</span></span><br><span class="line">    y_pre = estimator.predict(x_test)</span><br><span class="line">    <span class="comment"># print("预测值是：\n", y_pre)</span></span><br><span class="line">    <span class="comment"># 5.2均方误差</span></span><br><span class="line">    ret = mean_squared_error(y_test, y_pre)</span><br><span class="line">    print(<span class="string">"均方误差是："</span>, ret)</span><br><span class="line"></span><br><span class="line">dump_load()</span><br></pre></td></tr></table></figure>
</li>
<li><h5 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h5><ul>
<li>1.保存文件，后缀名是**.pkl</li>
<li>2.加载模型是需要通过一个变量进行承接</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>机器学习算法</tag>
        <tag>机器学习</tag>
        <tag>监督学习算法</tag>
        <tag>梯度下降</tag>
        <tag>正规方程</tag>
        <tag>正则化</tag>
        <tag>岭回归</tag>
        <tag>模型保存和加载</tag>
      </tags>
  </entry>
  <entry>
    <title>knn算法</title>
    <url>/2020/01/07/knn%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h4 id="K-近邻算法定义"><a href="#K-近邻算法定义" class="headerlink" title="K-近邻算法定义"></a>K-近邻算法定义</h4><p> K Nearest Neighbor算法又叫KNN算法，这个算法是机器学习里面一个比较经典的算法， 总体来说KNN算法是相对比较容易理解的算法 ， 如果一个样本在特征空间中的<strong>k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别</strong>，则该样本也属于这个类别。 </p>
<a id="more"></a>
<h4 id="KNN算法流程"><a href="#KNN算法流程" class="headerlink" title="KNN算法流程"></a>KNN算法流程</h4><ol>
<li><p>计算已知类别数据集中的点与当前点之间的距离</p>
</li>
<li><p>按距离递增次序排序</p>
</li>
<li><p>选取与当前点距离最小的k个点</p>
</li>
<li><p>统计前k个点所在的类别出现的频率</p>
</li>
<li><p>返回前k个点出现频率最高的类别作为当前点的预测分类</p>
</li>
</ol>
<h4 id="k近邻算法api初步使用"><a href="#k近邻算法api初步使用" class="headerlink" title="k近邻算法api初步使用"></a>k近邻算法api初步使用</h4><p> <strong>机器学习流程 :</strong></p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span>获取数据集</span><br><span class="line"><span class="number">2.</span>数据基本处理</span><br><span class="line"><span class="number">3.</span>特征工程</span><br><span class="line"><span class="number">4.</span>机器学习</span><br><span class="line"><span class="number">5.</span>模型评估</span><br></pre></td></tr></table></figure>
<p><strong>Scikit-learn工具介绍：</strong></p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">Python语言的机器学习工具</span><br><span class="line">	Scikit-learn包括许多知名的机器学习算法的实现</span><br><span class="line">	Scikit-learn文档完善，容易上手，丰富的API</span><br><span class="line">包含内容：</span><br><span class="line">	分类、聚类、回归</span><br><span class="line">	特征工程</span><br><span class="line">	模型选择、调优</span><br><span class="line">优点：</span><br><span class="line">	文档多,且规范,包含的算法多,实现起来容易</span><br><span class="line">目前稳定版本<span class="number">0.19</span><span class="number">.1</span></span><br></pre></td></tr></table></figure>
<p><a href="网址：https://scikit-learn.org/">Scikit-learn官网地址</a></p>
<p>K-近邻算法API**：</p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">sklearn.neighbors.KNeighborsClassifier(n_neighbors=<span class="number">5</span>)</span><br><span class="line">	n_neighbors：<span class="built_in">int</span>,可选（默认= <span class="number">5</span>），k_neighbors查询默认使用的邻居数</span><br></pre></td></tr></table></figure>
<p><strong>小案例</strong>：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.获取数据集</span></span><br><span class="line">x = [[<span class="number">0</span>], [<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]]</span><br><span class="line">y = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line"><span class="comment"># 2.数据基本处理（该案例中省略）</span></span><br><span class="line"><span class="comment"># 3.特征工程（该案例中省略）</span></span><br><span class="line"><span class="comment"># 4.机器学习</span></span><br><span class="line"><span class="comment"># 实例化API</span></span><br><span class="line">estimator = KNeighborsClassifier(n_neighbors=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 使用fit方法进行训练</span></span><br><span class="line">estimator.fit(x, y)</span><br><span class="line"><span class="comment"># 输出预测值</span></span><br><span class="line">print(estimator.predict([[<span class="number">1</span>]]))</span><br><span class="line"><span class="comment"># 5.模型评估（该案例中省略）</span></span><br></pre></td></tr></table></figure>
<h4 id="K值选择"><a href="#K值选择" class="headerlink" title="K值选择"></a>K值选择</h4><p><strong>K值选择:</strong></p>
<p><strong>李航博士的一书「统计学习方法」上所说：</strong></p>
<ol>
<li><p>选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，<strong>K值的减小就意味着整体模型变得复杂，容易发生过拟合；</strong></p>
</li>
<li><p>选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，<strong>与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。</strong></p>
</li>
<li><p>K=N（N为训练样本个数），则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的类，模型过于简单，忽略了训练实例中大量有用信息。</p>
</li>
</ol>
<p>在<strong>实际应用中，K值一般取一个比较小的数值</strong>，例如采用交叉验证法（简单来说，就是把训练数据在分成两组:训练集和验证集）来选择最优的K值。</p>
<h4 id="KNN中K值选择"><a href="#KNN中K值选择" class="headerlink" title="KNN中K值选择"></a>KNN中K值选择</h4><ul>
<li>K值过小<ul>
<li>容易受到异常点的影响</li>
<li>容易过拟合</li>
</ul>
</li>
<li>k值过大：<ul>
<li>受到样本均衡的问题</li>
<li>容易欠拟合</li>
</ul>
</li>
</ul>
<h4 id="kd树"><a href="#kd树" class="headerlink" title="kd树"></a>kd树</h4><p><strong>定义:</strong></p>
<p>根据<strong>KNN</strong>每次需要预测一个点时，我们都需要计算训练数据集里每个点到这个点的距离，然后选出距离最近的k个点进行投票。<strong>当数据集很大时，这个计算成本非常高</strong>。</p>
<p><strong>kd树</strong>：为了避免每次都重新计算一遍距离，算法会把距离信息保存在一棵树里，这样在计算之前从树里查询距离信息，尽量避免重新计算。其基本原理是，<strong>如果A和B距离很远，B和C距离很近，那么A和C的距离也很远</strong>。有了这个信息，就可以在合适的时候跳过距离远的点。</p>
<p> <strong>最近邻域搜索:</strong></p>
<p>  kd树(K-dimension tree)是<strong>一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。</strong>kd树是一种二叉树，表示对k维空间的一个划分，<strong>构造kd树相当于不断地用垂直于坐标轴的超平面将K维空间切分，构成一系列的K维超矩形区域</strong>。kd树的每个结点对应于一个k维超矩形区域。<strong>利用kd树可以省去对大部分数据点的搜索，从而减少搜索的计算量。</strong> </p>
<p> <img src="/2020/01/07/knn%E7%AE%97%E6%B3%95/kd%E6%A0%914.png" alt="image-20190213223817957"> </p>
<p><strong>构造方法:</strong></p>
<ol>
<li><p><strong>构造根结点，使根结点对应于K维空间中包含所有实例点的超矩形区域；</strong></p>
</li>
<li><p><strong>通过递归的方法，不断地对k维空间进行切分，生成子结点。</strong>在超矩形区域上选择一个坐标轴和在此坐标轴上的一个切分点，确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，将当前超矩形区域切分为左右两个子区域（子结点）；这时，实例被分到两个子区域。</p>
</li>
<li><p><strong>上述过程直到子区域内没有实例时终止（终止时的结点为叶结点）</strong>。在此过程中，将实例保存在相应的结点上。</p>
</li>
<li><p>通常，循环的选择坐标轴对空间切分，选择训练实例点在坐标轴上的中位数为切分点，这样得到的kd树是平衡的（平衡二叉树：它是一棵空树，或其左子树和右子树的深度之差的绝对值不超过1，且它的左子树和右子树都是平衡二叉树）。</p>
</li>
</ol>
<p>KD树中每个节点是一个向量，和二叉树按照数的大小划分不同的是，KD树每层需要选定向量中的某一维，然后根据这一维按左小右大的方式划分数据。在构建KD树时，关键需要解决2个问题：</p>
<ol>
<li><p><strong>选择向量的哪一维进行划分；</strong></p>
<p>解决方法可以是随机选择某一维或按顺序选择，但是<strong>更好的方法应该是在数据比较分散的那一维进行划分（分散的程度可以根据方差来衡量）</strong></p>
</li>
<li><p><strong>如何划分数据；</strong></p>
<p>好的划分方法可以使构建的树比较平衡，可以每次选择中位数来进行划分。</p>
<p><strong>kd树的搜索过程:</strong></p>
</li>
</ol>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line"><span class="number">1.</span> 二叉树搜索比较待查询节点和分裂节点的分裂维的值，（小于等于就进入左子树分支，大于就进入右子树分支直到叶子结点）</span><br><span class="line"><span class="number">2.</span> 顺着“搜索路径”找到最近邻的近似点</span><br><span class="line"><span class="number">3.</span> 回溯搜索路径，并判断搜索路径上的结点的其他子结点空间中是否可能有距离查询点更近的数据点，如果有可能，则需要跳到其他子结点空间中去搜索</span><br><span class="line"><span class="number">4.</span> 重复这个过程直到搜索路径为空</span><br></pre></td></tr></table></figure>
<h4 id="scikit-learn中数据集"><a href="#scikit-learn中数据集" class="headerlink" title="scikit-learn中数据集"></a>scikit-learn中数据集</h4><p>scikit-learn数据集API</p>
<figure class="highlight autohotkey"><table><tr><td class="code"><pre><span class="line">sklearn.datasets</span><br><span class="line">	加载获取流行数据集</span><br><span class="line">	datasets.load_*()</span><br><span class="line">		获取小规模数据集，数据包含在datasets里</span><br><span class="line">	datasets.fetch_*(dat<span class="built_in">a_home</span>=None)</span><br><span class="line">		获取大规模数据集，需要从网络上下载，</span><br><span class="line">		函数的第一个参数是dat<span class="built_in">a_home</span>，表示数据集下载的目录,</span><br><span class="line">		默认是 ~/scikit_learn_data/</span><br></pre></td></tr></table></figure>
<p><strong>sklearn小数据集</strong></p>
<ul>
<li><p>sklearn.datasets.load_iris()：加载并返回鸢尾花数据集</p>
<p> <img src="/2020/01/07/knn%E7%AE%97%E6%B3%95/%E9%B8%A2%E5%B0%BE%E8%8A%B1%E6%95%B0%E6%8D%AE%E9%9B%86.png" alt="img"> </p>
</li>
</ul>
<p><strong>sklearn大数据集</strong></p>
<ul>
<li>sklearn.datasets.fetch_20newsgroups(data_home=None,subset=‘train’)<ul>
<li>subset：’train’或者’test’，’all’，可选，选择要加载的数据集。</li>
<li>训练集的“训练”，测试集的“测试”，两者的“全部”</li>
</ul>
</li>
</ul>
<p><strong>sklearn数据集返回值</strong></p>
<ul>
<li>load和fetch返回的数据类型datasets.base.Bunch(字典格式)<ul>
<li>data：特征数据数组，是 [n_samples * n_features] 的二维 numpy.ndarray 数组</li>
<li>target：标签数组，是 n_samples 的一维 numpy.ndarray 数组</li>
<li>DESCR：数据描述</li>
<li>feature_names：特征名,新闻数据，手写数字、回归数据集没有</li>
<li>target_names：标签名</li>
</ul>
</li>
</ul>
<p><strong>查看数据分布</strong></p>
<p>通过创建一些图，以查看不同类别是如何通过特征来区分的。 在理想情况下，标签类将由一个或多个特征对完美分隔。 在现实世界中，这种理想情况很少会发生。</p>
<ul>
<li>seaborn介绍<ul>
<li>Seaborn 是基于 Matplotlib 核心库进行了更高级的 API 封装，可以让你轻松地画出更漂亮的图形。而 Seaborn 的漂亮主要体现在配色更加舒服、以及图形元素的样式更加细腻。</li>
<li>安装 pip3 install seaborn</li>
<li>seaborn.lmplot() 是一个非常有用的方法，它会在绘制二维散点图时，自动完成回归拟合<ul>
<li>sns.lmplot() 里的 x, y 分别代表横纵坐标的列名,</li>
<li>data= 是关联到数据集,</li>
<li>hue=*代表按照 species即花的类别分类显示,</li>
<li>fit_reg=是否进行线性拟合。</li>
</ul>
</li>
<li><a href="http://seaborn.pydata.org/" target="_blank" rel="noopener">参考链接: api链接</a></li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">%matplotlib inline  </span><br><span class="line"><span class="comment"># 内嵌绘图</span></span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把数据转换成dataframe的格式</span></span><br><span class="line">iris_d = pd.DataFrame(iris[<span class="string">'data'</span>], columns = [<span class="string">'Sepal_Length'</span>, <span class="string">'Sepal_Width'</span>, <span class="string">'Petal_Length'</span>, <span class="string">'Petal_Width'</span>])</span><br><span class="line">iris_d[<span class="string">'Species'</span>] = iris.target</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_iris</span><span class="params">(iris, col1, col2)</span>:</span></span><br><span class="line">    sns.lmplot(x = col1, y = col2, data = iris, hue = <span class="string">"Species"</span>, fit_reg = <span class="literal">False</span>)</span><br><span class="line">    plt.xlabel(col1)</span><br><span class="line">    plt.ylabel(col2)</span><br><span class="line">    plt.title(<span class="string">'鸢尾花种类分布图'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">plot_iris(iris_d, <span class="string">'Petal_Width'</span>, <span class="string">'Sepal_Length'</span>)</span><br></pre></td></tr></table></figure>
<p> <img src="/2020/01/07/knn%E7%AE%97%E6%B3%95/%E9%B8%A2%E5%B0%BE%E8%8A%B1%E6%95%B0%E6%8D%AE%E5%88%86%E7%B1%BB%E5%B1%95%E7%A4%BA.png" alt="image-20190225193311519"> </p>
<h4 id="数据集划分"><a href="#数据集划分" class="headerlink" title="数据集划分"></a>数据集划分</h4><p>机器学习一般的数据集会划分为两个部分：</p>
<ul>
<li>训练数据：用于训练，<strong>构建模型</strong></li>
<li>测试数据：在模型检验时使用，用于<strong>评估模型是否有效</strong></li>
</ul>
<p>划分比例：</p>
<ul>
<li>训练集：70% 80% 75%</li>
<li>测试集：30% 20% 25%</li>
</ul>
<p><strong>数据集划分api</strong></p>
<ul>
<li>sklearn.model_selection.train_test_split(arrays, *options)<ul>
<li>参数：<ul>
<li>x 数据集的特征值</li>
<li>y 数据集的标签值</li>
<li>test_size 测试集的大小，一般为float</li>
<li>random_state 随机数种子,不同的种子会造成不同的随机采样结果。相同的种子采样结果相同。</li>
</ul>
</li>
<li>return<ul>
<li>x_train, x_test, y_train, y_test</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment"># 1、获取鸢尾花数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line"><span class="comment"># 对鸢尾花数据集进行分割</span></span><br><span class="line"><span class="comment"># 训练集的特征值x_train 测试集的特征值x_test 训练集的目标值y_train 测试集的目标值y_test</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=<span class="number">22</span>)</span><br><span class="line">print(<span class="string">"x_train:\n"</span>, x_train.shape)</span><br><span class="line"><span class="comment"># 随机数种子</span></span><br><span class="line">x_train1, x_test1, y_train1, y_test1 = train_test_split(iris.data, iris.target, random_state=<span class="number">6</span>)</span><br><span class="line">x_train2, x_test2, y_train2, y_test2 = train_test_split(iris.data, iris.target, random_state=<span class="number">6</span>)</span><br><span class="line">print(<span class="string">"如果随机数种子不一致：\n"</span>, x_train == x_train1)</span><br><span class="line">print(<span class="string">"如果随机数种子一致：\n"</span>, x_train1 == x_train2)</span><br></pre></td></tr></table></figure>
<h4 id="特征工程-特征预处理"><a href="#特征工程-特征预处理" class="headerlink" title="特征工程-特征预处理"></a>特征工程-特征预处理</h4><p><strong>定义：</strong></p>
<figure class="highlight routeros"><table><tr><td class="code"><pre><span class="line">scikit-learn的解释</span><br><span class="line">provides several common utility functions <span class="keyword">and</span> transformer classes <span class="keyword">to</span> change<span class="built_in"> raw </span>feature vectors into a representation that is more suitable <span class="keyword">for</span> the downstream estimators.</span><br><span class="line">翻译过来：通过一些转换函数将特征数据转换成更加适合算法模型的特征数据过程</span><br></pre></td></tr></table></figure>
<p><img src="/2020/01/07/knn%E7%AE%97%E6%B3%95/%E7%89%B9%E5%BE%81%E9%A2%84%E5%A4%84%E7%90%86%E5%9B%BE.png" alt="ç‰¹å¾é¢„å¤„ç†å›¾"></p>
<p> <strong>特征预处理API</strong></p>
<figure class="highlight css"><table><tr><td class="code"><pre><span class="line"><span class="selector-tag">sklearn</span><span class="selector-class">.preprocessing</span></span><br></pre></td></tr></table></figure>
<p><strong>包含内容</strong></p>
<ul>
<li><p>归一化</p>
<ul>
<li>鲁棒性比较差(容易受到异常点的影响)</li>
<li>只适合传统精确小数据场景(以后基本不会用)</li>
</ul>
</li>
<li><p>标准化</p>
<ul>
<li>异常值对我影响小</li>
<li>适合现代嘈杂大数据场景</li>
</ul>
</li>
</ul>
<p><strong>归一化/标准化原因</strong></p>
<p>  特征的<strong>单位或者大小相差较大，或者某特征的方差相比其他的特征要大出几个数量级</strong>，<strong>容易影响（支配）目标结果</strong>，使得一些算法无法学习到其它的特征 。所以 我们需要用到一些方法进行<strong>无量纲化</strong>，<strong>使不同规格的数据转换到同一规格</strong>。</p>
<p><strong>归一化：</strong> 通过对原始数据进行变换把数据映射到(默认为[0,1])之间。</p>
<p> <img src="/2020/01/07/knn%E7%AE%97%E6%B3%95/%E5%BD%92%E4%B8%80%E5%8C%96%E5%85%AC%E5%BC%8F.png" alt="å½’ä¸€åŒ–å…¬å¼"></p>
<p> <strong>API:</strong></p>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">sklearn.preprocessing.MinMaxScaler (feature_range=(<span class="number">0</span>,<span class="number">1</span>)… )</span><br><span class="line">	<span class="module-access"><span class="module"><span class="identifier">MinMaxScalar</span>.</span></span>fit<span class="constructor">_transform(X)</span></span><br><span class="line">		X:numpy <span class="built_in">array</span>格式的数据<span class="literal">[<span class="identifier">n_samples</span>,<span class="identifier">n_features</span>]</span></span><br><span class="line">	返回值：转换后的形状相同的<span class="built_in">array</span></span><br></pre></td></tr></table></figure>
<p><strong>标准化：</strong> 通过对原始数据进行变换把数据变换到均值为0,标准差为1范围内 </p>
<p> <img src="/2020/01/07/knn%E7%AE%97%E6%B3%95/%E6%A0%87%E5%87%86%E5%8C%96%E5%85%AC%E5%BC%8F.png" alt="æ ‡å‡†åŒ–å…¬å¼"></p>
<ul>
<li>对于归一化来说：如果出现异常点，影响了最大值和最小值，那么结果显然会发生改变</li>
<li><p>对于标准化来说：如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小</p>
<p><strong>API:</strong></p>
</li>
</ul>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">sklearn.preprocessing.<span class="constructor">StandardScaler( )</span></span><br><span class="line">	处理之后每列来说所有数据都聚集在均值<span class="number">0</span>附近标准差差为<span class="number">1</span></span><br><span class="line">	<span class="module-access"><span class="module"><span class="identifier">StandardScaler</span>.</span></span>fit<span class="constructor">_transform(X)</span></span><br><span class="line">		X:numpy <span class="built_in">array</span>格式的数据<span class="literal">[<span class="identifier">n_samples</span>,<span class="identifier">n_features</span>]</span></span><br><span class="line">	返回值：转换后的形状相同的<span class="built_in">array</span></span><br></pre></td></tr></table></figure>
<h4 id="交叉验证，网格搜索"><a href="#交叉验证，网格搜索" class="headerlink" title="交叉验证，网格搜索"></a>交叉验证，网格搜索</h4><p><strong>交叉验证：</strong> 将拿到的训练数据，分为训练和验证集。以下图为例：将数据分成4份，其中一份作为验证集。然后经过4次(组)的测试，每次都更换不同的验证集。即得到4组模型的结果，取平均值作为最终结果。又称4折交叉验证。 </p>
<p>数据分为训练集和测试集，但是<strong>为了让从训练得到模型结果更加准确。</strong>做以下处理</p>
<ul>
<li>训练集：训练集+验证集</li>
<li><p>测试集：测试集</p>
<p><img src="/2020/01/07/knn%E7%AE%97%E6%B3%95/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E8%BF%87%E7%A8%8B.png" alt="img"></p>
</li>
</ul>
<p><strong>网格搜索：</strong> 通常情况下，<strong>有很多参数是需要手动指定的（如k-近邻算法中的K值），这种叫超参数</strong>。但是手动过程繁杂，所以需要对模型预设几种超参数组合。<strong>每组超参数都采用交叉验证来进行评估。最后选出最优参数组合建立模型。</strong> </p>
<p><strong>API</strong></p>
<figure class="highlight reasonml"><table><tr><td class="code"><pre><span class="line">sklearn.model_selection.<span class="constructor">GridSearchCV(<span class="params">estimator</span>, <span class="params">param_grid</span>=None,<span class="params">cv</span>=None)</span></span><br><span class="line">	对估计器的指定参数值进行详尽搜索</span><br><span class="line">	estimator：估计器对象</span><br><span class="line">	param_grid：估计器参数(dict)&#123;“n_neighbors”:<span class="literal">[<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>]</span>&#125;</span><br><span class="line">	cv：指定几折交叉验证</span><br><span class="line">	fit：输入训练数据</span><br><span class="line">	score：准确率</span><br><span class="line">结果分析：</span><br><span class="line">	bestscore__:在交叉验证中验证的最好结果</span><br><span class="line">	bestestimator：最好的参数模型</span><br><span class="line">	cvresults:每次交叉验证后的验证集准确率结果和训练集准确率结果</span><br></pre></td></tr></table></figure>
<h4 id="案例1-鸢尾花种类预测"><a href="#案例1-鸢尾花种类预测" class="headerlink" title="案例1-鸢尾花种类预测"></a>案例1-鸢尾花种类预测</h4><p> Iris数据集是常用的分类实验数据集，由Fisher, 1936收集整理。Iris也称鸢尾花卉数据集，是一类多重变量分析的数据集。关于数据集的具体介绍： </p>
<p> <img src="/2020/01/07/knn%E7%AE%97%E6%B3%95/%E9%B8%A2%E5%B0%BE%E8%8A%B1%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BD%BF%E7%94%A8.png" alt="img"></p>
<p> <strong>代码实现：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、获取数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、数据基本处理 -- 划分数据集</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=<span class="number">22</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、特征工程：标准化</span></span><br><span class="line"><span class="comment"># 实例化一个转换器类</span></span><br><span class="line">transfer = StandardScaler()</span><br><span class="line"><span class="comment"># 调用fit_transform</span></span><br><span class="line">x_train = transfer.fit_transform(x_train)</span><br><span class="line">x_test = transfer.transform(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、KNN预估器流程</span></span><br><span class="line"><span class="comment">#  4.1 实例化预估器类</span></span><br><span class="line">estimator = KNeighborsClassifier()</span><br><span class="line"><span class="comment"># 4.2 模型选择与调优——网格搜索和交叉验证</span></span><br><span class="line"><span class="comment"># 准备要调的超参数</span></span><br><span class="line">param_dict = &#123;<span class="string">"n_neighbors"</span>: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>]&#125;</span><br><span class="line">estimator = GridSearchCV(estimator, param_grid=param_dict, cv=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 4.3 fit数据进行训练</span></span><br><span class="line">estimator.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5、评估模型效果</span></span><br><span class="line"><span class="comment"># 方法a：比对预测结果和真实值</span></span><br><span class="line">y_predict = estimator.predict(x_test)</span><br><span class="line">print(<span class="string">"比对预测结果和真实值：\n"</span>, y_predict == y_test)</span><br><span class="line"><span class="comment"># 方法b：直接计算准确率</span></span><br><span class="line">score = estimator.score(x_test, y_test)</span><br><span class="line">print(<span class="string">"直接计算准确率：\n"</span>, score)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后进行评估查看最终选择的结果和交叉验证的结果</span></span><br><span class="line">print(<span class="string">"在交叉验证中验证的最好结果：\n"</span>, estimator.best_score_)</span><br><span class="line">print(<span class="string">"最好的参数模型：\n"</span>, estimator.best_estimator_)</span><br><span class="line">print(<span class="string">"每次交叉验证后的准确率结果：\n"</span>, estimator.cv_results_)</span><br></pre></td></tr></table></figure>
<h4 id="案例2-预测facebook签到位置"><a href="#案例2-预测facebook签到位置" class="headerlink" title="案例2-预测facebook签到位置"></a>案例2-预测facebook签到位置</h4><p> <img src="/2020/01/07/knn%E7%AE%97%E6%B3%95/Facebook%E7%AD%BE%E5%88%B0%E4%BD%8D%E7%BD%AE%E9%A2%84%E6%B5%8B.png" alt="image-20190515115928245"> </p>
<p> 本次比赛的目的是<strong>预测一个人将要签到的地方。</strong> 为了本次比赛，Facebook创建了一个虚拟世界，其中包括<strong>10公里*10公里共100平方公里的约10万个地方。</strong> 对于给定的坐标集，您的任务将<strong>根据用户的位置，准确性和时间戳等预测用户下一次的签到位置。</strong> 数据被制作成类似于来自移动设备的位置数据。 请注意：您只能使用提供的数据进行预测。 </p>
<p><strong>数据集介绍：</strong></p>
<p> <img src="/2020/01/07/knn%E7%AE%97%E6%B3%95/facebook%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D.png" alt="image-20190515120143596"> </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">文件说明 train.csv, test.csv</span><br><span class="line">  row id：签入事件的id</span><br><span class="line">  x y：坐标</span><br><span class="line">  accuracy: 准确度，定位精度</span><br><span class="line">  time: 时间戳</span><br><span class="line">  place_id: 签到的位置，这也是你需要预测的内容</span><br></pre></td></tr></table></figure>
<p><strong>数据来源：</strong> 官网：<a href="https://www.kaggle.com/navoshta/grid-knn/data" target="_blank" rel="noopener">https://www.kaggle.com/navoshta/grid-knn/data</a> </p>
<p><strong>步骤分析：</strong></p>
<ul>
<li>对于数据做一些基本处理（这里所做的一些处理不一定达到很好的效果，我们只是简单尝试，有些特征我们可以根据一些特征选择的方式去做处理）<ul>
<li>1 缩小数据集范围 DataFrame.query()</li>
<li>2 选取有用的时间特征</li>
<li>3 将签到位置少于n个用户的删除</li>
</ul>
</li>
<li>分割数据集</li>
<li>标准化处理</li>
<li>k-近邻预测</li>
</ul>
<p><strong>代码实现：</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.获取数据集</span></span><br><span class="line">facebook = pd.read_csv(<span class="string">"./data/FBlocation/train.csv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.基本数据处理</span></span><br><span class="line"><span class="comment"># 2.1 缩小数据范围</span></span><br><span class="line">facebook_data = facebook.query(<span class="string">"x&gt;2.0 &amp; x&lt;2.2 &amp; y&gt;2.0 &amp; y&lt;2.2"</span>)</span><br><span class="line"><span class="comment"># 2.2 选择时间特征(脱敏)</span></span><br><span class="line">time = pd.to_datetime(facebook_data[<span class="string">"time"</span>], unit=<span class="string">"s"</span>)</span><br><span class="line"><span class="comment"># 转换</span></span><br><span class="line">time = pd.DatetimeIndex(time)</span><br><span class="line">facebook_data[<span class="string">"day"</span>] = time.day</span><br><span class="line">facebook_data[<span class="string">"hour"</span>] = time.hour</span><br><span class="line">facebook_data[<span class="string">"weekday"</span>] = time.weekday</span><br><span class="line"><span class="comment"># 2.3 去掉签到较少的地方</span></span><br><span class="line">place_count = facebook_data.groupby(<span class="string">"place_id"</span>).count()</span><br><span class="line">place_count = place_count[place_count[<span class="string">"row_id"</span>]&gt;<span class="number">3</span>]</span><br><span class="line">facebook_data = facebook_data[facebook_data[<span class="string">"place_id"</span>]].isin(place_count.index)</span><br><span class="line"><span class="comment"># 2.4 确定特征值和目标值</span></span><br><span class="line">x = facebook_data[[<span class="string">"x"</span>, <span class="string">"y"</span>, <span class="string">"accuracy"</span>, <span class="string">"day"</span>, <span class="string">"hour"</span>, <span class="string">"weekday"</span>]]</span><br><span class="line">y = facebook_data[<span class="string">"place_id"</span>]</span><br><span class="line"><span class="comment"># 2.5 分割数据集</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=<span class="number">22</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.特征工程 -- 特征预处理(标准化)</span></span><br><span class="line"><span class="comment"># 3.1 实例化一个转换器</span></span><br><span class="line">transfer = StandardScaler()</span><br><span class="line"><span class="comment"># 3.2 调用fit_transform</span></span><br><span class="line">x_train = transfer.fit_transform(x_train)</span><br><span class="line">x_test = transfer.fit_transform(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.机器学习 -- knn+cv</span></span><br><span class="line"><span class="comment"># 4.1 实例化一个训练器</span></span><br><span class="line">estimator = KNeighborsClassifier()</span><br><span class="line"><span class="comment"># 4.2 交叉验证，网格搜索实现，调用gridsearchCV</span></span><br><span class="line">param_grid = &#123;<span class="string">"n_neighbors"</span>: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>]&#125;</span><br><span class="line"><span class="comment"># n_jobs：指定几个CPU跑程序</span></span><br><span class="line">estimator = GridSearchCV(estimator, param_grid=param_grid, cv=<span class="number">5</span>, n_jobs=<span class="number">8</span>)</span><br><span class="line"><span class="comment"># 4.3 模型训练</span></span><br><span class="line">estimator.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.模型评估</span></span><br><span class="line"><span class="comment"># 5.1 基本评估方式</span></span><br><span class="line">score = estimator.score(x_test, y_test)</span><br><span class="line">print(<span class="string">"最后预测的准确率为:\n"</span>, score)</span><br><span class="line">y_predict = estimator.predict(x_test)</span><br><span class="line">print(<span class="string">"最后的预测值为:\n"</span>, y_predict)</span><br><span class="line">print(<span class="string">"预测值和真实值的对比情况:\n"</span>, y_predict == y_test)</span><br><span class="line"><span class="comment"># 5.2 使用交叉验证后的评估方式</span></span><br><span class="line">print(<span class="string">"在交叉验证中验证的最好结果:\n"</span>, estimator.best_score_)</span><br><span class="line">print(<span class="string">"最好的参数模型:\n"</span>, estimator.best_estimator_)</span><br><span class="line">print(<span class="string">"每次交叉验证后的验证集准确率结果和训练集准确率结果:\n"</span>,estimator.cv_results_)</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>机器学习算法</category>
      </categories>
      <tags>
        <tag>机器学习算法</tag>
        <tag>机器学习</tag>
        <tag>监督学习算法</tag>
        <tag>特征工程</tag>
        <tag>特征工程预处理</tag>
        <tag>交叉验证网格搜索</tag>
      </tags>
  </entry>
  <entry>
    <title>距离度量</title>
    <url>/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/</url>
    <content><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p> 机器学习中常见的距离计算公式 </p>
<a id="more"></a>
<h4 id="欧式距离"><a href="#欧式距离" class="headerlink" title="欧式距离"></a>欧式距离</h4><p>欧氏距离(<strong>Euclidean Distance</strong>)是最容易直观理解的距离度量方法，我们小学、初中和高中接触到的两个点在空间中的距离一般都是指欧氏距离。 </p>
<p><strong>公式</strong>：</p>
<p> <img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/1.4%20%E6%AC%A7%E5%BC%8F%E8%B7%9D%E7%A6%BB2-1965410.png" alt="img"> </p>
<h4 id="曼哈顿距离"><a href="#曼哈顿距离" class="headerlink" title="曼哈顿距离"></a>曼哈顿距离</h4><p>在曼哈顿街区要从一个十字路口开车到另一个十字路口，驾驶距离显然不是两点间的直线距离。这个实际驾驶距离就是“曼哈顿距离”。曼哈顿距离(<strong>Manhattan Distance</strong>)也称为“城市街区距离”(City Block distance)。 </p>
<p> <img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/1.5%20%E6%9B%BC%E5%93%88%E9%A1%BF%E8%B7%9D%E7%A6%BB.png" alt="img"></p>
<p>  <strong>公式</strong>：<img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/1.6%20%E6%9B%BC%E5%93%88%E9%A1%BF%E8%B7%9D%E7%A6%BB.png" alt="img"> </p>
<h4 id="切比雪夫距离"><a href="#切比雪夫距离" class="headerlink" title="切比雪夫距离"></a>切比雪夫距离</h4><p>国际象棋中，国王可以直行、横行、斜行，所以国王走一步可以移动到相邻8个方格中的任意一个。国王从格子(x1,y1)走到格子(x2,y2)最少需要多少步？这个距离就叫切比雪夫距离（<strong>Chebyshev Distance</strong>）。 </p>
<p> <img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/1.7%20%E5%88%87%E6%AF%94%E9%9B%AA%E5%A4%AB%E8%B7%9D%E7%A6%BB.png" alt="img"></p>
<p><strong>公式</strong>：</p>
<p>  <img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/1.8%20%E5%88%87%E6%AF%94%E9%9B%AA%E5%A4%AB%E8%B7%9D%E7%A6%BB.png" alt="img"> </p>
<h4 id="闵可夫斯基距离"><a href="#闵可夫斯基距离" class="headerlink" title="闵可夫斯基距离"></a>闵可夫斯基距离</h4><p>闵氏距离不是一种距离，而是一组距离的定义，是对多个距离度量公式的概括性的表述。</p>
<p>两个n维变量a(x11,x12,…,x1n)与b(x21,x22,…,x2n)间的闵可夫斯基距离（<strong>Minkowski Distance</strong>）定义为：</p>
<p><strong>公式</strong>：</p>
<p> <img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/%E9%97%B5%E5%8F%AF%E5%A4%AB%E6%96%AF%E5%9F%BA%E8%B7%9D%E7%A6%BB.png" alt="image-20190225182628694"></p>
<p>其中p是一个变参数：</p>
<p>当p=1时，就是曼哈顿距离；</p>
<p>当p=2时，就是欧氏距离；</p>
<p>当p→∞时，就是切比雪夫距离。</p>
<p>根据p的不同，闵氏距离可以表示某一类/种的距离。</p>
<p><strong>闵氏距离的缺点：</strong></p>
<ol>
<li><p>将各个分量的量纲(scale)，也就是“单位”相同的看待了;</p>
</li>
<li><p>未考虑各个分量的分布（期望，方差等）可能是不同的。</p>
</li>
</ol>
<h4 id="标准化欧氏距离"><a href="#标准化欧氏距离" class="headerlink" title="标准化欧氏距离"></a>标准化欧氏距离</h4><p>标准化欧氏距离(<strong>Standardized EuclideanDistance</strong>)是针对欧氏距离的缺点而作的一种改进。</p>
<p>思路：既然数据各维分量的分布不一样，那先将各个分量都“标准化”到均值、方差相等。</p>
<p> $S_k$表示各个维度的标准差</p>
<p> <img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/%E6%A0%87%E5%87%86%E5%8C%96%E6%AC%A7%E5%BC%8F%E8%B7%9D%E7%A6%BB.png" alt="image-20190213184012294"> </p>
<h4 id="余弦距离"><a href="#余弦距离" class="headerlink" title="余弦距离"></a>余弦距离</h4><p>几何中，夹角余弦可用来衡量两个向量方向的差异；机器学习中，借用这一概念来衡量样本向量之间的差异。 </p>
<ul>
<li><p>二维空间中向量$A(x_1,y_1)$与向量$B(x_2,y_2)$的夹角余弦公式： </p>
<p><img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/14200251_RZRZ.png" alt="余弦距离"> </p>
</li>
<li><p>两个n维样本点$a(x11,x12,…,x1n)$和$b(x21,x22,…,x2n)$的夹角余弦为： </p>
<p><img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/14200252_SE1M.png" alt="余弦距离"> </p>
<p>即：</p>
<p><img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/14200252_tITR.png" alt="余弦距离"> </p>
</li>
</ul>
<h4 id="汉明距离"><a href="#汉明距离" class="headerlink" title="汉明距离"></a>汉明距离</h4><p>两个等长字符串s1与s2的汉明距离（<strong>Hamming Distance</strong>）为：将其中一个变为另外一个所需要作的最小字符替换次数。 </p>
<p> <img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB.png" alt="image-20190213184508110"> </p>
<figure class="highlight angelscript"><table><tr><td class="code"><pre><span class="line">求下列字符串的汉明距离：</span><br><span class="line"><span class="number">1011101</span>与 <span class="number">1001001</span> <span class="number">1111111</span> <span class="number">1101011</span> <span class="number">7</span><span class="number">-5</span>=<span class="number">2</span></span><br><span class="line"><span class="number">2143896</span>与 <span class="number">2233796</span> <span class="number">1111111</span> <span class="number">1001011</span> <span class="number">7</span><span class="number">-4</span>=<span class="number">3</span> </span><br><span class="line">irie与 rise <span class="number">1111</span> <span class="number">0001</span> <span class="number">4</span><span class="number">-1</span>=<span class="number">3</span></span><br></pre></td></tr></table></figure>
<h4 id="杰卡德距离"><a href="#杰卡德距离" class="headerlink" title="杰卡德距离"></a>杰卡德距离</h4><p>杰卡德相似系数(<strong>Jaccard similarity coefficient</strong>)：两个集合A和B的交集元素在A，B的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号J(A,B)表示： </p>
<p> <img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/%E6%9D%B0%E5%8D%A1%E5%BE%B7%E8%B7%9D%E7%A6%BB1.png" alt="image-20190213184805616"></p>
<p> 杰卡德距离(<strong>Jaccard Distance</strong>)：与杰卡德相似系数相反，用两个集合中不同元素占所有元素的比例来衡量两个集合的区分度： </p>
<p> <img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/%E6%9D%B0%E5%8D%A1%E5%BE%B7%E8%B7%9D%E7%A6%BB2.png" alt="image-20190213184819510"></p>
<h4 id="马氏距离"><a href="#马氏距离" class="headerlink" title="马氏距离"></a>马氏距离</h4><p>下图有两个正态分布图，它们的均值分别为a和b，但方差不一样，则图中的A点离哪个总体更近？或者说A有更大的概率属于谁？显然，A离左边的更近，A属于左边总体的概率更大，尽管A与a的欧式距离远一些。这就是马氏距离的直观解释。 </p>
<p> <img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/%E9%A9%AC%E6%B0%8F%E8%B7%9D%E7%A6%BB.png" alt="image-20190213183101699"></p>
<p>马氏距离是基于样本分布的一种距离。</p>
<p>马氏距离是由印度统计学家马哈拉诺比斯提出的，表示数据的协方差距离。它是一种有效的计算两个位置样本集的相似度的方法。</p>
<p>与欧式距离不同的是，它考虑到各种特性之间的联系，即独立于测量尺度。</p>
<p><strong>马氏距离定义：</strong>设总体G为m维总体（考察m个指标），均值向量为$μ=(μ1，μ2，… …，μm,)$，协方差阵为$∑=(σij)$,</p>
<p>则样本$X=(X1，X2，… …，Xm，)$与总体G的马氏距离定义为：</p>
<p> <img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/%E9%A9%AC%E6%B0%8F%E8%B7%9D%E7%A6%BB1.png" alt="image-20190316193646073"> </p>
<p>马氏距离也可以定义为两个服从同一分布并且其协方差矩阵为∑的随机变量的差异程度：如果协方差矩阵为单位矩阵，马氏距离就简化为欧式距离；如果协方差矩阵为对角矩阵，则其也可称为正规化的欧式距离。 </p>
<p><strong>马氏距离特性：</strong></p>
<ol>
<li><p><strong>量纲无关</strong>，排除变量之间的相关性的干扰；</p>
</li>
<li><p><strong>马氏距离的计算是建立在总体样本的基础上的</strong>，如果拿同样的两个样本，放入两个不同的总体中，最后计算得出的两个样本间的马氏距离通常是不相同的，除非这两个总体的协方差矩阵碰巧相同；</p>
</li>
<li><p>计算马氏距离过程中，<strong>要求总体样本数大于样本的维数</strong>，否则得到的总体样本协方差矩阵逆矩阵不存在，这种情况下，用欧式距离计算即可。</p>
</li>
<li><p>还有一种情况，满足了条件总体样本数大于样本的维数，但是协方差矩阵的逆矩阵仍然不存在，比如三个样本点$（3，4），（5，6），（7，8）$，这种情况是因为这三个样本在其所处的二维空间平面内共线。这种情况下，也采用欧式距离计算。</p>
<p><strong>欧式距离&amp;马氏距离：</strong> </p>
<p><img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/%E6%AC%A7%E5%BC%8Fvs%E9%A9%AC%E6%B0%8F.png" alt="img"> </p>
</li>
</ol>
<p>举例：</p>
<p>已知有两个类G1和G2，比如G1是设备A生产的产品，G2是设备B生产的同类产品。设备A的产品质量高（如考察指标为耐磨度X），其平均耐磨度μ1=80，反映设备精度的方差σ2(1)=0.25;设备B的产品质量稍差，其平均耐磨损度μ2=75，反映设备精度的方差σ2(2)=4.</p>
<p>今有一产品G0，测的耐磨损度X0=78，试判断该产品是哪一台设备生产的？</p>
<p>直观地看，X0与μ1（设备A）的绝对距离近些，按距离最近的原则，是否应把该产品判断设备A生产的？</p>
<p>考虑一种相对于分散性的距离，记X0与G1，G2的相对距离为d1，d2,则：</p>
<p> <img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/%E9%A9%AC%E6%B0%8F%E8%B7%9D%E7%A6%BB%E4%B8%BE%E4%BE%8B1.png" alt="image-20190316192358557"> </p>
<p>因为d2=1.5 &lt; d1=4，按这种距离准则，应判断X0为设备B生产的。</p>
<p>设备B生产的产品质量较分散，出现X0为78的可能性较大；而设备A生产的产品质量较集中，出现X0为78的可能性较小。</p>
<p>这种相对于分散性的距离判断就是马氏距离。</p>
<p><img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/%E9%A9%AC%E6%B0%8F%E8%B7%9D%E7%A6%BB%E4%B8%BE%E4%BE%8B2.png" alt="image-20190316192851778"> </p>
]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>距离计算公式</tag>
      </tags>
  </entry>
  <entry>
    <title>查找算法</title>
    <url>/2020/01/06/%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95/</url>
    <content><![CDATA[<h4 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h4><p>搜索是在一个项目集合中找到一个特定项目的算法过程。搜索通常的答案是真的或假的，因为该项目是否存在。 搜索的八大查找算法：顺序查找，二分查找，插值查找，分块查找， 斐波那契查找，树表查找，哈希查找。</p>
<a id="more"></a>
<h4 id="顺序查找"><a href="#顺序查找" class="headerlink" title="顺序查找"></a>顺序查找</h4><p><strong>1. 定义</strong></p>
<p> 顺序查找是按照序列原有顺序对数组进行遍历比较查询的基本查找算法。  对于任意一个序列以及一个给定的元素，将给定元素与序列中元素依次比较，直到找出与给定关键字相同的元素，或者将序列中的元素与其都比较完为止。 </p>
<p><strong>2. 图示</strong></p>
<p> <img src="/2020/01/06/%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95/1387338-20180530200925684-1605416993.png" alt="img"> </p>
<p><strong>3. 代码实现</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">order_search</span><span class="params">(arr, value)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> arr:</span><br><span class="line">        <span class="keyword">if</span> value == i:</span><br><span class="line">            print(<span class="string">"在列表中找到该值&#123;&#125;"</span>.format(value))</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">"没找到该值"</span>)</span><br><span class="line"></span><br><span class="line">arr = [<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>, <span class="string">"e"</span>, <span class="string">"f"</span>, <span class="string">"g"</span>]</span><br><span class="line">order_search(arr, <span class="string">"f"</span>)</span><br><span class="line">order_search(arr, <span class="string">"d"</span>)</span><br></pre></td></tr></table></figure>
<p><strong>4. 适用</strong></p>
<p> 无序表查找，也就是数据不排序的线性查找，遍历数据元素 </p>
<h4 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h4><p><strong>定义</strong></p>
<p>二分查找又称折半查找，优点是比较次数少，查找速度快，平均性能好；其缺点是要求待查表为有序表，且插入删除困难。因此，折半查找方法适用于不经常变动而查找频繁的有序列表。首先，假设表中元素是按升序排列，将表中间位置记录的关键字与查找关键字比较，如果两者相等，则查找成功；否则利用中间位置记录将表分成前、后两个子表，如果中间位置记录的关键字大于查找关键字，则进一步查找前一子表，否则进一步查找后一子表。重复以上过程，直到找到满足条件的记录，使查找成功，或直到子表不存在为止，此时查找不成功。 </p>
<p><strong>图示</strong></p>
<p> <img src="/2020/01/06/%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95/Binary_search_into_array.png" alt="Binary_search_into_array"></p>
<p><strong>代码实现</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 非递归实现</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search1</span><span class="params">(alist, item)</span>:</span></span><br><span class="line">    first = <span class="number">0</span></span><br><span class="line">    last = len(alist) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> first &lt;= last:</span><br><span class="line">        midpoint = (first + last) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> alist[midpoint] == item:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">elif</span> item &lt; alist[midpoint]:</span><br><span class="line">            last = midpoint - <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            first = midpoint + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">testlist = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">13</span>, <span class="number">17</span>, <span class="number">19</span>, <span class="number">32</span>, <span class="number">42</span>, <span class="number">78</span>]</span><br><span class="line">print(binary_search1(testlist, <span class="number">3</span>))</span><br><span class="line">print(binary_search1(testlist, <span class="number">13</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 递归实现</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search2</span><span class="params">(alist, item)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(alist) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        midpoint = len(alist)//<span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> alist[midpoint]==item:</span><br><span class="line">          <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          <span class="keyword">if</span> item&lt;alist[midpoint]:</span><br><span class="line">            <span class="keyword">return</span> binary_search2(alist[:midpoint],item)</span><br><span class="line">          <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> binary_search2(alist[midpoint+<span class="number">1</span>:],item)</span><br><span class="line"></span><br><span class="line">testlist = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">13</span>, <span class="number">17</span>, <span class="number">19</span>, <span class="number">32</span>, <span class="number">42</span>, <span class="number">78</span>]</span><br><span class="line">print(binary_search2(testlist, <span class="number">3</span>))</span><br><span class="line">print(binary_search2(testlist, <span class="number">13</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># leecode: 二分查找</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">search</span><span class="params">(self, nums, target)</span>:</span></span><br><span class="line">        left = <span class="number">0</span></span><br><span class="line">        right = len(nums)<span class="number">-1</span></span><br><span class="line">        <span class="keyword">while</span>(left&lt;=right):</span><br><span class="line">            midpoint = left + int((right-left)/<span class="number">2</span>)</span><br><span class="line">            <span class="keyword">if</span> nums[midpoint]==target:</span><br><span class="line">                <span class="keyword">return</span> midpoint</span><br><span class="line">            <span class="keyword">if</span> target&lt;nums[midpoint]:</span><br><span class="line">                right = midpoint <span class="number">-1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                left = midpoint+<span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">alist = [<span class="number">-1</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">9</span>,<span class="number">12</span>]</span><br><span class="line">Binary = Solution()</span><br><span class="line">result= Binary.search(nums=alist, target=<span class="number">9</span>)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure>
<p><strong>4. 适用</strong></p>
<p> 有序表查找，查找表中的数据必须按某个主键进行某种排序。  需要频繁执行插入或删除操作的数据集来说，维护有序的排序会带来不小的工作，不建议适用二分查找。</p>
<h4 id="插值查找"><a href="#插值查找" class="headerlink" title="插值查找"></a>插值查找</h4><p><strong>1. 定义</strong></p>
<p> 二分查找每次都是从中间开始，没有考虑数据之间的关系，这是一种比较低效的实现方法，插值查找基于二分查找，改进的中间记录的选取  ，将查找点的选择改进为自适应选择，可以提高查找效率。  </p>
<p><strong>2. 图示</strong></p>
<p>和二分查找一样但是中间值选择方式不同为自适应选择</p>
<p><img src="/2020/01/06/%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95/Binary_search_into_array.png" alt="Binary_search_into_array"></p>
<p><strong>3. 代码实现</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 递归实现</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inter_search</span><span class="params">(alist, item, low)</span>:</span></span><br><span class="line">    hight = len(alist)<span class="number">-1</span></span><br><span class="line">    <span class="keyword">if</span> alist[low]==item:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> alist[hight] == item:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> len(alist) &lt;= <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> alist[hight]-alist[low] != <span class="number">0</span>:</span><br><span class="line">            midpoint = int(low+(hight-low)*(item-alist[low])/(alist[hight]-alist[low]))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            midpoint = len(alist)//<span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> alist[midpoint]==item:</span><br><span class="line">          <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          <span class="keyword">if</span> item&lt;alist[midpoint]:</span><br><span class="line">            <span class="keyword">return</span> inter_search(alist[:midpoint],item, <span class="number">0</span>)</span><br><span class="line">          <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> inter_search(alist[midpoint+<span class="number">1</span>:],item, midpoint+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">testlist = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">13</span>, <span class="number">17</span>, <span class="number">19</span>, <span class="number">32</span>, <span class="number">42</span>, <span class="number">78</span>]</span><br><span class="line">print(inter_search(testlist, <span class="number">3</span>, <span class="number">0</span>))</span><br><span class="line">print(inter_search(testlist, <span class="number">13</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># leecode 二分查找</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">search</span><span class="params">(self, nums, target)</span>:</span></span><br><span class="line">        left = <span class="number">0</span></span><br><span class="line">        right = len(nums)<span class="number">-1</span></span><br><span class="line">        <span class="keyword">while</span>(left&lt;=right):</span><br><span class="line">            <span class="keyword">if</span> nums[right]-nums[left] != <span class="number">0</span>:</span><br><span class="line">                midpoint = left + int((right - left)*(target-nums[left])/(nums[right]-nums[left]))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                midpoint = left + int((right-left)/<span class="number">2</span>)</span><br><span class="line">            <span class="keyword">if</span> nums[midpoint]==target:</span><br><span class="line">                <span class="keyword">return</span> midpoint</span><br><span class="line">            <span class="keyword">if</span> target&lt;nums[midpoint]:</span><br><span class="line">                right = midpoint <span class="number">-1</span></span><br><span class="line">            <span class="keyword">if</span> target&lt;nums[midpoint]:</span><br><span class="line">                right = midpoint <span class="number">-1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                left = midpoint+<span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure>
<p><strong>4. 适用</strong></p>
<p> 表比较大，而关键字分布比较均匀的查找，插值算法的平均性能比折半查找要好很多。反之，线性表中记录分布不均匀，用插值查找未必是很好的选择。 </p>
<h4 id="分块查找"><a href="#分块查找" class="headerlink" title="分块查找"></a>分块查找</h4><p><strong>1. 定义</strong></p>
<p>分块查找，又称为索引顺序查找，吸取了顺序查找和折半查找各自的优点，既有动态结构，又适合快速查找。将查找表分为若干个子块。块内元素可以无序，但块之间是有序的，即第一个块中的最小关键字小于第二个块中的所有记录的关键字，第二个块中的最大关键字小于第三个块中的所有记录的关键字，以此类推。在建立一个索引表，索引表中的每个元素含有各块的最大关键字和各块中第一个元素的地址，索引表按关键字有序排列。</p>
<p><strong>2. 图示</strong></p>
<p> <img src="/2020/01/06/%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95/2-1G01610424A16.png" alt="img"> </p>
<p><strong>3. 代码实现</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p><strong>4. 适用</strong></p>
<p> 要求是顺序表，分块查找又称索引顺序查找，它是顺序查找的一种改进方法，分块查找算法的效率介于顺序查找和二分查找之间。 </p>
<h4 id="斐波那契查找"><a href="#斐波那契查找" class="headerlink" title="斐波那契查找"></a>斐波那契查找</h4><p><strong>1.定义</strong></p>
<p><strong>2.图示</strong></p>
<p><strong>3.代码实现</strong></p>
<p><strong>4. 适用</strong></p>
<h4 id="树表查找"><a href="#树表查找" class="headerlink" title="树表查找"></a>树表查找</h4><p><strong>1.定义</strong></p>
<p><strong>2.图示</strong></p>
<p><strong>3.代码实现</strong></p>
<p><strong>4. 适用</strong></p>
<h4 id="哈希查找"><a href="#哈希查找" class="headerlink" title="哈希查找"></a>哈希查找</h4><p><strong>1.定义</strong></p>
<p><strong>2.图示</strong></p>
<p><strong>3.代码实现</strong></p>
<p><strong>4. 适用</strong></p>
<h4 id="时间复杂度对比"><a href="#时间复杂度对比" class="headerlink" title="时间复杂度对比"></a>时间复杂度对比</h4>]]></content>
      <categories>
        <category>搜索查找算法</category>
      </categories>
      <tags>
        <tag>搜索查找算法</tag>
      </tags>
  </entry>
  <entry>
    <title>基数排序</title>
    <url>/2020/01/05/%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p> 基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序。最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前。 </p>
<a id="more"></a>
<h4 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h4><ul>
<li>取得数组中的最大数，并取得位数；</li>
<li>arr为原始数组，从最低位开始取每个位组成radix数组；</li>
<li>对radix进行计数排序（利用计数排序适用于小范围数的特点）；</li>
</ul>
<h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment">#基于桶排序的基数排序</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">radixSort</span><span class="params">(list)</span>:</span></span><br><span class="line">    d = len(str(max(list)))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(d):<span class="comment">#d轮排序</span></span><br><span class="line">        s=[[] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)]<span class="comment">#因为每一位数字都是0~9，故建立10个桶</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> list:</span><br><span class="line">            s[int(i/(<span class="number">10</span>**k)%<span class="number">10</span>)].append(i)</span><br><span class="line">        list=[j <span class="keyword">for</span> i <span class="keyword">in</span> s <span class="keyword">for</span> j <span class="keyword">in</span> i]</span><br><span class="line">    <span class="keyword">return</span> list</span><br><span class="line"></span><br><span class="line">nums = [<span class="number">5</span>, <span class="number">6</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">8</span>, <span class="number">0</span>, <span class="number">65</span>]</span><br><span class="line">print(radixSort(nums))</span><br></pre></td></tr></table></figure>
<h4 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h4><h4 id="时间复杂度-1"><a href="#时间复杂度-1" class="headerlink" title="时间复杂度"></a>时间复杂度</h4><ul>
<li>最优时间复杂度：O(n*k)</li>
<li>最坏时间复杂度：O(n*k)</li>
<li>稳定性：稳定</li>
</ul>
<h4 id="动画演示"><a href="#动画演示" class="headerlink" title="动画演示"></a>动画演示</h4><p> <img src="/2020/01/05/%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F/849589-20171015232453668-1397662527.gif" alt="img"> </p>
]]></content>
      <categories>
        <category>排序算法</category>
      </categories>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title>桶排序</title>
    <url>/2020/01/05/%E6%A1%B6%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p> 桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。桶排序 (Bucket sort)的工作的原理：假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排）。 </p>
<a id="more"></a>
<ol>
<li><p><strong>什么时候最快</strong></p>
<p>当输入的数据可以均匀的分配到每一个桶中。</p>
</li>
<li><p><strong>什么时候最慢</strong></p>
<p>当输入的数据被分配到了同一个桶中。</p>
</li>
</ol>
<h4 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h4><ul>
<li>设置一个定量的数组当作空桶；</li>
<li>遍历输入数据，并且把数据一个一个放到对应的桶里去；</li>
<li>对每个不是空的桶进行排序；</li>
<li>从不是空的桶里把排好序的数据拼接起来。 </li>
</ul>
<h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bucketSort</span><span class="params">(nums, n)</span>:</span></span><br><span class="line">    <span class="comment"># 选择一个最大的数</span></span><br><span class="line">    <span class="keyword">if</span> max(nums) &gt; len(nums):</span><br><span class="line">        n = max(nums)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        n = len(nums) + <span class="number">1</span></span><br><span class="line">    mid = max(nums) // n</span><br><span class="line">    <span class="keyword">if</span> mid == <span class="number">0</span>:</span><br><span class="line">        mid = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 创建一个元素全是0的列表, 当做桶</span></span><br><span class="line">    bucket = [[] <span class="keyword">for</span> i <span class="keyword">in</span> range(n + <span class="number">1</span>)]</span><br><span class="line">    <span class="comment"># 把所有元素放入桶中</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> nums:</span><br><span class="line">        bucket[int(i / mid)].append(i)</span><br><span class="line">    print(bucket)</span><br><span class="line">    sort_nums = []</span><br><span class="line">    <span class="comment"># 取出桶中的元素</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(bucket)):</span><br><span class="line">        <span class="keyword">if</span> len(bucket[j]) != <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 使用递归继续桶排序</span></span><br><span class="line">            <span class="keyword">if</span> len(bucket[j]) &gt;= <span class="number">2</span> <span class="keyword">and</span> len(set(nums)) != <span class="number">1</span>:</span><br><span class="line">                bucket[j] = bucketSort(bucket[j], n)</span><br><span class="line">            <span class="comment"># 取出排序好的元素</span></span><br><span class="line">            <span class="keyword">for</span> y <span class="keyword">in</span> bucket[j]:</span><br><span class="line">                sort_nums.append(y)</span><br><span class="line">    <span class="keyword">return</span> sort_nums</span><br><span class="line"></span><br><span class="line">nums = [<span class="number">5</span>, <span class="number">6</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">8</span>, <span class="number">0</span>, <span class="number">65</span>]</span><br><span class="line">print(bucketSort(nums, <span class="number">5</span>))</span><br></pre></td></tr></table></figure>
<h4 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h4><ul>
<li>最优时间复杂度：O(n)</li>
<li>最坏时间复杂度：O(n^2)</li>
<li>稳定性：稳定</li>
</ul>
<h4 id="动画演示"><a href="#动画演示" class="headerlink" title="动画演示"></a>动画演示</h4><p> 元素分布在桶中： </p>
<p> <img src="/2020/01/05/%E6%A1%B6%E6%8E%92%E5%BA%8F/Bucket_sort_1.svg_.png" alt="img"></p>
<p>然后，元素在每个桶中排序</p>
<p> <img src="/2020/01/05/%E6%A1%B6%E6%8E%92%E5%BA%8F/Bucket_sort_2.svg_.png" alt="img"></p>
]]></content>
      <categories>
        <category>排序算法</category>
      </categories>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title>计数排序</title>
    <url>/2020/01/05/%E8%AE%A1%E6%95%B0%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p> 计数排序不是基于比较的排序算法，其核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。 </p>
<a id="more"></a>
<h4 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h4><ul>
<li>找出待排序的数组中最大和最小的元素；</li>
<li>统计数组中每个值为i的元素出现的次数，存入数组C的第i项；</li>
<li>对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加）；</li>
<li>反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1。</li>
</ul>
<h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><p><strong>数字排序</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countSort</span><span class="params">(arr)</span>:</span></span><br><span class="line">    <span class="comment"># 输出序列</span></span><br><span class="line">    <span class="keyword">if</span> max(arr) &gt; len(arr):</span><br><span class="line">        n = max(arr) + <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        n = len(arr) + <span class="number">1</span></span><br><span class="line">    output = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line">    <span class="comment"># 计数序列</span></span><br><span class="line">    count = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> arr:</span><br><span class="line">        count[i] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr)):</span><br><span class="line">        count[i + <span class="number">1</span>] += count[i]</span><br><span class="line">    print(count)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr)):</span><br><span class="line">        <span class="comment"># 下标从0开始</span></span><br><span class="line">        output[count[arr[i]] - <span class="number">1</span>] = arr[i]</span><br><span class="line">        count[arr[i]] -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> (output[:len(arr)])</span><br><span class="line"></span><br><span class="line">arr = [<span class="number">5</span>,<span class="number">6</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">8</span>,<span class="number">0</span>]</span><br><span class="line">ans = countSort(arr)</span><br><span class="line">print(ans)</span><br></pre></td></tr></table></figure>
<p><strong>字母排序</strong></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countSort</span><span class="params">(arr)</span>:</span></span><br><span class="line">    n = len(arr)</span><br><span class="line">    <span class="comment"># 输出序列</span></span><br><span class="line">    output = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">256</span>)]</span><br><span class="line">    <span class="comment"># 计数序列</span></span><br><span class="line">    count = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">256</span>)]</span><br><span class="line">    ans = [<span class="string">""</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> arr:</span><br><span class="line">        count[ord(i)] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">256</span>):</span><br><span class="line">        count[i] += count[i - <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="comment"># 下标从0开始</span></span><br><span class="line">        output[count[ord(arr[i])] - <span class="number">1</span>] = arr[i]</span><br><span class="line">        count[ord(arr[i])] -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr)):</span><br><span class="line">        ans[i] = output[i]</span><br><span class="line">    <span class="keyword">return</span> ans</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">arr = <span class="string">"wwwrunoobcom"</span></span><br><span class="line">ans = countSort(arr)</span><br><span class="line">print(<span class="string">"字符数组排序 %s"</span> % (<span class="string">""</span>.join(ans)))</span><br></pre></td></tr></table></figure>
<h4 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h4><ul>
<li>最优时间复杂度：O(n+k)</li>
<li>最坏时间复杂度：O(n+k)</li>
<li>稳定性：稳定</li>
</ul>
<h4 id="动画演示"><a href="#动画演示" class="headerlink" title="动画演示"></a>动画演示</h4><p> <img src="/2020/01/05/%E8%AE%A1%E6%95%B0%E6%8E%92%E5%BA%8F/countingSort.gif" alt="img"> </p>
]]></content>
      <categories>
        <category>排序算法</category>
      </categories>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title>堆排序</title>
    <url>/2020/01/04/%E5%A0%86%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>堆排序（Heapsort）是指利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于（或者大于）它的父节点。堆排序可以说是一种利用堆的概念来排序的选择排序。 </p>
<p><strong>堆</strong>是一种完全二叉树， <strong>堆</strong>有两种类型: <strong>大根堆</strong>  <strong>小根堆</strong>，两种类型的概念如下：<br> 大根堆：每个结点的值都大于或等于左右孩子结点<br> 小根堆：每个结点的值都小于或等于左右孩子结点</p>
 <a id="more"></a>
<p>大根堆 <img src="/2020/01/04/%E5%A0%86%E6%8E%92%E5%BA%8F/3401773-06756e8ab766dcff.webp" alt="大根堆"> </p>
<p> 小根堆<img src="/2020/01/04/%E5%A0%86%E6%8E%92%E5%BA%8F/3401773-cb6fe3ac34af3183.webp" alt="小根堆"></p>
<p> <strong>完全二叉树</strong>：是 一种除了最后一层之外的其他每一层都被完全填充，并且所有结点都保持向左对齐的树 </p>
<h4 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h4><ul>
<li>首先将待排序的数组构造出一个大根堆</li>
<li>取出这个大根堆的堆顶节点(最大值)，与堆的最下最右的元素进行交换，然后把剩下的元素再构造出一个大根堆</li>
<li>重复第二步，直到这个大根堆的长度为1，此时完成排序。</li>
</ul>
<h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">heapify</span><span class="params">(arr, n, i)</span>:</span></span><br><span class="line">    largest = i</span><br><span class="line">    l = <span class="number">2</span> * i + <span class="number">1</span>  <span class="comment"># left = 2*i + 1</span></span><br><span class="line">    r = <span class="number">2</span> * i + <span class="number">2</span>  <span class="comment"># right = 2*i + 2</span></span><br><span class="line">    <span class="keyword">if</span> l &lt; n <span class="keyword">and</span> arr[i] &lt; arr[l]:</span><br><span class="line">        largest = l</span><br><span class="line">    <span class="keyword">if</span> r &lt; n <span class="keyword">and</span> arr[largest] &lt; arr[r]:</span><br><span class="line">        largest = r</span><br><span class="line">    <span class="keyword">if</span> largest != i:</span><br><span class="line">        arr[i], arr[largest] = arr[largest], arr[i]  <span class="comment"># 交换</span></span><br><span class="line">        heapify(arr, n, largest)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">heapSort</span><span class="params">(arr)</span>:</span></span><br><span class="line">    n = len(arr)</span><br><span class="line">    <span class="comment"># Build a maxheap.</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        print(i)</span><br><span class="line">        heapify(arr, n, i)</span><br><span class="line">    <span class="comment"># 一个个交换元素</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n - <span class="number">1</span>, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">        arr[i], arr[<span class="number">0</span>] = arr[<span class="number">0</span>], arr[i]  <span class="comment"># 交换</span></span><br><span class="line">        heapify(arr, i, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">arr = [<span class="number">12</span>, <span class="number">11</span>, <span class="number">13</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line">heapSort(arr)</span><br><span class="line">print(<span class="string">"排序后"</span>, arr)</span><br></pre></td></tr></table></figure>
<h4 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h4><ul>
<li>最优时间复杂度：O(nlogn) （升序排列，序列已经处于升序状态）</li>
<li>最坏时间复杂度：O(nlogn)</li>
<li>稳定性：不稳定</li>
</ul>
<h4 id="动画演示"><a href="#动画演示" class="headerlink" title="动画演示"></a>动画演示</h4><p> <img src="/2020/01/04/%E5%A0%86%E6%8E%92%E5%BA%8F/849589-20171015231308699-356134237.gif" alt="img"> </p>
]]></content>
      <categories>
        <category>排序算法</category>
      </categories>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title>归并排序</title>
    <url>/2020/01/02/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>归并排序是采用分治法的一个非常典型的应用。归并排序的思想就是先递归分解数组，再合并数组。将数组分解最小之后，然后合并两个有序数组，基本思路是比较两个数组的最前面的数，谁小就先取谁，取了后相应的指针就往后移一位。然后再比较，直至一个数组为空，最后把另一个数组的剩余部分复制过来即可<br><a id="more"></a></p>
<h4 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h4><ul>
<li>把长度为n的输入序列分成两个长度为n/2的子序列；</li>
<li>对这两个子序列分别采用归并排序；</li>
<li>将两个排序好的子序列合并成一个最终的排序序列。</li>
</ul>
<h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_sort</span><span class="params">(alist)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(alist) &lt;= <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> alist</span><br><span class="line">    <span class="comment"># 二分分解</span></span><br><span class="line">    num = len(alist)/<span class="number">2</span></span><br><span class="line">    left = merge_sort(alist[:num])</span><br><span class="line">    right = merge_sort(alist[num:])</span><br><span class="line">    <span class="comment"># 合并</span></span><br><span class="line">    <span class="keyword">return</span> merge(left,right)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge</span><span class="params">(left, right)</span>:</span></span><br><span class="line">    <span class="string">'''合并操作，将两个有序数组left[]和right[]合并成一个大的有序数组'''</span></span><br><span class="line">    <span class="comment">#left与right的下标指针</span></span><br><span class="line">    l, r = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">while</span> l&lt;len(left) <span class="keyword">and</span> r&lt;len(right):</span><br><span class="line">        <span class="keyword">if</span> left[l] &lt;= right[r]:</span><br><span class="line">            result.append(left[l])</span><br><span class="line">            l += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            result.append(right[r])</span><br><span class="line">            r += <span class="number">1</span></span><br><span class="line">    result += left[l:]</span><br><span class="line">    result += right[r:]</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">alist = [<span class="number">54</span>,<span class="number">26</span>,<span class="number">93</span>,<span class="number">17</span>,<span class="number">77</span>,<span class="number">31</span>,<span class="number">44</span>,<span class="number">55</span>,<span class="number">20</span>]</span><br><span class="line">sorted_alist = mergeSort(alist)</span><br><span class="line">print(sorted_alist)</span><br></pre></td></tr></table></figure>
<h4 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h4><ul>
<li>最优时间复杂度：O(nlogn)</li>
<li>最坏时间复杂度：O(nlogn)</li>
<li>稳定性：稳定</li>
</ul>
<h4 id="动画演示"><a href="#动画演示" class="headerlink" title="动画演示"></a>动画演示</h4><p> <img src="/2020/01/02/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/Merge-sort-example.gif" alt="Merge-sort-example"> </p>
]]></content>
      <categories>
        <category>排序算法</category>
      </categories>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title>快速排序</title>
    <url>/2020/01/02/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p><strong>快速排序</strong>（英语：Quicksort），又称划分交换排序（partition-exchange sort），通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。  使用分治法来把一个串（list）分为两个子串（sub-lists） 。<br><a id="more"></a></p>
<h4 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h4><ul>
<li>从数列中挑出一个元素，称为”基准”（pivot），</li>
<li>重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区结束之后，该基准就处于数列的中间位置。这个称为分区（partition）操作。</li>
<li>递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。</li>
</ul>
<h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quick_sort</span><span class="params">(alist, start, end)</span>:</span></span><br><span class="line">    <span class="string">"""快速排序"""</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 递归的退出条件</span></span><br><span class="line">    <span class="keyword">if</span> start &gt;= end:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="comment"># 设定起始元素为要寻找位置的基准元素</span></span><br><span class="line">    mid = alist[start]</span><br><span class="line">    <span class="comment"># low为序列左边的由左向右移动的游标</span></span><br><span class="line">    low = start</span><br><span class="line">    <span class="comment"># high为序列右边的由右向左移动的游标</span></span><br><span class="line">    high = end</span><br><span class="line">    <span class="keyword">while</span> low &lt; high:</span><br><span class="line">        <span class="comment"># 如果low与high未重合，high指向的元素不比基准元素小，则high向左移动</span></span><br><span class="line">        <span class="keyword">while</span> low &lt; high <span class="keyword">and</span> alist[high] &gt;= mid:</span><br><span class="line">            high -= <span class="number">1</span></span><br><span class="line">        <span class="comment"># 将high指向的元素放到low的位置上</span></span><br><span class="line">        alist[low] = alist[high]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果low与high未重合，low指向的元素比基准元素小，则low向右移动</span></span><br><span class="line">        <span class="keyword">while</span> low &lt; high <span class="keyword">and</span> alist[low] &lt; mid:</span><br><span class="line">            low += <span class="number">1</span></span><br><span class="line">        <span class="comment"># 将low指向的元素放到high的位置上</span></span><br><span class="line">        alist[high] = alist[low]</span><br><span class="line">    <span class="comment"># 退出循环后，low与high重合，此时所指位置为基准元素的正确位置</span></span><br><span class="line">    <span class="comment"># 将基准元素放到该位置</span></span><br><span class="line">    alist[low] = mid</span><br><span class="line">    <span class="comment"># 对基准元素左边的子序列进行快速排序</span></span><br><span class="line">    quick_sort(alist, start, low<span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># 对基准元素右边的子序列进行快速排序</span></span><br><span class="line">    quick_sort(alist, low+<span class="number">1</span>, end)</span><br><span class="line"></span><br><span class="line">alist = [<span class="number">54</span>,<span class="number">26</span>,<span class="number">93</span>,<span class="number">17</span>,<span class="number">77</span>,<span class="number">31</span>,<span class="number">44</span>,<span class="number">55</span>,<span class="number">20</span>]</span><br><span class="line">quick_sort(alist,<span class="number">0</span>,len(alist)<span class="number">-1</span>)</span><br><span class="line">print(alist)</span><br></pre></td></tr></table></figure>
<h4 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h4><ul>
<li>最优时间复杂度：O(nlogn)</li>
<li>最坏时间复杂度：O(n2)</li>
<li>稳定性：不稳定</li>
</ul>
<h4 id="动画演示"><a href="#动画演示" class="headerlink" title="动画演示"></a>动画演示</h4><p> <img src="/2020/01/02/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/quicksort.gif" alt="quicksort"> </p>
]]></content>
      <categories>
        <category>排序算法</category>
      </categories>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title>希尔排序</title>
    <url>/2020/01/02/%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>希尔排序(Shell Sort)是插入排序的一种。也称<strong>缩小增量排序</strong>，是直接插入排序算法的一种更高效的改进版本。希尔排序是非稳定排序算法。该方法因DL．Shell于1959年提出而得名。 希尔排序是把记录按下标的一定增量分组，对每组使用直接插入排序算法排序；随着增量逐渐减少，每组包含的关键词越来越多，当增量减至1时，整个文件恰被分成一组，算法便终止。</p>
<p>希尔排序的基本思想是：将数组列在一个表中并对列分别进行插入排序，重复这过程，不过每次用更长的列（步长更长了，列数更少了）来进行。最后整个表就只有一列了。将数组转换至表是为了更好地理解这算法，算法本身还是使用数组进行排序。<br><a id="more"></a></p>
<h4 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h4><ul>
<li>选择一个增量序列t1，t2，…，tk，其中ti&gt;tj，tk=1；</li>
<li>按增量序列个数k，对序列进行k 趟排序；</li>
<li>每趟排序，根据对应的增量ti，将待排序列分割成若干长度为m 的子序列，分别对各子表进行直接插入排序。仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。</li>
</ul>
<h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shell_sort</span><span class="params">(alist)</span>:</span></span><br><span class="line">    n = len(alist)</span><br><span class="line">    <span class="comment"># 初始步长</span></span><br><span class="line">    gap = n//<span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> gap &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># 按步长进行插入排序</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(gap, n):</span><br><span class="line">            j = i</span><br><span class="line">            <span class="comment"># 插入排序</span></span><br><span class="line">            <span class="keyword">while</span> j&gt;=gap <span class="keyword">and</span> alist[j-gap] &gt; alist[j]:</span><br><span class="line">                alist[j-gap], alist[j] = alist[j], alist[j-gap]</span><br><span class="line">                j -= gap</span><br><span class="line">        <span class="comment"># 得到新的步长</span></span><br><span class="line">        gap = gap // <span class="number">2</span></span><br><span class="line"></span><br><span class="line">alist = [<span class="number">54</span>,<span class="number">26</span>,<span class="number">93</span>,<span class="number">17</span>,<span class="number">77</span>,<span class="number">31</span>,<span class="number">44</span>,<span class="number">55</span>,<span class="number">20</span>]</span><br><span class="line">shell_sort(alist)</span><br><span class="line">print(alist)</span><br></pre></td></tr></table></figure>
<h4 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h4><ul>
<li>最优时间复杂度：根据步长序列的不同而不同</li>
<li>最坏时间复杂度：O(n2)</li>
<li>稳定性：不稳定</li>
</ul>
<h4 id="动画演示"><a href="#动画演示" class="headerlink" title="动画演示"></a>动画演示</h4><p> <img src="/2020/01/02/%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F/shellsort.gif" alt="shellsort"> </p>
]]></content>
      <categories>
        <category>排序算法</category>
      </categories>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title>插入排序</title>
    <url>/2020/01/02/%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p><strong>插入排序</strong>（英语：Insertion Sort）是一种简单直观的排序算法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。插入排序在实现上，在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。<br> <a id="more"></a></p>
<h4 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h4><ul>
<li>从第一个元素开始，该元素可以认为已经被排序；</li>
<li>取出下一个元素，在已经排序的元素序列中从后向前扫描；</li>
<li>如果该元素（已排序）大于新元素，将该元素移到下一位置；</li>
<li>重复步骤3，直到找到已排序的元素小于或者等于新元素的位置；</li>
<li>将新元素插入到该位置后；</li>
<li>重复步骤2~5。</li>
</ul>
<h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_sort</span><span class="params">(alist)</span>:</span></span><br><span class="line">    <span class="comment"># 从第二个位置，即下标为1的元素开始向前插入</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(alist)):</span><br><span class="line">        <span class="comment"># 从第i个元素开始向前比较，如果小于前一个元素，交换位置</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">if</span> alist[j] &lt; alist[j<span class="number">-1</span>]:</span><br><span class="line">                alist[j], alist[j<span class="number">-1</span>] = alist[j<span class="number">-1</span>], alist[j]</span><br><span class="line"></span><br><span class="line">alist = [<span class="number">54</span>,<span class="number">26</span>,<span class="number">93</span>,<span class="number">17</span>,<span class="number">77</span>,<span class="number">31</span>,<span class="number">44</span>,<span class="number">55</span>,<span class="number">20</span>]</span><br><span class="line">insert_sort(alist)</span><br><span class="line">print(alist)</span><br></pre></td></tr></table></figure>
<h4 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h4><ul>
<li>最优时间复杂度：O(n) （升序排列，序列已经处于升序状态）</li>
<li>最坏时间复杂度：O(n2)</li>
<li>稳定性：稳定</li>
</ul>
<h4 id="动画演示"><a href="#动画演示" class="headerlink" title="动画演示"></a>动画演示</h4><p>​    <img src="/2020/01/02/%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/insert.gif" alt="insert"></p>
]]></content>
      <categories>
        <category>排序算法</category>
      </categories>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title>选择排序</title>
    <url>/2020/01/02/%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p><strong>选择排序（Selection sort）</strong>是一种简单直观的排序算法。它的工作原理如下。首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。</p>
<p>选择排序的主要优点与数据移动有关。如果某个元素位于正确的最终位置上，则它不会被移动。选择排序每次交换一对元素，它们当中至少有一个将被移到其最终位置上，因此对n个元素的表进行排序总共进行至多n-1次交换。在所有的完全依靠交换去移动元素的排序方法中，选择排序属于非常好的一种。<br><a id="more"></a></p>
<h4 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h4><ul>
<li>初始状态：无序区为R[1..n]，有序区为空；</li>
<li>第i趟排序(i=1,2,3…n-1)开始时，当前有序区和无序区分别为R[1..i-1]和R(i..n）。该趟排序从当前无序区中-选出关键字最小的记录 R[k]，将它与无序区的第1个记录R交换，使R[1..i]和R[i+1..n)分别变为记录个数增加1个的新有序区和记录个数减少1个的新无序区；</li>
<li>n-1趟结束，数组有序化了。</li>
</ul>
<h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">selection_sort</span><span class="params">(alist)</span>:</span></span><br><span class="line">    n = len(alist)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-1</span>):</span><br><span class="line">        <span class="comment"># 记录最小位置</span></span><br><span class="line">        min_index = i</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, n):</span><br><span class="line">            <span class="keyword">if</span> alist[j] &lt; alist[min_index]:</span><br><span class="line">                min_index = j</span><br><span class="line">        <span class="comment"># 如果选择出的数据不在正确位置，进行交换</span></span><br><span class="line">        <span class="keyword">if</span> min_index != i:</span><br><span class="line">            alist[i], alist[min_index] = alist[min_index], alist[i]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">alist = [<span class="number">54</span>,<span class="number">226</span>,<span class="number">93</span>,<span class="number">17</span>,<span class="number">77</span>,<span class="number">31</span>,<span class="number">44</span>,<span class="number">55</span>,<span class="number">20</span>]</span><br><span class="line">selection_sort(alist)</span><br><span class="line">print(alist)</span><br></pre></td></tr></table></figure>
<h4 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h4><ul>
<li>最优时间复杂度：O(n2)</li>
<li>最坏时间复杂度：O(n2)</li>
<li>稳定性：不稳定（考虑升序每次选择最大的情况）</li>
</ul>
<h4 id="动画演示"><a href="#动画演示" class="headerlink" title="动画演示"></a>动画演示</h4><p> <img src="/2020/01/02/%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/selection.gif" alt="selection"> </p>
]]></content>
      <categories>
        <category>排序算法</category>
      </categories>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
  <entry>
    <title>冒泡排序</title>
    <url>/2020/01/02/%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/</url>
    <content><![CDATA[<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>冒泡排序是一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。<br> <a id="more"></a></p>
<h4 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h4><ul>
<li>比较相邻的元素。如果第一个比第二个大（升序），就交换它们两个；</li>
<li>对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对，这样在最后的元素应该会是最大的数；</li>
<li>针对所有的元素重复以上的步骤，除了最后一个；</li>
<li>重复步骤1~3，直到排序完成。</li>
</ul>
<h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bubble_sort</span><span class="params">(alist)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(alist)<span class="number">-1</span>, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(j):</span><br><span class="line">            <span class="keyword">if</span> alist[i] &gt; alist[i+<span class="number">1</span>]:</span><br><span class="line">                alist[i], alist[i+<span class="number">1</span>] = alist[i+<span class="number">1</span>], alist[i]</span><br><span class="line"></span><br><span class="line">li = [<span class="number">54</span>, <span class="number">26</span>, <span class="number">93</span>, <span class="number">17</span>, <span class="number">77</span>, <span class="number">31</span>, <span class="number">44</span>, <span class="number">55</span>, <span class="number">20</span>]</span><br><span class="line">bubble_sort(li)</span><br><span class="line">print(li)</span><br></pre></td></tr></table></figure>
<h4 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h4><ul>
<li>最优时间复杂度：O(n) （表示遍历一次发现没有任何可以交换的元素，排序结束。）</li>
<li>最坏时间复杂度：O(n2)</li>
<li>稳定性：稳定</li>
</ul>
<h4 id="动画演示"><a href="#动画演示" class="headerlink" title="动画演示"></a>动画演示</h4><p> <img src="/2020/01/02/%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/bubble-1578136149213.gif" alt="冒泡排序"></p>
]]></content>
      <categories>
        <category>排序算法</category>
      </categories>
      <tags>
        <tag>排序算法</tag>
      </tags>
  </entry>
</search>
