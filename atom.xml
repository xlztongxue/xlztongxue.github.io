<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>小廖子的博客</title>
  
  <subtitle>好记性不如记笔记</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://xiaoliaozi.com/"/>
  <updated>2020-01-09T05:40:27.208Z</updated>
  <id>https://xiaoliaozi.com/</id>
  
  <author>
    <name>小廖子</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>线性回归</title>
    <link href="https://xiaoliaozi.com/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/"/>
    <id>https://xiaoliaozi.com/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/</id>
    <published>2020-01-08T06:23:50.000Z</published>
    <updated>2020-01-09T05:40:27.208Z</updated>
    
    <content type="html"><![CDATA[<h4 id="什么是线性回归"><a href="#什么是线性回归" class="headerlink" title="什么是线性回归"></a>什么是线性回归</h4><p><strong>定义：</strong> 线性回归(Linear regression)是利用<strong>回归方程(函数)</strong>对<strong>一个或多个自变量(特征值)和因变量(目标值)之间</strong>关系进行建模的一种分析方式。 </p><p>特点： 只有一个自变量的情况称为单变量回归，多于一个自变量情况的叫做多元回归 </p><a id="more"></a><p><img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1578465007318.png" alt="1578465007318"></p><p>在机器学习中<strong>特征值与目标值之间建立了一个关系，这个关系可以理解为线性模型</strong>。 </p><h4 id="线性回归的特征与目标的关系"><a href="#线性回归的特征与目标的关系" class="headerlink" title="线性回归的特征与目标的关系"></a>线性回归的特征与目标的关系</h4><p> 线性回归当中主要有两种模型，<strong>一种是线性关系，另一种是非线性关系。</strong>在这里我们只能画一个平面更好去理解，所以都用单个特征或两个特征举例子。 </p><ul><li><p>线性关系</p><ul><li><p>单变量关系：</p><p><img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1578465045629.png" alt="单变量"></p></li></ul></li><li><p>多变量线性关系 </p><p><img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%A4%9A%E5%8F%98%E9%87%8F%E7%BA%BF%E6%80%A7%E5%85%B3%E7%B3%BB.png" alt="å¤šå˜é‡çº¿æ€§å…³ç³&quot;"></p><p>注释： 单特征与目标值的关系呈直线关系，两个特征与目标值呈现平面的关系  </p></li><li><p>非线性关系 </p><p><img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E9%9D%9E%E7%BA%BF%E6%80%A7%E5%85%B3%E7%B3%BB.png" alt="éžçº¿æ€§å…³ç³&quot;"></p></li></ul><h4 id="线性回归API"><a href="#线性回归API" class="headerlink" title="线性回归API"></a>线性回归API</h4><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">sklearn</span><span class="selector-class">.linear_model</span><span class="selector-class">.LinearRegression</span>()</span><br><span class="line"># 基于正规方程</span><br><span class="line"><span class="selector-tag">LinearRegression</span><span class="selector-class">.coef_</span>：回归系数</span><br></pre></td></tr></table></figure><p><strong>小案例：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"><span class="comment"># 构造数据集</span></span><br><span class="line"><span class="comment"># 已知学生平时成绩和期末成绩，预测最终成绩</span></span><br><span class="line">x = [[<span class="number">80</span>, <span class="number">86</span>],[<span class="number">82</span>, <span class="number">80</span>],[<span class="number">85</span>, <span class="number">78</span>],[<span class="number">90</span>, <span class="number">90</span>],</span><br><span class="line">     [<span class="number">86</span>, <span class="number">82</span>],[<span class="number">82</span>, <span class="number">90</span>],[<span class="number">78</span>, <span class="number">80</span>],[<span class="number">92</span>, <span class="number">94</span>]]</span><br><span class="line">y = [<span class="number">84.2</span>, <span class="number">80.6</span>, <span class="number">80.1</span>, <span class="number">90</span>, <span class="number">83.2</span>, <span class="number">87.6</span>, <span class="number">79.4</span>, <span class="number">93.4</span>]</span><br><span class="line"><span class="comment"># 模型训练</span></span><br><span class="line"><span class="comment"># 实例化API</span></span><br><span class="line">estimator = LinearRegression()</span><br><span class="line"><span class="comment"># 使用fit方法进行训练</span></span><br><span class="line">estimator.fit(x,y)</span><br><span class="line">print(estimator.coef_)</span><br><span class="line">print(estimator.predict([[<span class="number">100</span>, <span class="number">80</span>]]))</span><br></pre></td></tr></table></figure><h4 id="线性回归的损失和优化"><a href="#线性回归的损失和优化" class="headerlink" title="线性回归的损失和优化"></a>线性回归的损失和优化</h4><ol><li><p><strong>损失函数</strong></p><p>总损失定义为：</p><p><img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0.png" alt="çº¿æ€§å›žå½’æŸå¤±å‡½æ•°"></p><ul><li>$y_i$为第i个训练样本的真实值</li><li>$h(x_i)$为第i个训练样本特征值组合预测函数</li><li>又称<strong>最小二乘法</strong></li></ul><p>如何去减少这个损失，使我们预测的更加准确些？既然存在了这个损失，我们一直说机器学习有自动学习的功能，在线性回归这里更是能够体现。这里可以通过一些优化方法去优化（其实是数学当中的求导功能）回归的总损失！！！</p></li><li><p><strong>优化算法</strong></p><p><strong>如何去求模型当中的W，使得损失最小？（目的是找到最小损失对应的W值）</strong></p><ul><li>线性回归经常使用的两种优化算法<ul><li>正规方程</li><li>梯度下降法</li></ul></li></ul></li></ol><h4 id="正规方程"><a href="#正规方程" class="headerlink" title="正规方程"></a>正规方程</h4><p> <img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B.png" alt="æ­£è§„æ–¹ç¨‹"></p><p> 理解：X为特征值矩阵，y为目标值矩阵。<strong>直接求到最好的结果</strong></p><p><strong>缺点：当特征过多过复杂时，求解速度太慢并且得不到结果</strong></p><p><strong>正规方程的推导：</strong></p><p> 把该损失函数转换成矩阵写法： </p><p> <img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1578468182435.png" alt="1578468182435"></p><p>其中$y$是真实值矩阵，$X$是特征值矩阵，$w$是权重矩阵</p><p>对其求解关于$w$的最小值，起止$y$，$X$ 均已知二次函数直接求导，导数为零的位置，即为最小值。</p><p>对损失函数求导：</p><p> <img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%AD%A3%E8%A7%84%E6%96%B9%E7%A8%8B%E6%8E%A8%E5%AF%BC2-3809987.png" alt="image-20190320211408492"></p><h4 id="梯度下降-Gradient-Descent"><a href="#梯度下降-Gradient-Descent" class="headerlink" title="梯度下降(Gradient Descent)"></a>梯度下降(Gradient Descent)</h4><p><strong>梯度</strong>：梯度是微积分中一个很重要的概念，<strong>在单变量的函数中，梯度其实就是函数的微分，代表着函数在某个给定点的切线的斜率；</strong> <strong>在多变量函数中，梯度是一个向量，向量有方向，梯度的方向就指出了函数在给定点的上升最快的方向；</strong></p><p> <strong>梯度下降（Gradient Descent）公式</strong></p><p> <img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E5%85%AC%E5%BC%8F.png" alt="image-20190709161202497"></p><p>  <strong>α是什么含义</strong> </p><p> α在梯度下降算法中被称作为<strong>学习率</strong>或者<strong>步长</strong>，意味着我们可以通过α来控制每一步走的距离，以保证不要走太快，错过了最低点。同时也要保证不要走的太慢，导致太阳下山了，还没有走到山下。所以α的选择在梯度下降法中往往是很重要的！α不能太大也不能太小，太小的话，可能导致迟迟走不到最低点，太大的话，会导致错过最低点。</p><p> <strong>所以有了梯度下降这样一个优化算法，回归就有了”自动学习”的能力</strong> </p><p> <img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92%E4%BC%98%E5%8C%96%E5%8A%A8%E6%80%81%E5%9B%BE-0716617.gif" alt="çº¿æ€§å›žå½’ä¼˜åŒ–åŠ¨æ€å›¾"> </p><h4 id="梯度下降和正规方程的对比"><a href="#梯度下降和正规方程的对比" class="headerlink" title="梯度下降和正规方程的对比"></a>梯度下降和正规方程的对比</h4><div class="table-container"><table><thead><tr><th>梯度下降</th><th>正规方程</th></tr></thead><tbody><tr><td>需要选择学习率</td><td>不需要</td></tr><tr><td>需要迭代求解</td><td>一次运算得出</td></tr><tr><td>特征数量较大可以使用</td><td>需要计算方程，时间复杂度高O(n3)</td></tr></tbody></table></div><h4 id="算法选择依据"><a href="#算法选择依据" class="headerlink" title="算法选择依据"></a>算法选择依据</h4><ul><li>小规模数据：<ul><li>正规方程：<strong>LinearRegression(不能解决拟合问题)</strong></li><li>岭回归</li></ul></li><li>大规模数据：<ul><li>梯度下降法：<strong>SGDRegressor</strong></li></ul></li></ul><h4 id="常见的梯度下降算法"><a href="#常见的梯度下降算法" class="headerlink" title="常见的梯度下降算法"></a>常见的梯度下降算法</h4><figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">全梯度下降算法(<span class="literal">Full</span> gradient descent）,</span><br><span class="line">随机梯度下降算法（Stochastic gradient descent）,</span><br><span class="line">小批量梯度下降算法（Mini<span class="params">-batch</span> gradient descent）,</span><br><span class="line">随机平均梯度下降算法（Stochastic <span class="keyword">average</span> gradient descent）</span><br></pre></td></tr></table></figure><ol><li><h5 id="全梯度下降算法（FG）"><a href="#全梯度下降算法（FG）" class="headerlink" title="全梯度下降算法（FG）"></a>全梯度下降算法（FG）</h5><ol><li>计算训练集所有样本误差，对其求和再取平均值作为目标函数。</li><li>权重向量沿其梯度相反的方向移动，从而使当前目标函数减少得最多。</li><li>因为在执行每次更新时，我们需要在整个数据集上计算所有的梯度，所以批梯度下降法的速度会很慢，同时，批梯度下降法无法处理超出内存容量限制的数据集。</li><li><p>批梯度下降法同样也不能在线更新模型，即在运行的过程中，不能增加新的样本。</p><p><img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/GD%E4%B8%8B%E9%99%8D%E5%85%AC%E5%BC%8F.png" alt="image-20190403165606134"> </p></li></ol></li></ol><ol><li><h5 id="随机梯度下降算法（SG）"><a href="#随机梯度下降算法（SG）" class="headerlink" title="随机梯度下降算法（SG）"></a>随机梯度下降算法（SG）</h5><p>由于FG每迭代更新一次权重都需要计算所有样本误差，而实际问题中经常有上亿的训练样本，故效率偏低，且容易陷入局部最优解，因此提出了随机梯度下降算法。</p><p>其每轮计算的目标函数不再是全体样本误差，而仅是单个样本误差，即<strong>每次只代入计算一个样本目标函数的梯度来更新权重，再取下一个样本重复此过程，直到损失函数值停止下降或损失函数值小于某个可以容忍的阈值。</strong></p><p>此过程简单，高效，通常可以较好地避免更新迭代收敛到局部最优解。 但是由于，SG每次只使用一个样本迭代，若遇上噪声则容易陷入局部最优解。 </p><p> <img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/SG%E4%B8%8B%E9%99%8D%E5%85%AC%E5%BC%8F.png" alt="image-20190403165840513"> </p></li></ol><ol><li><h5 id="小批量梯度下降算法（mini-batch）"><a href="#小批量梯度下降算法（mini-batch）" class="headerlink" title="小批量梯度下降算法（mini-batch）"></a>小批量梯度下降算法（mini-batch）</h5><p>小批量梯度下降算法是FG和SG的折中方案,在一定程度上兼顾了以上两种方法的优点。</p><p><strong>每次从训练样本集上随机抽取一个小样本集，在抽出来的小样本集上采用FG迭代更新权重。</strong></p><p>被抽出的小样本集所含样本点的个数称为batch_size，通常设置为2的幂次方，更有利于GPU加速处理。</p><p>特别的，若batch_size=1，则变成了SG；若batch_size=n，则变成了FG.其迭代形式为</p><p> <img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/mini-batch%E4%B8%8B%E9%99%8D%E5%85%AC%E5%BC%8F.png" alt="image-20190403170347851"></p></li></ol><ol><li><h5 id="随机平均梯度下降算法（SAG）"><a href="#随机平均梯度下降算法（SAG）" class="headerlink" title="随机平均梯度下降算法（SAG）"></a>随机平均梯度下降算法（SAG）</h5><p>在SG方法中，虽然避开了运算成本大的问题，但对于大数据训练而言，SG效果常不尽如人意，因为每一轮梯度更新都完全与上一轮的数据和梯度无关。</p><p><strong>随机平均梯度算法克服了这个问题，在内存中为每一个样本都维护一个旧的梯度，随机选择第i个样本来更新此样本的梯度，其他样本的梯度保持不变，然后求得所有梯度的平均值，进而更新了参数。</strong></p><p>如此，每一轮更新仅需计算一个样本的梯度，计算成本等同于SG，但收敛速度快得多。</p></li></ol><h4 id="梯度下降法算法比较"><a href="#梯度下降法算法比较" class="headerlink" title="梯度下降法算法比较"></a>梯度下降法算法比较</h4><ul><li>全梯度下降算法(Full gradient descent）,</li><li>随机梯度下降算法（Stochastic gradient descent）,</li><li>小批量梯度下降算法（Mini-batch gradient descent）,</li><li>随机平均梯度下降算法（Stochastic average gradient descent）</li></ul><ol><li><p>FG方法由于它每轮更新都要使用全体数据集，故花费的时间成本最多，内存存储最大。</p></li><li><p>SAG在训练初期表现不佳，优化速度较慢。这是因为我们常将初始梯度设为0，而SAG每轮梯度更新都结合了上一轮梯度值。</p></li><li>综合考虑迭代次数和运行时间，SG表现性能都很好，能在训练初期快速摆脱初始梯度值，快速将平均损失函数降到很低。但要注意，在使用SG方法时要慎重选择步长，否则容易错过最优解。 </li><li>mini-batch结合了SG的“胆大”和FG的“心细”，从6幅图像来看，它的表现也正好居于SG和FG二者之间。在目前的机器学习领域，mini-batch是使用最多的梯度下降算法，正是因为它避开了FG运算效率低成本大和SG收敛效果不稳定的缺点。 </li></ol><h4 id="基于梯度下降法API"><a href="#基于梯度下降法API" class="headerlink" title="基于梯度下降法API"></a>基于梯度下降法API</h4><p>正规方程</p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sklearn.linear_model.<span class="constructor">LinearRegression(<span class="params">fit_intercept</span>=True)</span></span><br><span class="line">通过正规方程优化</span><br><span class="line">参数</span><br><span class="line">fit_intercept：是否计算偏置</span><br><span class="line">属性</span><br><span class="line"><span class="module-access"><span class="module"><span class="identifier">LinearRegression</span>.</span></span>coef_：回归系数</span><br><span class="line"><span class="module-access"><span class="module"><span class="identifier">LinearRegression</span>.</span></span>intercept_：偏置</span><br></pre></td></tr></table></figure><p><strong>随机梯度下降法</strong></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">sklearn.linear_model.SGDRegressor(<span class="attribute">loss</span>=<span class="string">"squared_loss"</span>, <span class="attribute">fit_intercept</span>=<span class="literal">True</span>, learning_rate =<span class="string">'invscaling'</span>, <span class="attribute">eta0</span>=0.01)</span><br><span class="line">SGDRegressor类实现了随机梯度下降学习，它支持不同的loss函数和正则化惩罚项来拟合线性回归模型。</span><br><span class="line">参数：</span><br><span class="line">loss:损失类型</span><br><span class="line"><span class="attribute">loss</span>=”squared_loss”: 普通最小二乘法</span><br><span class="line">fit_intercept：是否计算偏置</span><br><span class="line">learning_rate : string, optional</span><br><span class="line">学习率填充</span><br><span class="line"><span class="string">'constant'</span>: eta = eta0</span><br><span class="line"><span class="string">'optimal'</span>: eta = 1.0 / (alpha * (t + t0) [default]</span><br><span class="line"><span class="string">'invscaling'</span>: eta = eta0 / pow(t, power_t)</span><br><span class="line"><span class="attribute">power_t</span>=0.25:存在父类当中</span><br><span class="line">对于一个常数值的学习率来说，可以使用<span class="attribute">learning_rate</span>=’constant’ ，并使用eta0来指定学习率。</span><br><span class="line">属性：</span><br><span class="line">SGDRegressor.coef_：回归系数</span><br><span class="line">SGDRegressor.intercept_：偏置</span><br></pre></td></tr></table></figure><h4 id="欠拟合和过拟合"><a href="#欠拟合和过拟合" class="headerlink" title="欠拟合和过拟合"></a>欠拟合和过拟合</h4><ol><li><h5 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h5><ul><li>过拟合：一个假设<strong>在训练数据上能够获得比其他假设更好的拟合， 但是在测试数据集上却不能很好地拟合数据</strong>，此时认为这个假设出现了过拟合的现象。(模型过于复杂)</li><li>欠拟合：一个假设<strong>在训练数据上不能获得更好的拟合，并且在测试数据集上也不能很好地拟合数据</strong>，此时认为这个假设出现了欠拟合的现象。(模型过于简单)</li></ul></li><li><h5 id="原因以及解决办法"><a href="#原因以及解决办法" class="headerlink" title="原因以及解决办法"></a>原因以及解决办法</h5><ul><li>欠拟合原因以及解决办法<ul><li>原因：学习到数据的特征过少</li><li>解决办法：继续学习<ul><li><strong>1）添加其他特征项，</strong>有时候我们模型出现欠拟合的时候是因为特征项不够导致的，可以添加其他特征项来很好地解决。例如，“组合”、“泛化”、“相关性”三类特征是特征添加的重要手段，无论在什么场景，都可以照葫芦画瓢，总会得到意想不到的效果。除上面的特征之外，“上下文特征”、“平台特征”等等，都可以作为特征添加的首选项。</li><li><strong>2）添加多项式特征</strong>，这个在机器学习算法里面用的很普遍，例如将线性模型通过添加二次项或者三次项使模型泛化能力更强。</li></ul></li></ul></li><li>过拟合原因以及解决办法<ul><li>原因：原始特征过多，存在一些嘈杂特征， 模型过于复杂是因为模型尝试去兼顾各个测试数据点</li><li>解决办法：<ul><li>1）重新清洗数据，导致过拟合的一个原因也有可能是数据不纯导致的，如果出现了过拟合就需要我们重新清洗数据。</li><li>2）增大数据的训练量，还有一个原因就是我们用于训练的数据量太小导致的，训练数据占总数据的比例过小。</li><li><strong>3）正则化</strong></li><li>4）减少特征维度，防止<strong>维灾难</strong></li></ul></li></ul></li></ul></li></ol><h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><p> 在解决回归过拟合中，我们选择正则化。但是对于其他机器学习算法如分类算法来说也会出现这样的问题，除了一些算法本身作用之外（决策树、神经网络），我们更多的也是去自己做特征选择，包括之前说的删除、合并一些特征 。</p><p> <img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%A8%A1%E5%9E%8B%E5%A4%8D%E6%9D%82.png" alt="æ¨¡åž‹å¤æ‚"></p><p>  <strong>如何解决？</strong> </p><p> <img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%AD%A3%E5%88%99%E5%8C%96.png" alt="æ­£åˆ™åŒ–"></p><p><strong>在学习的时候，数据提供的特征有些影响模型复杂度或者这个特征的数据点异常较多，所以算法在学习的时候尽量减少这个特征的影响（甚至删除某个特征的影响），这就是正则化</strong></p><p>注：调整时候，算法并不知道某个特征影响，而是去调整参数得出优化的结果</p><h4 id="正则化类别"><a href="#正则化类别" class="headerlink" title="正则化类别"></a>正则化类别</h4><ul><li>L2正则化<ul><li>作用：可以使得其中一些W的都很小，都接近于0，削弱某个特征的影响</li><li>优点：越小的参数说明模型越简单，越简单的模型则越不容易产生过拟合现象</li><li>Ridge回归</li></ul></li><li>L1正则化<ul><li>作用：可以使得其中一些W的值直接为0，删除这个特征的影响</li><li>LASSO回归</li></ul></li></ul><h4 id="正则化线性模型"><a href="#正则化线性模型" class="headerlink" title="正则化线性模型"></a>正则化线性模型</h4><ul><li><h5 id="岭回归（Ridge-Regression）"><a href="#岭回归（Ridge-Regression）" class="headerlink" title="岭回归（Ridge Regression）"></a>岭回归（Ridge Regression）</h5><p> 岭回归是线性回归的正则化版本，即在原来的线性回归的 cost function 中添加正则项（regularization term）: </p><p> <img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%B2%AD%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B1.png" alt="image-20190404180643584"> </p><p> 以达到在拟合数据的同时，使模型权重尽可能小的目的,岭回归代价函数: </p><p> <img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%B2%AD%E5%9B%9E%E5%BD%92%E6%A8%A1%E5%9E%8B2.png" alt="image-20190404180757307"> </p><p> α=0：岭回归退化为线性回归。 </p></li><li><h5 id="Lasso-Regression-Lasso-回归"><a href="#Lasso-Regression-Lasso-回归" class="headerlink" title="Lasso Regression(Lasso 回归)"></a>Lasso Regression(Lasso 回归)</h5><p>Lasso 回归是线性回归的另一种正则化版本，正则项为权值向量的ℓ1范数。</p><p>Lasso回归的代价函数 ：</p><p> <img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/lasso%E5%9B%9E%E5%BD%921.png" alt="image-20190404181600245"> </p><p>【注意 】</p><ul><li>Lasso Regression 的代价函数在 θi=0处是不可导的.</li><li>解决方法：在θi=0处用一个次梯度向量(subgradient vector)代替梯度，如下式</li><li><p>Lasso Regression 的次梯度向量</p><p><img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/lasso%E5%9B%9E%E5%BD%922.png" alt="image-20190404181709725"></p></li></ul><p>Lasso Regression 有一个很重要的性质是：倾向于完全消除不重要的权重。</p><p>例如：当α 取值相对较大时，高阶多项式退化为二次甚至是线性：高阶多项式特征的权重被置为0。</p><p>也就是说，Lasso Regression 能够自动进行特征选择，并输出一个稀疏模型（只有少数特征的权重是非零的）。</p></li></ul><ul><li><h5 id="Elastic-Net-弹性网络"><a href="#Elastic-Net-弹性网络" class="headerlink" title="Elastic Net (弹性网络)"></a>Elastic Net (弹性网络)</h5><p>弹性网络在岭回归和Lasso回归中进行了折中，通过 <strong>混合比(mix ratio) r</strong> 进行控制， 是前两个内容的综合 ：</p><ul><li>r=0：弹性网络变为岭回归</li><li>r=1：弹性网络便为Lasso回归</li></ul><p>弹性网络的代价函数 ：</p><p> <img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/elastic_net.png" alt="image-20190406110447953"></p></li></ul><ul><li><h5 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a>Early Stopping</h5><p>Early Stopping 也是正则化迭代学习的方法之一。</p><p>其做法为：在验证错误率达到最小值的时候停止训练。</p></li><li><h5 id="选择正则化方法"><a href="#选择正则化方法" class="headerlink" title="选择正则化方法"></a>选择正则化方法</h5><p> 一般来说，我们应避免使用<strong>朴素线性回归</strong>，而应对模型进行一定的正则化处理 。</p><ul><li>常用：岭回归</li><li>假设只有少部分特征是有用的：<ul><li>弹性网络</li><li>Lasso</li><li>一般来说，弹性网络的使用更为广泛。因为在特征维度高于训练样本数，或者特征是强相关的情况下，Lasso回归的表现不太稳定。</li></ul></li></ul></li><li><h5 id="岭回归API"><a href="#岭回归API" class="headerlink" title="岭回归API"></a>岭回归API</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge, ElasticNet, Lasso</span><br></pre></td></tr></table></figure><ul><li><p><strong>Ridge</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">sklearn.linear_model.Ridge(alpha=<span class="number">1.0</span>, fit_intercept=<span class="literal">True</span>,solver=<span class="string">"auto"</span>, normalize=<span class="literal">False</span>)</span><br><span class="line">具有l2正则化的线性回归</span><br><span class="line">alpha:正则化力度，也叫 λ</span><br><span class="line">λ取值：<span class="number">0</span>~<span class="number">1</span> <span class="number">1</span>~<span class="number">10</span></span><br><span class="line">solver:会根据数据自动选择优化方法</span><br><span class="line">sag:如果数据集、特征都比较大，选择该随机梯度下降优化</span><br><span class="line">normalize:数据是否进行标准化</span><br><span class="line">normalize=<span class="literal">False</span>:可以在fit之前调用preprocessing.StandardScaler标准化数据</span><br><span class="line">Ridge.coef_:回归权重</span><br><span class="line">Ridge.intercept_:回归偏置</span><br><span class="line">Ridge方法相当于SGDRegressor(penalty=<span class="string">'l2'</span>, loss=<span class="string">"squared_loss"</span>),</span><br><span class="line">只不过SGDRegressor实现了一个普通的随机梯度下降学习，推荐使用Ridge(实现了SAG)</span><br><span class="line">sklearn.linear_model.RidgeCV(_BaseRidgeCV, RegressorMixin)</span><br><span class="line">具有l2正则化的线性回归，可以进行交叉验证</span><br><span class="line">coef_:回归系数</span><br></pre></td></tr></table></figure></li></ul></li></ul><h4 id="案例-波士顿房价预测"><a href="#案例-波士顿房价预测" class="headerlink" title="案例-波士顿房价预测"></a>案例-波士顿房价预测</h4><p><strong>数据介绍：</strong></p><p><img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E6%88%BF%E4%BB%B7%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D.png" alt="æˆ¿ä&quot;·æ•°æ®é›†ä&quot;‹ç&quot;"></p><p><img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/%E5%B1%9E%E6%80%A7.png" alt="å±žæ€§"></p><p> <strong>案例分析：</strong></p><p>回归当中的数据大小不一致，是否会导致结果影响较大。所以需要做标准化处理。</p><ul><li>数据分割与标准化处理</li><li>回归预测</li><li>线性回归的算法效果评估</li></ul><p><strong>回归性能评估：</strong></p><p> 均方误差(Mean Squared Error)MSE)评价机制： </p><p><img src="/2020/01/08/%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92/1578471165671.png" alt="MSE"></p><p>$ y_i$为预测值，$\overline{y}$ 为真实值 </p><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sklearn.metrics.mean_squared_error(y_true, y_pred)</span><br><span class="line">均方误差回归损失</span><br><span class="line"><span class="symbol">y_true:</span>真实值</span><br><span class="line"><span class="symbol">y_pred:</span>预测值</span><br><span class="line"><span class="symbol">return:</span>浮点数结果</span><br></pre></td></tr></table></figure><p> <strong>代码实现：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression, SGDRegressor, Ridge, RidgeCV</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_model1</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""线性回归：正规方程"""</span></span><br><span class="line">    <span class="comment"># 1.获取数据</span></span><br><span class="line">    boston = load_boston()</span><br><span class="line">    <span class="comment"># print(boston)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2.数据基本处理</span></span><br><span class="line">    <span class="comment"># 2.1分割数据</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target,test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3.特征工程</span></span><br><span class="line">    transfer = StandardScaler()</span><br><span class="line">    x_train = transfer.fit_transform(x_train)</span><br><span class="line">    x_test = transfer.fit_transform(x_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4.机器学习-线性回归</span></span><br><span class="line">    estimator = LinearRegression()</span><br><span class="line">    estimator.fit(x_train, y_train)</span><br><span class="line">    print(<span class="string">"模型的偏置是：&#123;&#125;"</span>.format(estimator.intercept_))</span><br><span class="line">    print(<span class="string">"模型的系数是：&#123;&#125;"</span>.format(estimator.coef_))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5.模型评估</span></span><br><span class="line">    <span class="comment"># 5.1预测值</span></span><br><span class="line">    y_pre = estimator.predict(x_test)</span><br><span class="line">    <span class="comment"># print("预测值是：\n", y_pre)</span></span><br><span class="line">    <span class="comment"># 5.2均方误差</span></span><br><span class="line">    ret = mean_squared_error(y_test, y_pre)</span><br><span class="line">    print(<span class="string">"均方误差是："</span>, ret)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_model2</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""线性回归：梯度下降法"""</span></span><br><span class="line">    <span class="comment"># 1.获取数据</span></span><br><span class="line">    boston = load_boston()</span><br><span class="line">    <span class="comment"># print(boston)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2.数据基本处理</span></span><br><span class="line">    <span class="comment"># 2.1分割数据</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3.特征工程</span></span><br><span class="line">    transfer = StandardScaler()</span><br><span class="line">    x_train = transfer.fit_transform(x_train)</span><br><span class="line">    x_test = transfer.fit_transform(x_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4.机器学习-线性回归</span></span><br><span class="line">    estimator = SGDRegressor(max_iter=<span class="number">1000</span>)</span><br><span class="line">    <span class="comment"># estimator = SGDRegressor(max_iter=1000, learning_rate="constant", eta0=1)</span></span><br><span class="line">    estimator.fit(x_train, y_train)</span><br><span class="line">    print(<span class="string">"模型的偏置是：&#123;&#125;"</span>.format(estimator.intercept_))</span><br><span class="line">    print(<span class="string">"模型的系数是：&#123;&#125;"</span>.format(estimator.coef_))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5.模型评估</span></span><br><span class="line">    <span class="comment"># 5.1预测值</span></span><br><span class="line">    y_pre = estimator.predict(x_test)</span><br><span class="line">    <span class="comment"># print("预测值是：\n", y_pre)</span></span><br><span class="line">    <span class="comment"># 5.2均方误差</span></span><br><span class="line">    ret = mean_squared_error(y_test, y_pre)</span><br><span class="line">    print(<span class="string">"均方误差是："</span>, ret)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">linear_model3</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""线性回归：岭回归"""</span></span><br><span class="line">    <span class="comment"># 1.获取数据</span></span><br><span class="line">    boston = load_boston()</span><br><span class="line">    <span class="comment"># print(boston)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2.数据基本处理</span></span><br><span class="line">    <span class="comment"># 2.1分割数据</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3.特征工程</span></span><br><span class="line">    transfer = StandardScaler()</span><br><span class="line">    x_train = transfer.fit_transform(x_train)</span><br><span class="line">    x_test = transfer.fit_transform(x_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4.机器学习-线性回归</span></span><br><span class="line">    <span class="comment"># estimator = Ridge(alpha=1.0)</span></span><br><span class="line">    estimator = RidgeCV()</span><br><span class="line">    estimator.fit(x_train, y_train)</span><br><span class="line">    print(<span class="string">"模型的偏置是：&#123;&#125;"</span>.format(estimator.intercept_))</span><br><span class="line">    print(<span class="string">"模型的系数是：&#123;&#125;"</span>.format(estimator.coef_))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5.模型评估</span></span><br><span class="line">    <span class="comment"># 5.1预测值</span></span><br><span class="line">    y_pre = estimator.predict(x_test)</span><br><span class="line">    <span class="comment"># print("预测值是：\n", y_pre)</span></span><br><span class="line">    <span class="comment"># 5.2均方误差</span></span><br><span class="line">    ret = mean_squared_error(y_test, y_pre)</span><br><span class="line">    print(<span class="string">"均方误差是："</span>, ret)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    linear_model1()</span><br><span class="line">    linear_model2()</span><br><span class="line">    linear_model3()</span><br></pre></td></tr></table></figure><h4 id="模型的保存和加载"><a href="#模型的保存和加载" class="headerlink" title="模型的保存和加载"></a>模型的保存和加载</h4><ul><li><h5 id="sklearn模型的保存和加载API"><a href="#sklearn模型的保存和加载API" class="headerlink" title="sklearn模型的保存和加载API"></a>sklearn模型的保存和加载API</h5></li><li><h5 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_boston</span><br><span class="line"><span class="keyword">from</span> sklearn.externals <span class="keyword">import</span> joblib</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge, RidgeCV</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dump_load</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""模型保存和加载"""</span></span><br><span class="line">    <span class="comment"># 1.获取数据</span></span><br><span class="line">    boston = load_boston()</span><br><span class="line">    <span class="comment"># print(boston)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2.数据基本处理</span></span><br><span class="line">    <span class="comment"># 2.1分割数据</span></span><br><span class="line">    x_train, x_test, y_train, y_test = train_test_split(boston.data, boston.target, random_state=<span class="number">22</span>,test_size=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3.特征工程</span></span><br><span class="line">    transfer = StandardScaler()</span><br><span class="line">    x_train = transfer.fit_transform(x_train)</span><br><span class="line">    x_test = transfer.fit_transform(x_test)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4.机器学习-线性回归</span></span><br><span class="line">    <span class="comment"># 4.1 模型训练</span></span><br><span class="line">    <span class="comment"># estimator = Ridge(alpha=1.0)</span></span><br><span class="line">    <span class="comment"># estimator = RidgeCV()</span></span><br><span class="line">    <span class="comment"># estimator.fit(x_train, y_train)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># # 4.2 模型保存</span></span><br><span class="line">    <span class="comment"># joblib.dump(estimator, "./data/test.pkl")</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 4.3 模型加载</span></span><br><span class="line">    estimator = joblib.load(<span class="string">"./data/test.pkl"</span>)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"模型的偏置是：&#123;&#125;"</span>.format(estimator.intercept_))</span><br><span class="line">    print(<span class="string">"模型的系数是：&#123;&#125;"</span>.format(estimator.coef_))</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 5.模型评估</span></span><br><span class="line">    <span class="comment"># 5.1预测值</span></span><br><span class="line">    y_pre = estimator.predict(x_test)</span><br><span class="line">    <span class="comment"># print("预测值是：\n", y_pre)</span></span><br><span class="line">    <span class="comment"># 5.2均方误差</span></span><br><span class="line">    ret = mean_squared_error(y_test, y_pre)</span><br><span class="line">    print(<span class="string">"均方误差是："</span>, ret)</span><br><span class="line"></span><br><span class="line">dump_load()</span><br></pre></td></tr></table></figure></li><li><h5 id="注意："><a href="#注意：" class="headerlink" title="注意："></a>注意：</h5><ul><li>1.保存文件，后缀名是**.pkl</li><li>2.加载模型是需要通过一个变量进行承接</li></ul></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;什么是线性回归&quot;&gt;&lt;a href=&quot;#什么是线性回归&quot; class=&quot;headerlink&quot; title=&quot;什么是线性回归&quot;&gt;&lt;/a&gt;什么是线性回归&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;定义：&lt;/strong&gt; 线性回归(Linear regression)是利用&lt;strong&gt;回归方程(函数)&lt;/strong&gt;对&lt;strong&gt;一个或多个自变量(特征值)和因变量(目标值)之间&lt;/strong&gt;关系进行建模的一种分析方式。 &lt;/p&gt;
&lt;p&gt;特点： 只有一个自变量的情况称为单变量回归，多于一个自变量情况的叫做多元回归 &lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习算法" scheme="https://xiaoliaozi.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="机器学习算法" scheme="https://xiaoliaozi.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
      <category term="人工智能" scheme="https://xiaoliaozi.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="机器学习" scheme="https://xiaoliaozi.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>knn算法</title>
    <link href="https://xiaoliaozi.com/2020/01/07/knn%E7%AE%97%E6%B3%95/"/>
    <id>https://xiaoliaozi.com/2020/01/07/knn%E7%AE%97%E6%B3%95/</id>
    <published>2020-01-07T12:33:06.000Z</published>
    <updated>2020-01-09T05:32:43.875Z</updated>
    
    <content type="html"><![CDATA[<h4 id="K-近邻算法定义"><a href="#K-近邻算法定义" class="headerlink" title="K-近邻算法定义"></a>K-近邻算法定义</h4><p> K Nearest Neighbor算法又叫KNN算法，这个算法是机器学习里面一个比较经典的算法， 总体来说KNN算法是相对比较容易理解的算法 ， 如果一个样本在特征空间中的<strong>k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别</strong>，则该样本也属于这个类别。 </p><a id="more"></a><h4 id="KNN算法流程"><a href="#KNN算法流程" class="headerlink" title="KNN算法流程"></a>KNN算法流程</h4><ol><li><p>计算已知类别数据集中的点与当前点之间的距离</p></li><li><p>按距离递增次序排序</p></li><li><p>选取与当前点距离最小的k个点</p></li><li><p>统计前k个点所在的类别出现的频率</p></li><li><p>返回前k个点出现频率最高的类别作为当前点的预测分类</p></li></ol><h4 id="k近邻算法api初步使用"><a href="#k近邻算法api初步使用" class="headerlink" title="k近邻算法api初步使用"></a>k近邻算法api初步使用</h4><p> <strong>机器学习流程 :</strong></p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>获取数据集</span><br><span class="line"><span class="number">2.</span>数据基本处理</span><br><span class="line"><span class="number">3.</span>特征工程</span><br><span class="line"><span class="number">4.</span>机器学习</span><br><span class="line"><span class="number">5.</span>模型评估</span><br></pre></td></tr></table></figure><p><strong>Scikit-learn工具介绍：</strong></p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Python语言的机器学习工具</span><br><span class="line">Scikit-learn包括许多知名的机器学习算法的实现</span><br><span class="line">Scikit-learn文档完善，容易上手，丰富的API</span><br><span class="line">包含内容：</span><br><span class="line">分类、聚类、回归</span><br><span class="line">特征工程</span><br><span class="line">模型选择、调优</span><br><span class="line">优点：</span><br><span class="line">文档多,且规范,包含的算法多,实现起来容易</span><br><span class="line">目前稳定版本<span class="number">0.19</span><span class="number">.1</span></span><br></pre></td></tr></table></figure><p><a href="网址：https://scikit-learn.org/">Scikit-learn官网地址</a></p><p>K-近邻算法API**：</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sklearn.neighbors.KNeighborsClassifier(n_neighbors=<span class="number">5</span>)</span><br><span class="line">n_neighbors：<span class="built_in">int</span>,可选（默认= <span class="number">5</span>），k_neighbors查询默认使用的邻居数</span><br></pre></td></tr></table></figure><p><strong>小案例</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入模块</span></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.获取数据集</span></span><br><span class="line">x = [[<span class="number">0</span>], [<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]]</span><br><span class="line">y = [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">1</span>]</span><br><span class="line"><span class="comment"># 2.数据基本处理（该案例中省略）</span></span><br><span class="line"><span class="comment"># 3.特征工程（该案例中省略）</span></span><br><span class="line"><span class="comment"># 4.机器学习</span></span><br><span class="line"><span class="comment"># 实例化API</span></span><br><span class="line">estimator = KNeighborsClassifier(n_neighbors=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 使用fit方法进行训练</span></span><br><span class="line">estimator.fit(x, y)</span><br><span class="line"><span class="comment"># 输出预测值</span></span><br><span class="line">print(estimator.predict([[<span class="number">1</span>]]))</span><br><span class="line"><span class="comment"># 5.模型评估（该案例中省略）</span></span><br></pre></td></tr></table></figure><h4 id="K值选择"><a href="#K值选择" class="headerlink" title="K值选择"></a>K值选择</h4><p><strong>K值选择:</strong></p><p><strong>李航博士的一书「统计学习方法」上所说：</strong></p><ol><li><p>选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，<strong>K值的减小就意味着整体模型变得复杂，容易发生过拟合；</strong></p></li><li><p>选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，<strong>与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。</strong></p></li><li><p>K=N（N为训练样本个数），则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的类，模型过于简单，忽略了训练实例中大量有用信息。</p></li></ol><p>在<strong>实际应用中，K值一般取一个比较小的数值</strong>，例如采用交叉验证法（简单来说，就是把训练数据在分成两组:训练集和验证集）来选择最优的K值。</p><h4 id="KNN中K值选择"><a href="#KNN中K值选择" class="headerlink" title="KNN中K值选择"></a>KNN中K值选择</h4><ul><li>K值过小<ul><li>容易受到异常点的影响</li><li>容易过拟合</li></ul></li><li>k值过大：<ul><li>受到样本均衡的问题</li><li>容易欠拟合</li></ul></li></ul><h4 id="kd树"><a href="#kd树" class="headerlink" title="kd树"></a>kd树</h4><p><strong>定义:</strong></p><p>根据<strong>KNN</strong>每次需要预测一个点时，我们都需要计算训练数据集里每个点到这个点的距离，然后选出距离最近的k个点进行投票。<strong>当数据集很大时，这个计算成本非常高</strong>。</p><p><strong>kd树</strong>：为了避免每次都重新计算一遍距离，算法会把距离信息保存在一棵树里，这样在计算之前从树里查询距离信息，尽量避免重新计算。其基本原理是，<strong>如果A和B距离很远，B和C距离很近，那么A和C的距离也很远</strong>。有了这个信息，就可以在合适的时候跳过距离远的点。</p><p> <strong>最近邻域搜索:</strong></p><p>  kd树(K-dimension tree)是<strong>一种对k维空间中的实例点进行存储以便对其进行快速检索的树形数据结构。</strong>kd树是一种二叉树，表示对k维空间的一个划分，<strong>构造kd树相当于不断地用垂直于坐标轴的超平面将K维空间切分，构成一系列的K维超矩形区域</strong>。kd树的每个结点对应于一个k维超矩形区域。<strong>利用kd树可以省去对大部分数据点的搜索，从而减少搜索的计算量。</strong> </p><p> <img src="/2020/01/07/knn%E7%AE%97%E6%B3%95/kd%E6%A0%914.png" alt="image-20190213223817957"> </p><p><strong>构造方法:</strong></p><ol><li><p><strong>构造根结点，使根结点对应于K维空间中包含所有实例点的超矩形区域；</strong></p></li><li><p><strong>通过递归的方法，不断地对k维空间进行切分，生成子结点。</strong>在超矩形区域上选择一个坐标轴和在此坐标轴上的一个切分点，确定一个超平面，这个超平面通过选定的切分点并垂直于选定的坐标轴，将当前超矩形区域切分为左右两个子区域（子结点）；这时，实例被分到两个子区域。</p></li><li><p><strong>上述过程直到子区域内没有实例时终止（终止时的结点为叶结点）</strong>。在此过程中，将实例保存在相应的结点上。</p></li><li><p>通常，循环的选择坐标轴对空间切分，选择训练实例点在坐标轴上的中位数为切分点，这样得到的kd树是平衡的（平衡二叉树：它是一棵空树，或其左子树和右子树的深度之差的绝对值不超过1，且它的左子树和右子树都是平衡二叉树）。</p></li></ol><p>KD树中每个节点是一个向量，和二叉树按照数的大小划分不同的是，KD树每层需要选定向量中的某一维，然后根据这一维按左小右大的方式划分数据。在构建KD树时，关键需要解决2个问题：</p><ol><li><p><strong>选择向量的哪一维进行划分；</strong></p><p>解决方法可以是随机选择某一维或按顺序选择，但是<strong>更好的方法应该是在数据比较分散的那一维进行划分（分散的程度可以根据方差来衡量）</strong></p></li><li><p><strong>如何划分数据；</strong></p><p>好的划分方法可以使构建的树比较平衡，可以每次选择中位数来进行划分。</p><p><strong>kd树的搜索过程:</strong></p></li></ol><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span> 二叉树搜索比较待查询节点和分裂节点的分裂维的值，（小于等于就进入左子树分支，大于就进入右子树分支直到叶子结点）</span><br><span class="line"><span class="number">2.</span> 顺着“搜索路径”找到最近邻的近似点</span><br><span class="line"><span class="number">3.</span> 回溯搜索路径，并判断搜索路径上的结点的其他子结点空间中是否可能有距离查询点更近的数据点，如果有可能，则需要跳到其他子结点空间中去搜索</span><br><span class="line"><span class="number">4.</span> 重复这个过程直到搜索路径为空</span><br></pre></td></tr></table></figure><h4 id="scikit-learn中数据集"><a href="#scikit-learn中数据集" class="headerlink" title="scikit-learn中数据集"></a>scikit-learn中数据集</h4><p>scikit-learn数据集API</p><figure class="highlight autohotkey"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sklearn.datasets</span><br><span class="line">加载获取流行数据集</span><br><span class="line">datasets.load_*()</span><br><span class="line">获取小规模数据集，数据包含在datasets里</span><br><span class="line">datasets.fetch_*(dat<span class="built_in">a_home</span>=None)</span><br><span class="line">获取大规模数据集，需要从网络上下载，</span><br><span class="line">函数的第一个参数是dat<span class="built_in">a_home</span>，表示数据集下载的目录,</span><br><span class="line">默认是 ~/scikit_learn_data/</span><br></pre></td></tr></table></figure><p><strong>sklearn小数据集</strong></p><ul><li><p>sklearn.datasets.load_iris()：加载并返回鸢尾花数据集</p><p> <img src="/2020/01/07/knn%E7%AE%97%E6%B3%95/%E9%B8%A2%E5%B0%BE%E8%8A%B1%E6%95%B0%E6%8D%AE%E9%9B%86.png" alt="img"> </p></li></ul><p><strong>sklearn大数据集</strong></p><ul><li>sklearn.datasets.fetch_20newsgroups(data_home=None,subset=‘train’)<ul><li>subset：’train’或者’test’，’all’，可选，选择要加载的数据集。</li><li>训练集的“训练”，测试集的“测试”，两者的“全部”</li></ul></li></ul><p><strong>sklearn数据集返回值</strong></p><ul><li>load和fetch返回的数据类型datasets.base.Bunch(字典格式)<ul><li>data：特征数据数组，是 [n_samples * n_features] 的二维 numpy.ndarray 数组</li><li>target：标签数组，是 n_samples 的一维 numpy.ndarray 数组</li><li>DESCR：数据描述</li><li>feature_names：特征名,新闻数据，手写数字、回归数据集没有</li><li>target_names：标签名</li></ul></li></ul><p><strong>查看数据分布</strong></p><p>通过创建一些图，以查看不同类别是如何通过特征来区分的。 在理想情况下，标签类将由一个或多个特征对完美分隔。 在现实世界中，这种理想情况很少会发生。</p><ul><li>seaborn介绍<ul><li>Seaborn 是基于 Matplotlib 核心库进行了更高级的 API 封装，可以让你轻松地画出更漂亮的图形。而 Seaborn 的漂亮主要体现在配色更加舒服、以及图形元素的样式更加细腻。</li><li>安装 pip3 install seaborn</li><li>seaborn.lmplot() 是一个非常有用的方法，它会在绘制二维散点图时，自动完成回归拟合<ul><li>sns.lmplot() 里的 x, y 分别代表横纵坐标的列名,</li><li>data= 是关联到数据集,</li><li>hue=*代表按照 species即花的类别分类显示,</li><li>fit_reg=是否进行线性拟合。</li></ul></li><li><a href="http://seaborn.pydata.org/" target="_blank" rel="noopener">参考链接: api链接</a></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">%matplotlib inline  </span><br><span class="line"><span class="comment"># 内嵌绘图</span></span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"><span class="comment"># 把数据转换成dataframe的格式</span></span><br><span class="line">iris_d = pd.DataFrame(iris[<span class="string">'data'</span>], columns = [<span class="string">'Sepal_Length'</span>, <span class="string">'Sepal_Width'</span>, <span class="string">'Petal_Length'</span>, <span class="string">'Petal_Width'</span>])</span><br><span class="line">iris_d[<span class="string">'Species'</span>] = iris.target</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_iris</span><span class="params">(iris, col1, col2)</span>:</span></span><br><span class="line">    sns.lmplot(x = col1, y = col2, data = iris, hue = <span class="string">"Species"</span>, fit_reg = <span class="literal">False</span>)</span><br><span class="line">    plt.xlabel(col1)</span><br><span class="line">    plt.ylabel(col2)</span><br><span class="line">    plt.title(<span class="string">'鸢尾花种类分布图'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line">plot_iris(iris_d, <span class="string">'Petal_Width'</span>, <span class="string">'Sepal_Length'</span>)</span><br></pre></td></tr></table></figure><p> <img src="/2020/01/07/knn%E7%AE%97%E6%B3%95/%E9%B8%A2%E5%B0%BE%E8%8A%B1%E6%95%B0%E6%8D%AE%E5%88%86%E7%B1%BB%E5%B1%95%E7%A4%BA.png" alt="image-20190225193311519"> </p><h4 id="数据集划分"><a href="#数据集划分" class="headerlink" title="数据集划分"></a>数据集划分</h4><p>机器学习一般的数据集会划分为两个部分：</p><ul><li>训练数据：用于训练，<strong>构建模型</strong></li><li>测试数据：在模型检验时使用，用于<strong>评估模型是否有效</strong></li></ul><p>划分比例：</p><ul><li>训练集：70% 80% 75%</li><li>测试集：30% 20% 25%</li></ul><p><strong>数据集划分api</strong></p><ul><li>sklearn.model_selection.train_test_split(arrays, *options)<ul><li>参数：<ul><li>x 数据集的特征值</li><li>y 数据集的标签值</li><li>test_size 测试集的大小，一般为float</li><li>random_state 随机数种子,不同的种子会造成不同的随机采样结果。相同的种子采样结果相同。</li></ul></li><li>return<ul><li>x_train, x_test, y_train, y_test</li></ul></li></ul></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="comment"># 1、获取鸢尾花数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line"><span class="comment"># 对鸢尾花数据集进行分割</span></span><br><span class="line"><span class="comment"># 训练集的特征值x_train 测试集的特征值x_test 训练集的目标值y_train 测试集的目标值y_test</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=<span class="number">22</span>)</span><br><span class="line">print(<span class="string">"x_train:\n"</span>, x_train.shape)</span><br><span class="line"><span class="comment"># 随机数种子</span></span><br><span class="line">x_train1, x_test1, y_train1, y_test1 = train_test_split(iris.data, iris.target, random_state=<span class="number">6</span>)</span><br><span class="line">x_train2, x_test2, y_train2, y_test2 = train_test_split(iris.data, iris.target, random_state=<span class="number">6</span>)</span><br><span class="line">print(<span class="string">"如果随机数种子不一致：\n"</span>, x_train == x_train1)</span><br><span class="line">print(<span class="string">"如果随机数种子一致：\n"</span>, x_train1 == x_train2)</span><br></pre></td></tr></table></figure><h4 id="特征工程-特征预处理"><a href="#特征工程-特征预处理" class="headerlink" title="特征工程-特征预处理"></a>特征工程-特征预处理</h4><p><strong>定义：</strong></p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">scikit-learn的解释</span><br><span class="line">provides several common utility functions <span class="keyword">and</span> transformer classes <span class="keyword">to</span> change<span class="built_in"> raw </span>feature vectors into a representation that is more suitable <span class="keyword">for</span> the downstream estimators.</span><br><span class="line">翻译过来：通过一些转换函数将特征数据转换成更加适合算法模型的特征数据过程</span><br></pre></td></tr></table></figure><p><img src="/2020/01/07/knn%E7%AE%97%E6%B3%95/%E7%89%B9%E5%BE%81%E9%A2%84%E5%A4%84%E7%90%86%E5%9B%BE.png" alt="ç‰¹å¾é¢„å¤„ç†å›¾"></p><p> <strong>特征预处理API</strong></p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">sklearn</span><span class="selector-class">.preprocessing</span></span><br></pre></td></tr></table></figure><p><strong>包含内容</strong></p><ul><li><p>归一化</p><ul><li>鲁棒性比较差(容易受到异常点的影响)</li><li>只适合传统精确小数据场景(以后基本不会用)</li></ul></li><li><p>标准化</p><ul><li>异常值对我影响小</li><li>适合现代嘈杂大数据场景</li></ul></li></ul><p><strong>归一化/标准化原因</strong></p><p>  特征的<strong>单位或者大小相差较大，或者某特征的方差相比其他的特征要大出几个数量级</strong>，<strong>容易影响（支配）目标结果</strong>，使得一些算法无法学习到其它的特征 。所以 我们需要用到一些方法进行<strong>无量纲化</strong>，<strong>使不同规格的数据转换到同一规格</strong>。</p><p><strong>归一化：</strong> 通过对原始数据进行变换把数据映射到(默认为[0,1])之间。</p><p> <img src="/2020/01/07/knn%E7%AE%97%E6%B3%95/%E5%BD%92%E4%B8%80%E5%8C%96%E5%85%AC%E5%BC%8F.png" alt="å½’ä¸€åŒ–å…¬å¼"></p><p> <strong>API:</strong></p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sklearn.preprocessing.MinMaxScaler (feature_range=(<span class="number">0</span>,<span class="number">1</span>)… )</span><br><span class="line"><span class="module-access"><span class="module"><span class="identifier">MinMaxScalar</span>.</span></span>fit<span class="constructor">_transform(X)</span></span><br><span class="line">X:numpy <span class="built_in">array</span>格式的数据<span class="literal">[<span class="identifier">n_samples</span>,<span class="identifier">n_features</span>]</span></span><br><span class="line">返回值：转换后的形状相同的<span class="built_in">array</span></span><br></pre></td></tr></table></figure><p><strong>标准化：</strong> 通过对原始数据进行变换把数据变换到均值为0,标准差为1范围内 </p><p> <img src="/2020/01/07/knn%E7%AE%97%E6%B3%95/%E6%A0%87%E5%87%86%E5%8C%96%E5%85%AC%E5%BC%8F.png" alt="æ ‡å‡†åŒ–å…¬å¼"></p><ul><li>对于归一化来说：如果出现异常点，影响了最大值和最小值，那么结果显然会发生改变</li><li><p>对于标准化来说：如果出现异常点，由于具有一定数据量，少量的异常点对于平均值的影响并不大，从而方差改变较小</p><p><strong>API:</strong></p></li></ul><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sklearn.preprocessing.<span class="constructor">StandardScaler( )</span></span><br><span class="line">处理之后每列来说所有数据都聚集在均值<span class="number">0</span>附近标准差差为<span class="number">1</span></span><br><span class="line"><span class="module-access"><span class="module"><span class="identifier">StandardScaler</span>.</span></span>fit<span class="constructor">_transform(X)</span></span><br><span class="line">X:numpy <span class="built_in">array</span>格式的数据<span class="literal">[<span class="identifier">n_samples</span>,<span class="identifier">n_features</span>]</span></span><br><span class="line">返回值：转换后的形状相同的<span class="built_in">array</span></span><br></pre></td></tr></table></figure><h4 id="交叉验证，网格搜索"><a href="#交叉验证，网格搜索" class="headerlink" title="交叉验证，网格搜索"></a>交叉验证，网格搜索</h4><p><strong>交叉验证：</strong> 将拿到的训练数据，分为训练和验证集。以下图为例：将数据分成4份，其中一份作为验证集。然后经过4次(组)的测试，每次都更换不同的验证集。即得到4组模型的结果，取平均值作为最终结果。又称4折交叉验证。 </p><p>数据分为训练集和测试集，但是<strong>为了让从训练得到模型结果更加准确。</strong>做以下处理</p><ul><li>训练集：训练集+验证集</li><li><p>测试集：测试集</p><p><img src="/2020/01/07/knn%E7%AE%97%E6%B3%95/%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81%E8%BF%87%E7%A8%8B.png" alt="img"></p></li></ul><p><strong>网格搜索：</strong> 通常情况下，<strong>有很多参数是需要手动指定的（如k-近邻算法中的K值），这种叫超参数</strong>。但是手动过程繁杂，所以需要对模型预设几种超参数组合。<strong>每组超参数都采用交叉验证来进行评估。最后选出最优参数组合建立模型。</strong> </p><p><strong>API</strong></p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">sklearn.model_selection.<span class="constructor">GridSearchCV(<span class="params">estimator</span>, <span class="params">param_grid</span>=None,<span class="params">cv</span>=None)</span></span><br><span class="line">对估计器的指定参数值进行详尽搜索</span><br><span class="line">estimator：估计器对象</span><br><span class="line">param_grid：估计器参数(dict)&#123;“n_neighbors”:<span class="literal">[<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>]</span>&#125;</span><br><span class="line">cv：指定几折交叉验证</span><br><span class="line">fit：输入训练数据</span><br><span class="line">score：准确率</span><br><span class="line">结果分析：</span><br><span class="line">bestscore__:在交叉验证中验证的最好结果</span><br><span class="line">bestestimator：最好的参数模型</span><br><span class="line">cvresults:每次交叉验证后的验证集准确率结果和训练集准确率结果</span><br></pre></td></tr></table></figure><h4 id="案例1-鸢尾花种类预测"><a href="#案例1-鸢尾花种类预测" class="headerlink" title="案例1-鸢尾花种类预测"></a>案例1-鸢尾花种类预测</h4><p> Iris数据集是常用的分类实验数据集，由Fisher, 1936收集整理。Iris也称鸢尾花卉数据集，是一类多重变量分析的数据集。关于数据集的具体介绍： </p><p> <img src="/2020/01/07/knn%E7%AE%97%E6%B3%95/%E9%B8%A2%E5%B0%BE%E8%8A%B1%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BD%BF%E7%94%A8.png" alt="img"></p><p> <strong>代码实现：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1、获取数据集</span></span><br><span class="line">iris = load_iris()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2、数据基本处理 -- 划分数据集</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(iris.data, iris.target, random_state=<span class="number">22</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3、特征工程：标准化</span></span><br><span class="line"><span class="comment"># 实例化一个转换器类</span></span><br><span class="line">transfer = StandardScaler()</span><br><span class="line"><span class="comment"># 调用fit_transform</span></span><br><span class="line">x_train = transfer.fit_transform(x_train)</span><br><span class="line">x_test = transfer.transform(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4、KNN预估器流程</span></span><br><span class="line"><span class="comment">#  4.1 实例化预估器类</span></span><br><span class="line">estimator = KNeighborsClassifier()</span><br><span class="line"><span class="comment"># 4.2 模型选择与调优——网格搜索和交叉验证</span></span><br><span class="line"><span class="comment"># 准备要调的超参数</span></span><br><span class="line">param_dict = &#123;<span class="string">"n_neighbors"</span>: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>]&#125;</span><br><span class="line">estimator = GridSearchCV(estimator, param_grid=param_dict, cv=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 4.3 fit数据进行训练</span></span><br><span class="line">estimator.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5、评估模型效果</span></span><br><span class="line"><span class="comment"># 方法a：比对预测结果和真实值</span></span><br><span class="line">y_predict = estimator.predict(x_test)</span><br><span class="line">print(<span class="string">"比对预测结果和真实值：\n"</span>, y_predict == y_test)</span><br><span class="line"><span class="comment"># 方法b：直接计算准确率</span></span><br><span class="line">score = estimator.score(x_test, y_test)</span><br><span class="line">print(<span class="string">"直接计算准确率：\n"</span>, score)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 然后进行评估查看最终选择的结果和交叉验证的结果</span></span><br><span class="line">print(<span class="string">"在交叉验证中验证的最好结果：\n"</span>, estimator.best_score_)</span><br><span class="line">print(<span class="string">"最好的参数模型：\n"</span>, estimator.best_estimator_)</span><br><span class="line">print(<span class="string">"每次交叉验证后的准确率结果：\n"</span>, estimator.cv_results_)</span><br></pre></td></tr></table></figure><h4 id="案例2-预测facebook签到位置"><a href="#案例2-预测facebook签到位置" class="headerlink" title="案例2-预测facebook签到位置"></a>案例2-预测facebook签到位置</h4><p> <img src="/2020/01/07/knn%E7%AE%97%E6%B3%95/Facebook%E7%AD%BE%E5%88%B0%E4%BD%8D%E7%BD%AE%E9%A2%84%E6%B5%8B.png" alt="image-20190515115928245"> </p><p> 本次比赛的目的是<strong>预测一个人将要签到的地方。</strong> 为了本次比赛，Facebook创建了一个虚拟世界，其中包括<strong>10公里*10公里共100平方公里的约10万个地方。</strong> 对于给定的坐标集，您的任务将<strong>根据用户的位置，准确性和时间戳等预测用户下一次的签到位置。</strong> 数据被制作成类似于来自移动设备的位置数据。 请注意：您只能使用提供的数据进行预测。 </p><p><strong>数据集介绍：</strong></p><p> <img src="/2020/01/07/knn%E7%AE%97%E6%B3%95/facebook%E6%95%B0%E6%8D%AE%E9%9B%86%E4%BB%8B%E7%BB%8D.png" alt="image-20190515120143596"> </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">文件说明 train.csv, test.csv</span><br><span class="line">  row id：签入事件的id</span><br><span class="line">  x y：坐标</span><br><span class="line">  accuracy: 准确度，定位精度</span><br><span class="line">  time: 时间戳</span><br><span class="line">  place_id: 签到的位置，这也是你需要预测的内容</span><br></pre></td></tr></table></figure><p><strong>数据来源：</strong> 官网：<a href="https://www.kaggle.com/navoshta/grid-knn/data" target="_blank" rel="noopener">https://www.kaggle.com/navoshta/grid-knn/data</a> </p><p><strong>步骤分析：</strong></p><ul><li>对于数据做一些基本处理（这里所做的一些处理不一定达到很好的效果，我们只是简单尝试，有些特征我们可以根据一些特征选择的方式去做处理）<ul><li>1 缩小数据集范围 DataFrame.query()</li><li>2 选取有用的时间特征</li><li>3 将签到位置少于n个用户的删除</li></ul></li><li>分割数据集</li><li>标准化处理</li><li>k-近邻预测</li></ul><p><strong>代码实现：</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split, GridSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.获取数据集</span></span><br><span class="line">facebook = pd.read_csv(<span class="string">"./data/FBlocation/train.csv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.基本数据处理</span></span><br><span class="line"><span class="comment"># 2.1 缩小数据范围</span></span><br><span class="line">facebook_data = facebook.query(<span class="string">"x&gt;2.0 &amp; x&lt;2.2 &amp; y&gt;2.0 &amp; y&lt;2.2"</span>)</span><br><span class="line"><span class="comment"># 2.2 选择时间特征(脱敏)</span></span><br><span class="line">time = pd.to_datetime(facebook_data[<span class="string">"time"</span>], unit=<span class="string">"s"</span>)</span><br><span class="line"><span class="comment"># 转换</span></span><br><span class="line">time = pd.DatetimeIndex(time)</span><br><span class="line">facebook_data[<span class="string">"day"</span>] = time.day</span><br><span class="line">facebook_data[<span class="string">"hour"</span>] = time.hour</span><br><span class="line">facebook_data[<span class="string">"weekday"</span>] = time.weekday</span><br><span class="line"><span class="comment"># 2.3 去掉签到较少的地方</span></span><br><span class="line">place_count = facebook_data.groupby(<span class="string">"place_id"</span>).count()</span><br><span class="line">place_count = place_count[place_count[<span class="string">"row_id"</span>]&gt;<span class="number">3</span>]</span><br><span class="line">facebook_data = facebook_data[facebook_data[<span class="string">"place_id"</span>]].isin(place_count.index)</span><br><span class="line"><span class="comment"># 2.4 确定特征值和目标值</span></span><br><span class="line">x = facebook_data[[<span class="string">"x"</span>, <span class="string">"y"</span>, <span class="string">"accuracy"</span>, <span class="string">"day"</span>, <span class="string">"hour"</span>, <span class="string">"weekday"</span>]]</span><br><span class="line">y = facebook_data[<span class="string">"place_id"</span>]</span><br><span class="line"><span class="comment"># 2.5 分割数据集</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=<span class="number">22</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.特征工程 -- 特征预处理(标准化)</span></span><br><span class="line"><span class="comment"># 3.1 实例化一个转换器</span></span><br><span class="line">transfer = StandardScaler()</span><br><span class="line"><span class="comment"># 3.2 调用fit_transform</span></span><br><span class="line">x_train = transfer.fit_transform(x_train)</span><br><span class="line">x_test = transfer.fit_transform(x_test)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.机器学习 -- knn+cv</span></span><br><span class="line"><span class="comment"># 4.1 实例化一个训练器</span></span><br><span class="line">estimator = KNeighborsClassifier()</span><br><span class="line"><span class="comment"># 4.2 交叉验证，网格搜索实现，调用gridsearchCV</span></span><br><span class="line">param_grid = &#123;<span class="string">"n_neighbors"</span>: [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">7</span>, <span class="number">9</span>]&#125;</span><br><span class="line"><span class="comment"># n_jobs：指定几个CPU跑程序</span></span><br><span class="line">estimator = GridSearchCV(estimator, param_grid=param_grid, cv=<span class="number">5</span>, n_jobs=<span class="number">8</span>)</span><br><span class="line"><span class="comment"># 4.3 模型训练</span></span><br><span class="line">estimator.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.模型评估</span></span><br><span class="line"><span class="comment"># 5.1 基本评估方式</span></span><br><span class="line">score = estimator.score(x_test, y_test)</span><br><span class="line">print(<span class="string">"最后预测的准确率为:\n"</span>, score)</span><br><span class="line">y_predict = estimator.predict(x_test)</span><br><span class="line">print(<span class="string">"最后的预测值为:\n"</span>, y_predict)</span><br><span class="line">print(<span class="string">"预测值和真实值的对比情况:\n"</span>, y_predict == y_test)</span><br><span class="line"><span class="comment"># 5.2 使用交叉验证后的评估方式</span></span><br><span class="line">print(<span class="string">"在交叉验证中验证的最好结果:\n"</span>, estimator.best_score_)</span><br><span class="line">print(<span class="string">"最好的参数模型:\n"</span>, estimator.best_estimator_)</span><br><span class="line">print(<span class="string">"每次交叉验证后的验证集准确率结果和训练集准确率结果:\n"</span>,estimator.cv_results_)</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;K-近邻算法定义&quot;&gt;&lt;a href=&quot;#K-近邻算法定义&quot; class=&quot;headerlink&quot; title=&quot;K-近邻算法定义&quot;&gt;&lt;/a&gt;K-近邻算法定义&lt;/h4&gt;&lt;p&gt; K Nearest Neighbor算法又叫KNN算法，这个算法是机器学习里面一个比较经典的算法， 总体来说KNN算法是相对比较容易理解的算法 ， 如果一个样本在特征空间中的&lt;strong&gt;k个最相似(即特征空间中最邻近)的样本中的大多数属于某一个类别&lt;/strong&gt;，则该样本也属于这个类别。 &lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习算法" scheme="https://xiaoliaozi.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="机器学习算法" scheme="https://xiaoliaozi.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
      <category term="人工智能" scheme="https://xiaoliaozi.com/tags/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD/"/>
    
      <category term="机器学习" scheme="https://xiaoliaozi.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
  </entry>
  
  <entry>
    <title>距离度量</title>
    <link href="https://xiaoliaozi.com/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/"/>
    <id>https://xiaoliaozi.com/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/</id>
    <published>2020-01-06T03:04:16.000Z</published>
    <updated>2020-01-06T11:55:50.674Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p> 机器学习中常见的距离计算公式 </p><a id="more"></a><h4 id="欧式距离"><a href="#欧式距离" class="headerlink" title="欧式距离"></a>欧式距离</h4><p>欧氏距离(<strong>Euclidean Distance</strong>)是最容易直观理解的距离度量方法，我们小学、初中和高中接触到的两个点在空间中的距离一般都是指欧氏距离。 </p><p><strong>公式</strong>：</p><p> <img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/1.4%20%E6%AC%A7%E5%BC%8F%E8%B7%9D%E7%A6%BB2-1965410.png" alt="img"> </p><h4 id="曼哈顿距离"><a href="#曼哈顿距离" class="headerlink" title="曼哈顿距离"></a>曼哈顿距离</h4><p>在曼哈顿街区要从一个十字路口开车到另一个十字路口，驾驶距离显然不是两点间的直线距离。这个实际驾驶距离就是“曼哈顿距离”。曼哈顿距离(<strong>Manhattan Distance</strong>)也称为“城市街区距离”(City Block distance)。 </p><p> <img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/1.5%20%E6%9B%BC%E5%93%88%E9%A1%BF%E8%B7%9D%E7%A6%BB.png" alt="img"></p><p>  <strong>公式</strong>：<img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/1.6%20%E6%9B%BC%E5%93%88%E9%A1%BF%E8%B7%9D%E7%A6%BB.png" alt="img"> </p><h4 id="切比雪夫距离"><a href="#切比雪夫距离" class="headerlink" title="切比雪夫距离"></a>切比雪夫距离</h4><p>国际象棋中，国王可以直行、横行、斜行，所以国王走一步可以移动到相邻8个方格中的任意一个。国王从格子(x1,y1)走到格子(x2,y2)最少需要多少步？这个距离就叫切比雪夫距离（<strong>Chebyshev Distance</strong>）。 </p><p> <img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/1.7%20%E5%88%87%E6%AF%94%E9%9B%AA%E5%A4%AB%E8%B7%9D%E7%A6%BB.png" alt="img"></p><p><strong>公式</strong>：</p><p>  <img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/1.8%20%E5%88%87%E6%AF%94%E9%9B%AA%E5%A4%AB%E8%B7%9D%E7%A6%BB.png" alt="img"> </p><h4 id="闵可夫斯基距离"><a href="#闵可夫斯基距离" class="headerlink" title="闵可夫斯基距离"></a>闵可夫斯基距离</h4><p>闵氏距离不是一种距离，而是一组距离的定义，是对多个距离度量公式的概括性的表述。</p><p>两个n维变量a(x11,x12,…,x1n)与b(x21,x22,…,x2n)间的闵可夫斯基距离（<strong>Minkowski Distance</strong>）定义为：</p><p><strong>公式</strong>：</p><p> <img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/%E9%97%B5%E5%8F%AF%E5%A4%AB%E6%96%AF%E5%9F%BA%E8%B7%9D%E7%A6%BB.png" alt="image-20190225182628694"></p><p>其中p是一个变参数：</p><p>当p=1时，就是曼哈顿距离；</p><p>当p=2时，就是欧氏距离；</p><p>当p→∞时，就是切比雪夫距离。</p><p>根据p的不同，闵氏距离可以表示某一类/种的距离。</p><p><strong>闵氏距离的缺点：</strong></p><ol><li><p>将各个分量的量纲(scale)，也就是“单位”相同的看待了;</p></li><li><p>未考虑各个分量的分布（期望，方差等）可能是不同的。</p></li></ol><h4 id="标准化欧氏距离"><a href="#标准化欧氏距离" class="headerlink" title="标准化欧氏距离"></a>标准化欧氏距离</h4><p>标准化欧氏距离(<strong>Standardized EuclideanDistance</strong>)是针对欧氏距离的缺点而作的一种改进。</p><p>思路：既然数据各维分量的分布不一样，那先将各个分量都“标准化”到均值、方差相等。</p><p> $S_k$表示各个维度的标准差</p><p> <img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/%E6%A0%87%E5%87%86%E5%8C%96%E6%AC%A7%E5%BC%8F%E8%B7%9D%E7%A6%BB.png" alt="image-20190213184012294"> </p><h4 id="余弦距离"><a href="#余弦距离" class="headerlink" title="余弦距离"></a>余弦距离</h4><p>几何中，夹角余弦可用来衡量两个向量方向的差异；机器学习中，借用这一概念来衡量样本向量之间的差异。 </p><ul><li><p>二维空间中向量$A(x_1,y_1)$与向量$B(x_2,y_2)$的夹角余弦公式： </p><p><img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/14200251_RZRZ.png" alt="余弦距离"> </p></li><li><p>两个n维样本点$a(x11,x12,…,x1n)$和$b(x21,x22,…,x2n)$的夹角余弦为： </p><p><img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/14200252_SE1M.png" alt="余弦距离"> </p><p>即：</p><p><img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/14200252_tITR.png" alt="余弦距离"> </p></li></ul><h4 id="汉明距离"><a href="#汉明距离" class="headerlink" title="汉明距离"></a>汉明距离</h4><p>两个等长字符串s1与s2的汉明距离（<strong>Hamming Distance</strong>）为：将其中一个变为另外一个所需要作的最小字符替换次数。 </p><p> <img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/%E6%B1%89%E6%98%8E%E8%B7%9D%E7%A6%BB.png" alt="image-20190213184508110"> </p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">求下列字符串的汉明距离：</span><br><span class="line"><span class="number">1011101</span>与 <span class="number">1001001</span> <span class="number">1111111</span> <span class="number">1101011</span> <span class="number">7</span><span class="number">-5</span>=<span class="number">2</span></span><br><span class="line"><span class="number">2143896</span>与 <span class="number">2233796</span> <span class="number">1111111</span> <span class="number">1001011</span> <span class="number">7</span><span class="number">-4</span>=<span class="number">3</span> </span><br><span class="line">irie与 rise <span class="number">1111</span> <span class="number">0001</span> <span class="number">4</span><span class="number">-1</span>=<span class="number">3</span></span><br></pre></td></tr></table></figure><h4 id="杰卡德距离"><a href="#杰卡德距离" class="headerlink" title="杰卡德距离"></a>杰卡德距离</h4><p>杰卡德相似系数(<strong>Jaccard similarity coefficient</strong>)：两个集合A和B的交集元素在A，B的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号J(A,B)表示： </p><p> <img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/%E6%9D%B0%E5%8D%A1%E5%BE%B7%E8%B7%9D%E7%A6%BB1.png" alt="image-20190213184805616"></p><p> 杰卡德距离(<strong>Jaccard Distance</strong>)：与杰卡德相似系数相反，用两个集合中不同元素占所有元素的比例来衡量两个集合的区分度： </p><p> <img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/%E6%9D%B0%E5%8D%A1%E5%BE%B7%E8%B7%9D%E7%A6%BB2.png" alt="image-20190213184819510"></p><h4 id="马氏距离"><a href="#马氏距离" class="headerlink" title="马氏距离"></a>马氏距离</h4><p>下图有两个正态分布图，它们的均值分别为a和b，但方差不一样，则图中的A点离哪个总体更近？或者说A有更大的概率属于谁？显然，A离左边的更近，A属于左边总体的概率更大，尽管A与a的欧式距离远一些。这就是马氏距离的直观解释。 </p><p> <img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/%E9%A9%AC%E6%B0%8F%E8%B7%9D%E7%A6%BB.png" alt="image-20190213183101699"></p><p>马氏距离是基于样本分布的一种距离。</p><p>马氏距离是由印度统计学家马哈拉诺比斯提出的，表示数据的协方差距离。它是一种有效的计算两个位置样本集的相似度的方法。</p><p>与欧式距离不同的是，它考虑到各种特性之间的联系，即独立于测量尺度。</p><p><strong>马氏距离定义：</strong>设总体G为m维总体（考察m个指标），均值向量为$μ=(μ1，μ2，… …，μm,)$，协方差阵为$∑=(σij)$,</p><p>则样本$X=(X1，X2，… …，Xm，)$与总体G的马氏距离定义为：</p><p> <img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/%E9%A9%AC%E6%B0%8F%E8%B7%9D%E7%A6%BB1.png" alt="image-20190316193646073"> </p><p>马氏距离也可以定义为两个服从同一分布并且其协方差矩阵为∑的随机变量的差异程度：如果协方差矩阵为单位矩阵，马氏距离就简化为欧式距离；如果协方差矩阵为对角矩阵，则其也可称为正规化的欧式距离。 </p><p><strong>马氏距离特性：</strong></p><ol><li><p><strong>量纲无关</strong>，排除变量之间的相关性的干扰；</p></li><li><p><strong>马氏距离的计算是建立在总体样本的基础上的</strong>，如果拿同样的两个样本，放入两个不同的总体中，最后计算得出的两个样本间的马氏距离通常是不相同的，除非这两个总体的协方差矩阵碰巧相同；</p></li><li><p>计算马氏距离过程中，<strong>要求总体样本数大于样本的维数</strong>，否则得到的总体样本协方差矩阵逆矩阵不存在，这种情况下，用欧式距离计算即可。</p></li><li><p>还有一种情况，满足了条件总体样本数大于样本的维数，但是协方差矩阵的逆矩阵仍然不存在，比如三个样本点$（3，4），（5，6），（7，8）$，这种情况是因为这三个样本在其所处的二维空间平面内共线。这种情况下，也采用欧式距离计算。</p><p><strong>欧式距离&amp;马氏距离：</strong> </p><p><img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/%E6%AC%A7%E5%BC%8Fvs%E9%A9%AC%E6%B0%8F.png" alt="img"> </p></li></ol><p>举例：</p><p>已知有两个类G1和G2，比如G1是设备A生产的产品，G2是设备B生产的同类产品。设备A的产品质量高（如考察指标为耐磨度X），其平均耐磨度μ1=80，反映设备精度的方差σ2(1)=0.25;设备B的产品质量稍差，其平均耐磨损度μ2=75，反映设备精度的方差σ2(2)=4.</p><p>今有一产品G0，测的耐磨损度X0=78，试判断该产品是哪一台设备生产的？</p><p>直观地看，X0与μ1（设备A）的绝对距离近些，按距离最近的原则，是否应把该产品判断设备A生产的？</p><p>考虑一种相对于分散性的距离，记X0与G1，G2的相对距离为d1，d2,则：</p><p> <img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/%E9%A9%AC%E6%B0%8F%E8%B7%9D%E7%A6%BB%E4%B8%BE%E4%BE%8B1.png" alt="image-20190316192358557"> </p><p>因为d2=1.5 &lt; d1=4，按这种距离准则，应判断X0为设备B生产的。</p><p>设备B生产的产品质量较分散，出现X0为78的可能性较大；而设备A生产的产品质量较集中，出现X0为78的可能性较小。</p><p>这种相对于分散性的距离判断就是马氏距离。</p><p><img src="/2020/01/06/%E8%B7%9D%E7%A6%BB%E5%BA%A6%E9%87%8F/%E9%A9%AC%E6%B0%8F%E8%B7%9D%E7%A6%BB%E4%B8%BE%E4%BE%8B2.png" alt="image-20190316192851778"> </p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h4&gt;&lt;p&gt; 机器学习中常见的距离计算公式 &lt;/p&gt;
    
    </summary>
    
    
      <category term="距离计算公式" scheme="https://xiaoliaozi.com/categories/%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F/"/>
    
    
      <category term="距离计算公式" scheme="https://xiaoliaozi.com/tags/%E8%B7%9D%E7%A6%BB%E8%AE%A1%E7%AE%97%E5%85%AC%E5%BC%8F/"/>
    
  </entry>
  
  <entry>
    <title>查找算法</title>
    <link href="https://xiaoliaozi.com/2020/01/06/%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95/"/>
    <id>https://xiaoliaozi.com/2020/01/06/%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95/</id>
    <published>2020-01-06T02:47:57.000Z</published>
    <updated>2020-01-06T10:30:12.826Z</updated>
    
    <content type="html"><![CDATA[<h4 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h4><p>搜索是在一个项目集合中找到一个特定项目的算法过程。搜索通常的答案是真的或假的，因为该项目是否存在。 搜索的八大查找算法：顺序查找，二分查找，插值查找，分块查找， 斐波那契查找，树表查找，哈希查找。</p><a id="more"></a><h4 id="顺序查找"><a href="#顺序查找" class="headerlink" title="顺序查找"></a>顺序查找</h4><p><strong>1. 定义</strong></p><p> 顺序查找是按照序列原有顺序对数组进行遍历比较查询的基本查找算法。  对于任意一个序列以及一个给定的元素，将给定元素与序列中元素依次比较，直到找出与给定关键字相同的元素，或者将序列中的元素与其都比较完为止。 </p><p><strong>2. 图示</strong></p><p> <img src="/2020/01/06/%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95/1387338-20180530200925684-1605416993.png" alt="img"> </p><p><strong>3. 代码实现</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">order_search</span><span class="params">(arr, value)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> arr:</span><br><span class="line">        <span class="keyword">if</span> value == i:</span><br><span class="line">            print(<span class="string">"在列表中找到该值&#123;&#125;"</span>.format(value))</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        print(<span class="string">"没找到该值"</span>)</span><br><span class="line"></span><br><span class="line">arr = [<span class="string">"a"</span>, <span class="string">"b"</span>, <span class="string">"c"</span>, <span class="string">"e"</span>, <span class="string">"f"</span>, <span class="string">"g"</span>]</span><br><span class="line">order_search(arr, <span class="string">"f"</span>)</span><br><span class="line">order_search(arr, <span class="string">"d"</span>)</span><br></pre></td></tr></table></figure><p><strong>4. 适用</strong></p><p> 无序表查找，也就是数据不排序的线性查找，遍历数据元素 </p><h4 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h4><p><strong>定义</strong></p><p>二分查找又称折半查找，优点是比较次数少，查找速度快，平均性能好；其缺点是要求待查表为有序表，且插入删除困难。因此，折半查找方法适用于不经常变动而查找频繁的有序列表。首先，假设表中元素是按升序排列，将表中间位置记录的关键字与查找关键字比较，如果两者相等，则查找成功；否则利用中间位置记录将表分成前、后两个子表，如果中间位置记录的关键字大于查找关键字，则进一步查找前一子表，否则进一步查找后一子表。重复以上过程，直到找到满足条件的记录，使查找成功，或直到子表不存在为止，此时查找不成功。 </p><p><strong>图示</strong></p><p> <img src="/2020/01/06/%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95/Binary_search_into_array.png" alt="Binary_search_into_array"></p><p><strong>代码实现</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 非递归实现</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search1</span><span class="params">(alist, item)</span>:</span></span><br><span class="line">    first = <span class="number">0</span></span><br><span class="line">    last = len(alist) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> first &lt;= last:</span><br><span class="line">        midpoint = (first + last) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> alist[midpoint] == item:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">elif</span> item &lt; alist[midpoint]:</span><br><span class="line">            last = midpoint - <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            first = midpoint + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">testlist = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">13</span>, <span class="number">17</span>, <span class="number">19</span>, <span class="number">32</span>, <span class="number">42</span>, <span class="number">78</span>]</span><br><span class="line">print(binary_search1(testlist, <span class="number">3</span>))</span><br><span class="line">print(binary_search1(testlist, <span class="number">13</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 递归实现</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_search2</span><span class="params">(alist, item)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(alist) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        midpoint = len(alist)//<span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> alist[midpoint]==item:</span><br><span class="line">          <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          <span class="keyword">if</span> item&lt;alist[midpoint]:</span><br><span class="line">            <span class="keyword">return</span> binary_search2(alist[:midpoint],item)</span><br><span class="line">          <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> binary_search2(alist[midpoint+<span class="number">1</span>:],item)</span><br><span class="line"></span><br><span class="line">testlist = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">13</span>, <span class="number">17</span>, <span class="number">19</span>, <span class="number">32</span>, <span class="number">42</span>, <span class="number">78</span>]</span><br><span class="line">print(binary_search2(testlist, <span class="number">3</span>))</span><br><span class="line">print(binary_search2(testlist, <span class="number">13</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># leecode: 二分查找</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">search</span><span class="params">(self, nums, target)</span>:</span></span><br><span class="line">        left = <span class="number">0</span></span><br><span class="line">        right = len(nums)<span class="number">-1</span></span><br><span class="line">        <span class="keyword">while</span>(left&lt;=right):</span><br><span class="line">            midpoint = left + int((right-left)/<span class="number">2</span>)</span><br><span class="line">            <span class="keyword">if</span> nums[midpoint]==target:</span><br><span class="line">                <span class="keyword">return</span> midpoint</span><br><span class="line">            <span class="keyword">if</span> target&lt;nums[midpoint]:</span><br><span class="line">                right = midpoint <span class="number">-1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                left = midpoint+<span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br><span class="line">alist = [<span class="number">-1</span>,<span class="number">0</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">9</span>,<span class="number">12</span>]</span><br><span class="line">Binary = Solution()</span><br><span class="line">result= Binary.search(nums=alist, target=<span class="number">9</span>)</span><br><span class="line">print(result)</span><br></pre></td></tr></table></figure><p><strong>4. 适用</strong></p><p> 有序表查找，查找表中的数据必须按某个主键进行某种排序。  需要频繁执行插入或删除操作的数据集来说，维护有序的排序会带来不小的工作，不建议适用二分查找。</p><h4 id="插值查找"><a href="#插值查找" class="headerlink" title="插值查找"></a>插值查找</h4><p><strong>1. 定义</strong></p><p> 二分查找每次都是从中间开始，没有考虑数据之间的关系，这是一种比较低效的实现方法，插值查找基于二分查找，改进的中间记录的选取  ，将查找点的选择改进为自适应选择，可以提高查找效率。  </p><p><strong>2. 图示</strong></p><p>和二分查找一样但是中间值选择方式不同为自适应选择</p><p><img src="/2020/01/06/%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95/Binary_search_into_array.png" alt="Binary_search_into_array"></p><p><strong>3. 代码实现</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 递归实现</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">inter_search</span><span class="params">(alist, item, low)</span>:</span></span><br><span class="line">    hight = len(alist)<span class="number">-1</span></span><br><span class="line">    <span class="keyword">if</span> alist[low]==item:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> alist[hight] == item:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> len(alist) &lt;= <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> alist[hight]-alist[low] != <span class="number">0</span>:</span><br><span class="line">            midpoint = int(low+(hight-low)*(item-alist[low])/(alist[hight]-alist[low]))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            midpoint = len(alist)//<span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> alist[midpoint]==item:</span><br><span class="line">          <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          <span class="keyword">if</span> item&lt;alist[midpoint]:</span><br><span class="line">            <span class="keyword">return</span> inter_search(alist[:midpoint],item, <span class="number">0</span>)</span><br><span class="line">          <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> inter_search(alist[midpoint+<span class="number">1</span>:],item, midpoint+<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">testlist = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">8</span>, <span class="number">13</span>, <span class="number">17</span>, <span class="number">19</span>, <span class="number">32</span>, <span class="number">42</span>, <span class="number">78</span>]</span><br><span class="line">print(inter_search(testlist, <span class="number">3</span>, <span class="number">0</span>))</span><br><span class="line">print(inter_search(testlist, <span class="number">13</span>, <span class="number">0</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># leecode 二分查找</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">search</span><span class="params">(self, nums, target)</span>:</span></span><br><span class="line">        left = <span class="number">0</span></span><br><span class="line">        right = len(nums)<span class="number">-1</span></span><br><span class="line">        <span class="keyword">while</span>(left&lt;=right):</span><br><span class="line">            <span class="keyword">if</span> nums[right]-nums[left] != <span class="number">0</span>:</span><br><span class="line">                midpoint = left + int((right - left)*(target-nums[left])/(nums[right]-nums[left]))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                midpoint = left + int((right-left)/<span class="number">2</span>)</span><br><span class="line">            <span class="keyword">if</span> nums[midpoint]==target:</span><br><span class="line">                <span class="keyword">return</span> midpoint</span><br><span class="line">            <span class="keyword">if</span> target&lt;nums[midpoint]:</span><br><span class="line">                right = midpoint <span class="number">-1</span></span><br><span class="line">            <span class="keyword">if</span> target&lt;nums[midpoint]:</span><br><span class="line">                right = midpoint <span class="number">-1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                left = midpoint+<span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">-1</span></span><br></pre></td></tr></table></figure><p><strong>4. 适用</strong></p><p> 表比较大，而关键字分布比较均匀的查找，插值算法的平均性能比折半查找要好很多。反之，线性表中记录分布不均匀，用插值查找未必是很好的选择。 </p><h4 id="分块查找"><a href="#分块查找" class="headerlink" title="分块查找"></a>分块查找</h4><p><strong>1. 定义</strong></p><p>分块查找，又称为索引顺序查找，吸取了顺序查找和折半查找各自的优点，既有动态结构，又适合快速查找。将查找表分为若干个子块。块内元素可以无序，但块之间是有序的，即第一个块中的最小关键字小于第二个块中的所有记录的关键字，第二个块中的最大关键字小于第三个块中的所有记录的关键字，以此类推。在建立一个索引表，索引表中的每个元素含有各块的最大关键字和各块中第一个元素的地址，索引表按关键字有序排列。</p><p><strong>2. 图示</strong></p><p> <img src="/2020/01/06/%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95/2-1G01610424A16.png" alt="img"> </p><p><strong>3. 代码实现</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><strong>4. 适用</strong></p><p> 要求是顺序表，分块查找又称索引顺序查找，它是顺序查找的一种改进方法，分块查找算法的效率介于顺序查找和二分查找之间。 </p><h4 id="斐波那契查找"><a href="#斐波那契查找" class="headerlink" title="斐波那契查找"></a>斐波那契查找</h4><p><strong>1.定义</strong></p><p><strong>2.图示</strong></p><p><strong>3.代码实现</strong></p><p><strong>4. 适用</strong></p><h4 id="树表查找"><a href="#树表查找" class="headerlink" title="树表查找"></a>树表查找</h4><p><strong>1.定义</strong></p><p><strong>2.图示</strong></p><p><strong>3.代码实现</strong></p><p><strong>4. 适用</strong></p><h4 id="哈希查找"><a href="#哈希查找" class="headerlink" title="哈希查找"></a>哈希查找</h4><p><strong>1.定义</strong></p><p><strong>2.图示</strong></p><p><strong>3.代码实现</strong></p><p><strong>4. 适用</strong></p><h4 id="时间复杂度对比"><a href="#时间复杂度对比" class="headerlink" title="时间复杂度对比"></a>时间复杂度对比</h4>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;搜索&quot;&gt;&lt;a href=&quot;#搜索&quot; class=&quot;headerlink&quot; title=&quot;搜索&quot;&gt;&lt;/a&gt;搜索&lt;/h4&gt;&lt;p&gt;搜索是在一个项目集合中找到一个特定项目的算法过程。搜索通常的答案是真的或假的，因为该项目是否存在。 搜索的八大查找算法：顺序查找，二分查找，插值查找，分块查找， 斐波那契查找，树表查找，哈希查找。&lt;/p&gt;
    
    </summary>
    
    
      <category term="搜索查找算法" scheme="https://xiaoliaozi.com/categories/%E6%90%9C%E7%B4%A2%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="搜索查找算法" scheme="https://xiaoliaozi.com/tags/%E6%90%9C%E7%B4%A2%E6%9F%A5%E6%89%BE%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>基数排序</title>
    <link href="https://xiaoliaozi.com/2020/01/05/%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F/"/>
    <id>https://xiaoliaozi.com/2020/01/05/%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F/</id>
    <published>2020-01-05T11:50:49.000Z</published>
    <updated>2020-01-05T12:08:58.780Z</updated>
    
    <content type="html"><![CDATA[<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p> 基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序。最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前。 </p><a id="more"></a><h4 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h4><ul><li>取得数组中的最大数，并取得位数；</li><li>arr为原始数组，从最低位开始取每个位组成radix数组；</li><li>对radix进行计数排序（利用计数排序适用于小范围数的特点）；</li></ul><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#基于桶排序的基数排序</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">radixSort</span><span class="params">(list)</span>:</span></span><br><span class="line">    d = len(str(max(list)))</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(d):<span class="comment">#d轮排序</span></span><br><span class="line">        s=[[] <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>)]<span class="comment">#因为每一位数字都是0~9，故建立10个桶</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> list:</span><br><span class="line">            s[int(i/(<span class="number">10</span>**k)%<span class="number">10</span>)].append(i)</span><br><span class="line">        list=[j <span class="keyword">for</span> i <span class="keyword">in</span> s <span class="keyword">for</span> j <span class="keyword">in</span> i]</span><br><span class="line">    <span class="keyword">return</span> list</span><br><span class="line"></span><br><span class="line">nums = [<span class="number">5</span>, <span class="number">6</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">8</span>, <span class="number">0</span>, <span class="number">65</span>]</span><br><span class="line">print(radixSort(nums))</span><br></pre></td></tr></table></figure><h4 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h4><h4 id="时间复杂度-1"><a href="#时间复杂度-1" class="headerlink" title="时间复杂度"></a>时间复杂度</h4><ul><li>最优时间复杂度：O(n*k)</li><li>最坏时间复杂度：O(n*k)</li><li>稳定性：稳定</li></ul><h4 id="动画演示"><a href="#动画演示" class="headerlink" title="动画演示"></a>动画演示</h4><p> <img src="/2020/01/05/%E5%9F%BA%E6%95%B0%E6%8E%92%E5%BA%8F/849589-20171015232453668-1397662527.gif" alt="img"> </p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;定义&quot;&gt;&lt;a href=&quot;#定义&quot; class=&quot;headerlink&quot; title=&quot;定义&quot;&gt;&lt;/a&gt;定义&lt;/h4&gt;&lt;p&gt; 基数排序是按照低位先排序，然后收集；再按照高位排序，然后再收集；依次类推，直到最高位。有时候有些属性是有优先级顺序的，先按低优先级排序，再按高优先级排序。最后的次序就是高优先级高的在前，高优先级相同的低优先级高的在前。 &lt;/p&gt;
    
    </summary>
    
    
      <category term="排序算法" scheme="https://xiaoliaozi.com/categories/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="排序算法" scheme="https://xiaoliaozi.com/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>桶排序</title>
    <link href="https://xiaoliaozi.com/2020/01/05/%E6%A1%B6%E6%8E%92%E5%BA%8F/"/>
    <id>https://xiaoliaozi.com/2020/01/05/%E6%A1%B6%E6%8E%92%E5%BA%8F/</id>
    <published>2020-01-05T07:28:24.000Z</published>
    <updated>2020-01-05T11:55:39.620Z</updated>
    
    <content type="html"><![CDATA[<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p> 桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。桶排序 (Bucket sort)的工作的原理：假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排）。 </p><a id="more"></a><ol><li><p><strong>什么时候最快</strong></p><p>当输入的数据可以均匀的分配到每一个桶中。</p></li><li><p><strong>什么时候最慢</strong></p><p>当输入的数据被分配到了同一个桶中。</p></li></ol><h4 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h4><ul><li>设置一个定量的数组当作空桶；</li><li>遍历输入数据，并且把数据一个一个放到对应的桶里去；</li><li>对每个不是空的桶进行排序；</li><li>从不是空的桶里把排好序的数据拼接起来。 </li></ul><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bucketSort</span><span class="params">(nums, n)</span>:</span></span><br><span class="line">    <span class="comment"># 选择一个最大的数</span></span><br><span class="line">    <span class="keyword">if</span> max(nums) &gt; len(nums):</span><br><span class="line">        n = max(nums)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        n = len(nums) + <span class="number">1</span></span><br><span class="line">    mid = max(nums) // n</span><br><span class="line">    <span class="keyword">if</span> mid == <span class="number">0</span>:</span><br><span class="line">        mid = <span class="number">1</span></span><br><span class="line">    <span class="comment"># 创建一个元素全是0的列表, 当做桶</span></span><br><span class="line">    bucket = [[] <span class="keyword">for</span> i <span class="keyword">in</span> range(n + <span class="number">1</span>)]</span><br><span class="line">    <span class="comment"># 把所有元素放入桶中</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> nums:</span><br><span class="line">        bucket[int(i / mid)].append(i)</span><br><span class="line">    print(bucket)</span><br><span class="line">    sort_nums = []</span><br><span class="line">    <span class="comment"># 取出桶中的元素</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(bucket)):</span><br><span class="line">        <span class="keyword">if</span> len(bucket[j]) != <span class="number">0</span>:</span><br><span class="line">            <span class="comment"># 使用递归继续桶排序</span></span><br><span class="line">            <span class="keyword">if</span> len(bucket[j]) &gt;= <span class="number">2</span> <span class="keyword">and</span> len(set(nums)) != <span class="number">1</span>:</span><br><span class="line">                bucket[j] = bucketSort(bucket[j], n)</span><br><span class="line">            <span class="comment"># 取出排序好的元素</span></span><br><span class="line">            <span class="keyword">for</span> y <span class="keyword">in</span> bucket[j]:</span><br><span class="line">                sort_nums.append(y)</span><br><span class="line">    <span class="keyword">return</span> sort_nums</span><br><span class="line"></span><br><span class="line">nums = [<span class="number">5</span>, <span class="number">6</span>, <span class="number">3</span>, <span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">0</span>, <span class="number">8</span>, <span class="number">0</span>, <span class="number">65</span>]</span><br><span class="line">print(bucketSort(nums, <span class="number">5</span>))</span><br></pre></td></tr></table></figure><h4 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h4><ul><li>最优时间复杂度：O(n)</li><li>最坏时间复杂度：O(n^2)</li><li>稳定性：稳定</li></ul><h4 id="动画演示"><a href="#动画演示" class="headerlink" title="动画演示"></a>动画演示</h4><p> 元素分布在桶中： </p><p> <img src="/2020/01/05/%E6%A1%B6%E6%8E%92%E5%BA%8F/Bucket_sort_1.svg_.png" alt="img"></p><p>然后，元素在每个桶中排序</p><p> <img src="/2020/01/05/%E6%A1%B6%E6%8E%92%E5%BA%8F/Bucket_sort_2.svg_.png" alt="img"></p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;定义&quot;&gt;&lt;a href=&quot;#定义&quot; class=&quot;headerlink&quot; title=&quot;定义&quot;&gt;&lt;/a&gt;定义&lt;/h4&gt;&lt;p&gt; 桶排序是计数排序的升级版。它利用了函数的映射关系，高效与否的关键就在于这个映射函数的确定。桶排序 (Bucket sort)的工作的原理：假设输入数据服从均匀分布，将数据分到有限数量的桶里，每个桶再分别排序（有可能再使用别的排序算法或是以递归方式继续使用桶排序进行排）。 &lt;/p&gt;
    
    </summary>
    
    
      <category term="排序算法" scheme="https://xiaoliaozi.com/categories/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="排序算法" scheme="https://xiaoliaozi.com/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>计数排序</title>
    <link href="https://xiaoliaozi.com/2020/01/05/%E8%AE%A1%E6%95%B0%E6%8E%92%E5%BA%8F/"/>
    <id>https://xiaoliaozi.com/2020/01/05/%E8%AE%A1%E6%95%B0%E6%8E%92%E5%BA%8F/</id>
    <published>2020-01-05T06:35:55.000Z</published>
    <updated>2020-01-05T11:50:00.354Z</updated>
    
    <content type="html"><![CDATA[<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p> 计数排序不是基于比较的排序算法，其核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。 </p><a id="more"></a><h4 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h4><ul><li>找出待排序的数组中最大和最小的元素；</li><li>统计数组中每个值为i的元素出现的次数，存入数组C的第i项；</li><li>对所有的计数累加（从C中的第一个元素开始，每一项和前一项相加）；</li><li>反向填充目标数组：将每个元素i放在新数组的第C(i)项，每放一个元素就将C(i)减去1。</li></ul><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><p><strong>数字排序</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countSort</span><span class="params">(arr)</span>:</span></span><br><span class="line">    <span class="comment"># 输出序列</span></span><br><span class="line">    <span class="keyword">if</span> max(arr) &gt; len(arr):</span><br><span class="line">        n = max(arr) + <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        n = len(arr) + <span class="number">1</span></span><br><span class="line">    output = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line">    <span class="comment"># 计数序列</span></span><br><span class="line">    count = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> arr:</span><br><span class="line">        count[i] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr)):</span><br><span class="line">        count[i + <span class="number">1</span>] += count[i]</span><br><span class="line">    print(count)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr)):</span><br><span class="line">        <span class="comment"># 下标从0开始</span></span><br><span class="line">        output[count[arr[i]] - <span class="number">1</span>] = arr[i]</span><br><span class="line">        count[arr[i]] -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> (output[:len(arr)])</span><br><span class="line"></span><br><span class="line">arr = [<span class="number">5</span>,<span class="number">6</span>,<span class="number">3</span>,<span class="number">2</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">0</span>,<span class="number">8</span>,<span class="number">0</span>]</span><br><span class="line">ans = countSort(arr)</span><br><span class="line">print(ans)</span><br></pre></td></tr></table></figure><p><strong>字母排序</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">countSort</span><span class="params">(arr)</span>:</span></span><br><span class="line">    n = len(arr)</span><br><span class="line">    <span class="comment"># 输出序列</span></span><br><span class="line">    output = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">256</span>)]</span><br><span class="line">    <span class="comment"># 计数序列</span></span><br><span class="line">    count = [<span class="number">0</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">256</span>)]</span><br><span class="line">    ans = [<span class="string">""</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(n)]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> arr:</span><br><span class="line">        count[ord(i)] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">256</span>):</span><br><span class="line">        count[i] += count[i - <span class="number">1</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">        <span class="comment"># 下标从0开始</span></span><br><span class="line">        output[count[ord(arr[i])] - <span class="number">1</span>] = arr[i]</span><br><span class="line">        count[ord(arr[i])] -= <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(arr)):</span><br><span class="line">        ans[i] = output[i]</span><br><span class="line">    <span class="keyword">return</span> ans</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">arr = <span class="string">"wwwrunoobcom"</span></span><br><span class="line">ans = countSort(arr)</span><br><span class="line">print(<span class="string">"字符数组排序 %s"</span> % (<span class="string">""</span>.join(ans)))</span><br></pre></td></tr></table></figure><h4 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h4><ul><li>最优时间复杂度：O(n+k)</li><li>最坏时间复杂度：O(n+k)</li><li>稳定性：稳定</li></ul><h4 id="动画演示"><a href="#动画演示" class="headerlink" title="动画演示"></a>动画演示</h4><p> <img src="/2020/01/05/%E8%AE%A1%E6%95%B0%E6%8E%92%E5%BA%8F/countingSort.gif" alt="img"> </p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;定义&quot;&gt;&lt;a href=&quot;#定义&quot; class=&quot;headerlink&quot; title=&quot;定义&quot;&gt;&lt;/a&gt;定义&lt;/h4&gt;&lt;p&gt; 计数排序不是基于比较的排序算法，其核心在于将输入的数据值转化为键存储在额外开辟的数组空间中。 作为一种线性时间复杂度的排序，计数排序要求输入的数据必须是有确定范围的整数。 &lt;/p&gt;
    
    </summary>
    
    
      <category term="排序算法" scheme="https://xiaoliaozi.com/categories/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="排序算法" scheme="https://xiaoliaozi.com/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>堆排序</title>
    <link href="https://xiaoliaozi.com/2020/01/04/%E5%A0%86%E6%8E%92%E5%BA%8F/"/>
    <id>https://xiaoliaozi.com/2020/01/04/%E5%A0%86%E6%8E%92%E5%BA%8F/</id>
    <published>2020-01-04T07:13:49.000Z</published>
    <updated>2020-01-04T11:44:32.270Z</updated>
    
    <content type="html"><![CDATA[<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>堆排序（Heapsort）是指利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于（或者大于）它的父节点。堆排序可以说是一种利用堆的概念来排序的选择排序。 </p><p><strong>堆</strong>是一种完全二叉树， <strong>堆</strong>有两种类型: <strong>大根堆</strong>  <strong>小根堆</strong>，两种类型的概念如下：<br> 大根堆：每个结点的值都大于或等于左右孩子结点<br> 小根堆：每个结点的值都小于或等于左右孩子结点</p> <a id="more"></a><p>大根堆 <img src="/2020/01/04/%E5%A0%86%E6%8E%92%E5%BA%8F/3401773-06756e8ab766dcff.webp" alt="大根堆"> </p><p> 小根堆<img src="/2020/01/04/%E5%A0%86%E6%8E%92%E5%BA%8F/3401773-cb6fe3ac34af3183.webp" alt="小根堆"></p><p> <strong>完全二叉树</strong>：是 一种除了最后一层之外的其他每一层都被完全填充，并且所有结点都保持向左对齐的树 </p><h4 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h4><ul><li>首先将待排序的数组构造出一个大根堆</li><li>取出这个大根堆的堆顶节点(最大值)，与堆的最下最右的元素进行交换，然后把剩下的元素再构造出一个大根堆</li><li>重复第二步，直到这个大根堆的长度为1，此时完成排序。</li></ul><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">heapify</span><span class="params">(arr, n, i)</span>:</span></span><br><span class="line">    largest = i</span><br><span class="line">    l = <span class="number">2</span> * i + <span class="number">1</span>  <span class="comment"># left = 2*i + 1</span></span><br><span class="line">    r = <span class="number">2</span> * i + <span class="number">2</span>  <span class="comment"># right = 2*i + 2</span></span><br><span class="line">    <span class="keyword">if</span> l &lt; n <span class="keyword">and</span> arr[i] &lt; arr[l]:</span><br><span class="line">        largest = l</span><br><span class="line">    <span class="keyword">if</span> r &lt; n <span class="keyword">and</span> arr[largest] &lt; arr[r]:</span><br><span class="line">        largest = r</span><br><span class="line">    <span class="keyword">if</span> largest != i:</span><br><span class="line">        arr[i], arr[largest] = arr[largest], arr[i]  <span class="comment"># 交换</span></span><br><span class="line">        heapify(arr, n, largest)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">heapSort</span><span class="params">(arr)</span>:</span></span><br><span class="line">    n = len(arr)</span><br><span class="line">    <span class="comment"># Build a maxheap.</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n, <span class="number">-1</span>, <span class="number">-1</span>):</span><br><span class="line">        print(i)</span><br><span class="line">        heapify(arr, n, i)</span><br><span class="line">    <span class="comment"># 一个个交换元素</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n - <span class="number">1</span>, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">        arr[i], arr[<span class="number">0</span>] = arr[<span class="number">0</span>], arr[i]  <span class="comment"># 交换</span></span><br><span class="line">        heapify(arr, i, <span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">arr = [<span class="number">12</span>, <span class="number">11</span>, <span class="number">13</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line">heapSort(arr)</span><br><span class="line">print(<span class="string">"排序后"</span>, arr)</span><br></pre></td></tr></table></figure><h4 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h4><ul><li>最优时间复杂度：O(nlogn) （升序排列，序列已经处于升序状态）</li><li>最坏时间复杂度：O(nlogn)</li><li>稳定性：不稳定</li></ul><h4 id="动画演示"><a href="#动画演示" class="headerlink" title="动画演示"></a>动画演示</h4><p> <img src="/2020/01/04/%E5%A0%86%E6%8E%92%E5%BA%8F/849589-20171015231308699-356134237.gif" alt="img"> </p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;定义&quot;&gt;&lt;a href=&quot;#定义&quot; class=&quot;headerlink&quot; title=&quot;定义&quot;&gt;&lt;/a&gt;定义&lt;/h4&gt;&lt;p&gt;堆排序（Heapsort）是指利用堆这种数据结构所设计的一种排序算法。堆积是一个近似完全二叉树的结构，并同时满足堆积的性质：即子结点的键值或索引总是小于（或者大于）它的父节点。堆排序可以说是一种利用堆的概念来排序的选择排序。 &lt;/p&gt;
&lt;p&gt;&lt;strong&gt;堆&lt;/strong&gt;是一种完全二叉树， &lt;strong&gt;堆&lt;/strong&gt;有两种类型: &lt;strong&gt;大根堆&lt;/strong&gt;  &lt;strong&gt;小根堆&lt;/strong&gt;，两种类型的概念如下：&lt;br&gt; 大根堆：每个结点的值都大于或等于左右孩子结点&lt;br&gt; 小根堆：每个结点的值都小于或等于左右孩子结点&lt;/p&gt;
    
    </summary>
    
    
      <category term="排序算法" scheme="https://xiaoliaozi.com/categories/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="排序算法" scheme="https://xiaoliaozi.com/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>归并排序</title>
    <link href="https://xiaoliaozi.com/2020/01/02/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/"/>
    <id>https://xiaoliaozi.com/2020/01/02/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/</id>
    <published>2020-01-02T08:23:12.000Z</published>
    <updated>2020-01-04T11:18:04.108Z</updated>
    
    <content type="html"><![CDATA[<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>归并排序是采用分治法的一个非常典型的应用。归并排序的思想就是先递归分解数组，再合并数组。将数组分解最小之后，然后合并两个有序数组，基本思路是比较两个数组的最前面的数，谁小就先取谁，取了后相应的指针就往后移一位。然后再比较，直至一个数组为空，最后把另一个数组的剩余部分复制过来即可<br><a id="more"></a></p><h4 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h4><ul><li>把长度为n的输入序列分成两个长度为n/2的子序列；</li><li>对这两个子序列分别采用归并排序；</li><li>将两个排序好的子序列合并成一个最终的排序序列。</li></ul><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge_sort</span><span class="params">(alist)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(alist) &lt;= <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> alist</span><br><span class="line">    <span class="comment"># 二分分解</span></span><br><span class="line">    num = len(alist)/<span class="number">2</span></span><br><span class="line">    left = merge_sort(alist[:num])</span><br><span class="line">    right = merge_sort(alist[num:])</span><br><span class="line">    <span class="comment"># 合并</span></span><br><span class="line">    <span class="keyword">return</span> merge(left,right)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">merge</span><span class="params">(left, right)</span>:</span></span><br><span class="line">    <span class="string">'''合并操作，将两个有序数组left[]和right[]合并成一个大的有序数组'''</span></span><br><span class="line">    <span class="comment">#left与right的下标指针</span></span><br><span class="line">    l, r = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">while</span> l&lt;len(left) <span class="keyword">and</span> r&lt;len(right):</span><br><span class="line">        <span class="keyword">if</span> left[l] &lt;= right[r]:</span><br><span class="line">            result.append(left[l])</span><br><span class="line">            l += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            result.append(right[r])</span><br><span class="line">            r += <span class="number">1</span></span><br><span class="line">    result += left[l:]</span><br><span class="line">    result += right[r:]</span><br><span class="line">    <span class="keyword">return</span> result</span><br><span class="line"></span><br><span class="line">alist = [<span class="number">54</span>,<span class="number">26</span>,<span class="number">93</span>,<span class="number">17</span>,<span class="number">77</span>,<span class="number">31</span>,<span class="number">44</span>,<span class="number">55</span>,<span class="number">20</span>]</span><br><span class="line">sorted_alist = mergeSort(alist)</span><br><span class="line">print(sorted_alist)</span><br></pre></td></tr></table></figure><h4 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h4><ul><li>最优时间复杂度：O(nlogn)</li><li>最坏时间复杂度：O(nlogn)</li><li>稳定性：稳定</li></ul><h4 id="动画演示"><a href="#动画演示" class="headerlink" title="动画演示"></a>动画演示</h4><p> <img src="/2020/01/02/%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F/Merge-sort-example.gif" alt="Merge-sort-example"> </p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;定义&quot;&gt;&lt;a href=&quot;#定义&quot; class=&quot;headerlink&quot; title=&quot;定义&quot;&gt;&lt;/a&gt;定义&lt;/h4&gt;&lt;p&gt;归并排序是采用分治法的一个非常典型的应用。归并排序的思想就是先递归分解数组，再合并数组。将数组分解最小之后，然后合并两个有序数组，基本思路是比较两个数组的最前面的数，谁小就先取谁，取了后相应的指针就往后移一位。然后再比较，直至一个数组为空，最后把另一个数组的剩余部分复制过来即可&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="排序算法" scheme="https://xiaoliaozi.com/categories/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="排序算法" scheme="https://xiaoliaozi.com/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>快速排序</title>
    <link href="https://xiaoliaozi.com/2020/01/02/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/"/>
    <id>https://xiaoliaozi.com/2020/01/02/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/</id>
    <published>2020-01-02T08:22:21.000Z</published>
    <updated>2020-01-04T11:16:28.146Z</updated>
    
    <content type="html"><![CDATA[<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p><strong>快速排序</strong>（英语：Quicksort），又称划分交换排序（partition-exchange sort），通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。  使用分治法来把一个串（list）分为两个子串（sub-lists） 。<br><a id="more"></a></p><h4 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h4><ul><li>从数列中挑出一个元素，称为”基准”（pivot），</li><li>重新排序数列，所有元素比基准值小的摆放在基准前面，所有元素比基准值大的摆在基准的后面（相同的数可以到任一边）。在这个分区结束之后，该基准就处于数列的中间位置。这个称为分区（partition）操作。</li><li>递归地（recursive）把小于基准值元素的子数列和大于基准值元素的子数列排序。</li></ul><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">quick_sort</span><span class="params">(alist, start, end)</span>:</span></span><br><span class="line">    <span class="string">"""快速排序"""</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 递归的退出条件</span></span><br><span class="line">    <span class="keyword">if</span> start &gt;= end:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="comment"># 设定起始元素为要寻找位置的基准元素</span></span><br><span class="line">    mid = alist[start]</span><br><span class="line">    <span class="comment"># low为序列左边的由左向右移动的游标</span></span><br><span class="line">    low = start</span><br><span class="line">    <span class="comment"># high为序列右边的由右向左移动的游标</span></span><br><span class="line">    high = end</span><br><span class="line">    <span class="keyword">while</span> low &lt; high:</span><br><span class="line">        <span class="comment"># 如果low与high未重合，high指向的元素不比基准元素小，则high向左移动</span></span><br><span class="line">        <span class="keyword">while</span> low &lt; high <span class="keyword">and</span> alist[high] &gt;= mid:</span><br><span class="line">            high -= <span class="number">1</span></span><br><span class="line">        <span class="comment"># 将high指向的元素放到low的位置上</span></span><br><span class="line">        alist[low] = alist[high]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 如果low与high未重合，low指向的元素比基准元素小，则low向右移动</span></span><br><span class="line">        <span class="keyword">while</span> low &lt; high <span class="keyword">and</span> alist[low] &lt; mid:</span><br><span class="line">            low += <span class="number">1</span></span><br><span class="line">        <span class="comment"># 将low指向的元素放到high的位置上</span></span><br><span class="line">        alist[high] = alist[low]</span><br><span class="line">    <span class="comment"># 退出循环后，low与high重合，此时所指位置为基准元素的正确位置</span></span><br><span class="line">    <span class="comment"># 将基准元素放到该位置</span></span><br><span class="line">    alist[low] = mid</span><br><span class="line">    <span class="comment"># 对基准元素左边的子序列进行快速排序</span></span><br><span class="line">    quick_sort(alist, start, low<span class="number">-1</span>)</span><br><span class="line">    <span class="comment"># 对基准元素右边的子序列进行快速排序</span></span><br><span class="line">    quick_sort(alist, low+<span class="number">1</span>, end)</span><br><span class="line"></span><br><span class="line">alist = [<span class="number">54</span>,<span class="number">26</span>,<span class="number">93</span>,<span class="number">17</span>,<span class="number">77</span>,<span class="number">31</span>,<span class="number">44</span>,<span class="number">55</span>,<span class="number">20</span>]</span><br><span class="line">quick_sort(alist,<span class="number">0</span>,len(alist)<span class="number">-1</span>)</span><br><span class="line">print(alist)</span><br></pre></td></tr></table></figure><h4 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h4><ul><li>最优时间复杂度：O(nlogn)</li><li>最坏时间复杂度：O(n2)</li><li>稳定性：不稳定</li></ul><h4 id="动画演示"><a href="#动画演示" class="headerlink" title="动画演示"></a>动画演示</h4><p> <img src="/2020/01/02/%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F/quicksort.gif" alt="quicksort"> </p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;定义&quot;&gt;&lt;a href=&quot;#定义&quot; class=&quot;headerlink&quot; title=&quot;定义&quot;&gt;&lt;/a&gt;定义&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;快速排序&lt;/strong&gt;（英语：Quicksort），又称划分交换排序（partition-exchange sort），通过一趟排序将要排序的数据分割成独立的两部分，其中一部分的所有数据都比另外一部分的所有数据都要小，然后再按此方法对这两部分数据分别进行快速排序，整个排序过程可以递归进行，以此达到整个数据变成有序序列。  使用分治法来把一个串（list）分为两个子串（sub-lists） 。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="排序算法" scheme="https://xiaoliaozi.com/categories/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="排序算法" scheme="https://xiaoliaozi.com/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>希尔排序</title>
    <link href="https://xiaoliaozi.com/2020/01/02/%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F/"/>
    <id>https://xiaoliaozi.com/2020/01/02/%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F/</id>
    <published>2020-01-02T08:08:00.000Z</published>
    <updated>2020-01-04T11:17:02.949Z</updated>
    
    <content type="html"><![CDATA[<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>希尔排序(Shell Sort)是插入排序的一种。也称<strong>缩小增量排序</strong>，是直接插入排序算法的一种更高效的改进版本。希尔排序是非稳定排序算法。该方法因DL．Shell于1959年提出而得名。 希尔排序是把记录按下标的一定增量分组，对每组使用直接插入排序算法排序；随着增量逐渐减少，每组包含的关键词越来越多，当增量减至1时，整个文件恰被分成一组，算法便终止。</p><p>希尔排序的基本思想是：将数组列在一个表中并对列分别进行插入排序，重复这过程，不过每次用更长的列（步长更长了，列数更少了）来进行。最后整个表就只有一列了。将数组转换至表是为了更好地理解这算法，算法本身还是使用数组进行排序。<br><a id="more"></a></p><h4 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h4><ul><li>选择一个增量序列t1，t2，…，tk，其中ti&gt;tj，tk=1；</li><li>按增量序列个数k，对序列进行k 趟排序；</li><li>每趟排序，根据对应的增量ti，将待排序列分割成若干长度为m 的子序列，分别对各子表进行直接插入排序。仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。</li></ul><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">shell_sort</span><span class="params">(alist)</span>:</span></span><br><span class="line">    n = len(alist)</span><br><span class="line">    <span class="comment"># 初始步长</span></span><br><span class="line">    gap = n//<span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> gap &gt; <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># 按步长进行插入排序</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(gap, n):</span><br><span class="line">            j = i</span><br><span class="line">            <span class="comment"># 插入排序</span></span><br><span class="line">            <span class="keyword">while</span> j&gt;=gap <span class="keyword">and</span> alist[j-gap] &gt; alist[j]:</span><br><span class="line">                alist[j-gap], alist[j] = alist[j], alist[j-gap]</span><br><span class="line">                j -= gap</span><br><span class="line">        <span class="comment"># 得到新的步长</span></span><br><span class="line">        gap = gap // <span class="number">2</span></span><br><span class="line"></span><br><span class="line">alist = [<span class="number">54</span>,<span class="number">26</span>,<span class="number">93</span>,<span class="number">17</span>,<span class="number">77</span>,<span class="number">31</span>,<span class="number">44</span>,<span class="number">55</span>,<span class="number">20</span>]</span><br><span class="line">shell_sort(alist)</span><br><span class="line">print(alist)</span><br></pre></td></tr></table></figure><h4 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h4><ul><li>最优时间复杂度：根据步长序列的不同而不同</li><li>最坏时间复杂度：O(n2)</li><li>稳定性：不稳定</li></ul><h4 id="动画演示"><a href="#动画演示" class="headerlink" title="动画演示"></a>动画演示</h4><p> <img src="/2020/01/02/%E5%B8%8C%E5%B0%94%E6%8E%92%E5%BA%8F/shellsort.gif" alt="shellsort"> </p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;定义&quot;&gt;&lt;a href=&quot;#定义&quot; class=&quot;headerlink&quot; title=&quot;定义&quot;&gt;&lt;/a&gt;定义&lt;/h4&gt;&lt;p&gt;希尔排序(Shell Sort)是插入排序的一种。也称&lt;strong&gt;缩小增量排序&lt;/strong&gt;，是直接插入排序算法的一种更高效的改进版本。希尔排序是非稳定排序算法。该方法因DL．Shell于1959年提出而得名。 希尔排序是把记录按下标的一定增量分组，对每组使用直接插入排序算法排序；随着增量逐渐减少，每组包含的关键词越来越多，当增量减至1时，整个文件恰被分成一组，算法便终止。&lt;/p&gt;
&lt;p&gt;希尔排序的基本思想是：将数组列在一个表中并对列分别进行插入排序，重复这过程，不过每次用更长的列（步长更长了，列数更少了）来进行。最后整个表就只有一列了。将数组转换至表是为了更好地理解这算法，算法本身还是使用数组进行排序。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="排序算法" scheme="https://xiaoliaozi.com/categories/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="排序算法" scheme="https://xiaoliaozi.com/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>插入排序</title>
    <link href="https://xiaoliaozi.com/2020/01/02/%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/"/>
    <id>https://xiaoliaozi.com/2020/01/02/%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/</id>
    <published>2020-01-02T07:53:12.000Z</published>
    <updated>2020-01-04T11:44:16.564Z</updated>
    
    <content type="html"><![CDATA[<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p><strong>插入排序</strong>（英语：Insertion Sort）是一种简单直观的排序算法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。插入排序在实现上，在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。<br> <a id="more"></a></p><h4 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h4><ul><li>从第一个元素开始，该元素可以认为已经被排序；</li><li>取出下一个元素，在已经排序的元素序列中从后向前扫描；</li><li>如果该元素（已排序）大于新元素，将该元素移到下一位置；</li><li>重复步骤3，直到找到已排序的元素小于或者等于新元素的位置；</li><li>将新元素插入到该位置后；</li><li>重复步骤2~5。</li></ul><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">insert_sort</span><span class="params">(alist)</span>:</span></span><br><span class="line">    <span class="comment"># 从第二个位置，即下标为1的元素开始向前插入</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(alist)):</span><br><span class="line">        <span class="comment"># 从第i个元素开始向前比较，如果小于前一个元素，交换位置</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">            <span class="keyword">if</span> alist[j] &lt; alist[j<span class="number">-1</span>]:</span><br><span class="line">                alist[j], alist[j<span class="number">-1</span>] = alist[j<span class="number">-1</span>], alist[j]</span><br><span class="line"></span><br><span class="line">alist = [<span class="number">54</span>,<span class="number">26</span>,<span class="number">93</span>,<span class="number">17</span>,<span class="number">77</span>,<span class="number">31</span>,<span class="number">44</span>,<span class="number">55</span>,<span class="number">20</span>]</span><br><span class="line">insert_sort(alist)</span><br><span class="line">print(alist)</span><br></pre></td></tr></table></figure><h4 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h4><ul><li>最优时间复杂度：O(n) （升序排列，序列已经处于升序状态）</li><li>最坏时间复杂度：O(n2)</li><li>稳定性：稳定</li></ul><h4 id="动画演示"><a href="#动画演示" class="headerlink" title="动画演示"></a>动画演示</h4><p>​    <img src="/2020/01/02/%E6%8F%92%E5%85%A5%E6%8E%92%E5%BA%8F/insert.gif" alt="insert"></p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;定义&quot;&gt;&lt;a href=&quot;#定义&quot; class=&quot;headerlink&quot; title=&quot;定义&quot;&gt;&lt;/a&gt;定义&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;插入排序&lt;/strong&gt;（英语：Insertion Sort）是一种简单直观的排序算法。它的工作原理是通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应位置并插入。插入排序在实现上，在从后向前扫描过程中，需要反复把已排序元素逐步向后挪位，为最新元素提供插入空间。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="排序算法" scheme="https://xiaoliaozi.com/categories/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="排序算法" scheme="https://xiaoliaozi.com/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>选择排序</title>
    <link href="https://xiaoliaozi.com/2020/01/02/%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/"/>
    <id>https://xiaoliaozi.com/2020/01/02/%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/</id>
    <published>2020-01-02T07:41:10.000Z</published>
    <updated>2020-01-04T11:15:34.160Z</updated>
    
    <content type="html"><![CDATA[<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p><strong>选择排序（Selection sort）</strong>是一种简单直观的排序算法。它的工作原理如下。首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。</p><p>选择排序的主要优点与数据移动有关。如果某个元素位于正确的最终位置上，则它不会被移动。选择排序每次交换一对元素，它们当中至少有一个将被移到其最终位置上，因此对n个元素的表进行排序总共进行至多n-1次交换。在所有的完全依靠交换去移动元素的排序方法中，选择排序属于非常好的一种。<br><a id="more"></a></p><h4 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h4><ul><li>初始状态：无序区为R[1..n]，有序区为空；</li><li>第i趟排序(i=1,2,3…n-1)开始时，当前有序区和无序区分别为R[1..i-1]和R(i..n）。该趟排序从当前无序区中-选出关键字最小的记录 R[k]，将它与无序区的第1个记录R交换，使R[1..i]和R[i+1..n)分别变为记录个数增加1个的新有序区和记录个数减少1个的新无序区；</li><li>n-1趟结束，数组有序化了。</li></ul><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">selection_sort</span><span class="params">(alist)</span>:</span></span><br><span class="line">    n = len(alist)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n<span class="number">-1</span>):</span><br><span class="line">        <span class="comment"># 记录最小位置</span></span><br><span class="line">        min_index = i</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, n):</span><br><span class="line">            <span class="keyword">if</span> alist[j] &lt; alist[min_index]:</span><br><span class="line">                min_index = j</span><br><span class="line">        <span class="comment"># 如果选择出的数据不在正确位置，进行交换</span></span><br><span class="line">        <span class="keyword">if</span> min_index != i:</span><br><span class="line">            alist[i], alist[min_index] = alist[min_index], alist[i]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">alist = [<span class="number">54</span>,<span class="number">226</span>,<span class="number">93</span>,<span class="number">17</span>,<span class="number">77</span>,<span class="number">31</span>,<span class="number">44</span>,<span class="number">55</span>,<span class="number">20</span>]</span><br><span class="line">selection_sort(alist)</span><br><span class="line">print(alist)</span><br></pre></td></tr></table></figure><h4 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h4><ul><li>最优时间复杂度：O(n2)</li><li>最坏时间复杂度：O(n2)</li><li>稳定性：不稳定（考虑升序每次选择最大的情况）</li></ul><h4 id="动画演示"><a href="#动画演示" class="headerlink" title="动画演示"></a>动画演示</h4><p> <img src="/2020/01/02/%E9%80%89%E6%8B%A9%E6%8E%92%E5%BA%8F/selection.gif" alt="selection"> </p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;定义&quot;&gt;&lt;a href=&quot;#定义&quot; class=&quot;headerlink&quot; title=&quot;定义&quot;&gt;&lt;/a&gt;定义&lt;/h4&gt;&lt;p&gt;&lt;strong&gt;选择排序（Selection sort）&lt;/strong&gt;是一种简单直观的排序算法。它的工作原理如下。首先在未排序序列中找到最小（大）元素，存放到排序序列的起始位置，然后，再从剩余未排序元素中继续寻找最小（大）元素，然后放到已排序序列的末尾。以此类推，直到所有元素均排序完毕。&lt;/p&gt;
&lt;p&gt;选择排序的主要优点与数据移动有关。如果某个元素位于正确的最终位置上，则它不会被移动。选择排序每次交换一对元素，它们当中至少有一个将被移到其最终位置上，因此对n个元素的表进行排序总共进行至多n-1次交换。在所有的完全依靠交换去移动元素的排序方法中，选择排序属于非常好的一种。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="排序算法" scheme="https://xiaoliaozi.com/categories/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="排序算法" scheme="https://xiaoliaozi.com/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>冒泡排序</title>
    <link href="https://xiaoliaozi.com/2020/01/02/%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/"/>
    <id>https://xiaoliaozi.com/2020/01/02/%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/</id>
    <published>2020-01-02T07:17:10.000Z</published>
    <updated>2020-01-04T11:44:24.308Z</updated>
    
    <content type="html"><![CDATA[<h4 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h4><p>冒泡排序是一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。<br> <a id="more"></a></p><h4 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h4><ul><li>比较相邻的元素。如果第一个比第二个大（升序），就交换它们两个；</li><li>对每一对相邻元素作同样的工作，从开始第一对到结尾的最后一对，这样在最后的元素应该会是最大的数；</li><li>针对所有的元素重复以上的步骤，除了最后一个；</li><li>重复步骤1~3，直到排序完成。</li></ul><h4 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bubble_sort</span><span class="params">(alist)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(len(alist)<span class="number">-1</span>, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(j):</span><br><span class="line">            <span class="keyword">if</span> alist[i] &gt; alist[i+<span class="number">1</span>]:</span><br><span class="line">                alist[i], alist[i+<span class="number">1</span>] = alist[i+<span class="number">1</span>], alist[i]</span><br><span class="line"></span><br><span class="line">li = [<span class="number">54</span>, <span class="number">26</span>, <span class="number">93</span>, <span class="number">17</span>, <span class="number">77</span>, <span class="number">31</span>, <span class="number">44</span>, <span class="number">55</span>, <span class="number">20</span>]</span><br><span class="line">bubble_sort(li)</span><br><span class="line">print(li)</span><br></pre></td></tr></table></figure><h4 id="时间复杂度"><a href="#时间复杂度" class="headerlink" title="时间复杂度"></a>时间复杂度</h4><ul><li>最优时间复杂度：O(n) （表示遍历一次发现没有任何可以交换的元素，排序结束。）</li><li>最坏时间复杂度：O(n2)</li><li>稳定性：稳定</li></ul><h4 id="动画演示"><a href="#动画演示" class="headerlink" title="动画演示"></a>动画演示</h4><p> <img src="/2020/01/02/%E5%86%92%E6%B3%A1%E6%8E%92%E5%BA%8F/bubble-1578136149213.gif" alt="冒泡排序"></p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;定义&quot;&gt;&lt;a href=&quot;#定义&quot; class=&quot;headerlink&quot; title=&quot;定义&quot;&gt;&lt;/a&gt;定义&lt;/h4&gt;&lt;p&gt;冒泡排序是一种简单的排序算法。它重复地走访过要排序的数列，一次比较两个元素，如果它们的顺序错误就把它们交换过来。走访数列的工作是重复地进行直到没有再需要交换，也就是说该数列已经排序完成。这个算法的名字由来是因为越小的元素会经由交换慢慢“浮”到数列的顶端。&lt;br&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="排序算法" scheme="https://xiaoliaozi.com/categories/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="排序算法" scheme="https://xiaoliaozi.com/tags/%E6%8E%92%E5%BA%8F%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
</feed>
