<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>小廖子的博客</title>
  
  <subtitle>好记性不如记笔记</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://xiaoliaozi.com/"/>
  <updated>2020-05-30T13:08:08.811Z</updated>
  <id>https://xiaoliaozi.com/</id>
  
  <author>
    <name>小廖子</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>C++30天学习打卡（一）</title>
    <link href="https://xiaoliaozi.com/2020/05/30/C-30%E5%A4%A9%E5%AD%A6%E4%B9%A0%E6%89%93%E5%8D%A1%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>https://xiaoliaozi.com/2020/05/30/C-30%E5%A4%A9%E5%AD%A6%E4%B9%A0%E6%89%93%E5%8D%A1%EF%BC%88%E4%B8%80%EF%BC%89/</id>
    <published>2020-05-30T13:04:43.000Z</published>
    <updated>2020-05-30T13:08:08.811Z</updated>
    
    <content type="html"><![CDATA[<h3 id="C-30天学习打卡（一）"><a href="#C-30天学习打卡（一）" class="headerlink" title="C++30天学习打卡（一）"></a>C++30天学习打卡（一）</h3><h4 id="1-注释"><a href="#1-注释" class="headerlink" title="1.注释"></a>1.注释</h4><blockquote><ol><li>单行注释：//  描述信息</li><li>多行注释：/<em> 注释信息 </em>/</li></ol></blockquote><h4 id="2-变量"><a href="#2-变量" class="headerlink" title="2.变量"></a>2.变量</h4><ol><li><p>作用：给一段指定的内存空间起名，方便操作这段内存。</p></li><li><p>语法：数据类型   变量名 = 初始值；</p></li><li><p>示例：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span><span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"><span class="keyword">int</span> a = <span class="number">10</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"a = "</span> &lt;&lt; a &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">system(<span class="string">"pause"</span>);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// 注意：C++在创建变量时，必须给变量一个初始值，否则会报错</span></span><br></pre></td></tr></table></figure></li></ol><a id="more"></a><h4 id="3-常量"><a href="#3-常量" class="headerlink" title="3.常量"></a>3.常量</h4><ol><li><p>作用：用于记录不可更改的数据；</p></li><li><p>定义常量的两种方式：</p><blockquote><ol><li>#define 宏常量：#define  常量名  常量值<ul><li>== 通常在文件上方定义==，表示一个常量</li></ul></li><li>const 修饰的变量：const 数据类型  变量名 = 常量值<ul><li>==通常在变量定义前加关键字const==，修饰该变量为常量，不可修改</li></ul></li></ol></blockquote></li><li><p>示例：</p><figure class="highlight c++"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">define</span> day 7</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 常量定义</span></span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"一周共有 "</span> &lt;&lt; day &lt;&lt; <span class="string">" 天"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span> month = <span class="number">12</span>;</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"一年有几个 "</span> &lt;&lt; month &lt;&lt; <span class="string">" 月"</span> &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">system(<span class="string">"pause"</span>);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></li></ol><h4 id="4-关键字"><a href="#4-关键字" class="headerlink" title="4.关键字"></a>4.关键字</h4><ol><li><p>作用：关键字是C++中预先保留的单词（标识符）</p></li><li><p>注意：在定义变量或者常量时候，不要用关键字，否则会产生歧义</p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200530204651.png"/></p></li></ol><h4 id="5-标识符命名规则"><a href="#5-标识符命名规则" class="headerlink" title="5.标识符命名规则"></a>5.标识符命名规则</h4><ol><li><p>作用：C++规定给标识符（变量、常量）命名时，有一套自己的规则；</p></li><li><p>注意事项：</p><blockquote><ul><li>标识符不能是关键字</li><li>标识符只能由字母、数字、下划线组成</li><li>第一个字符必须为字母或下划线</li><li>标识符中字母区分大小写</li></ul></blockquote></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;C-30天学习打卡（一）&quot;&gt;&lt;a href=&quot;#C-30天学习打卡（一）&quot; class=&quot;headerlink&quot; title=&quot;C++30天学习打卡（一）&quot;&gt;&lt;/a&gt;C++30天学习打卡（一）&lt;/h3&gt;&lt;h4 id=&quot;1-注释&quot;&gt;&lt;a href=&quot;#1-注释&quot; class=&quot;headerlink&quot; title=&quot;1.注释&quot;&gt;&lt;/a&gt;1.注释&lt;/h4&gt;&lt;blockquote&gt;
&lt;ol&gt;
&lt;li&gt;单行注释：//  描述信息&lt;/li&gt;
&lt;li&gt;多行注释：/&lt;em&gt; 注释信息 &lt;/em&gt;/&lt;/li&gt;
&lt;/ol&gt;
&lt;/blockquote&gt;
&lt;h4 id=&quot;2-变量&quot;&gt;&lt;a href=&quot;#2-变量&quot; class=&quot;headerlink&quot; title=&quot;2.变量&quot;&gt;&lt;/a&gt;2.变量&lt;/h4&gt;&lt;ol&gt;
&lt;li&gt;&lt;p&gt;作用：给一段指定的内存空间起名，方便操作这段内存。&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;语法：数据类型   变量名 = 初始值；&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;&lt;p&gt;示例：&lt;/p&gt;
&lt;figure class=&quot;highlight c++&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;8&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;9&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;10&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;meta&quot;&gt;#&lt;span class=&quot;meta-keyword&quot;&gt;include&lt;/span&gt;&lt;span class=&quot;meta-string&quot;&gt;&amp;lt;iostream&amp;gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;using&lt;/span&gt; &lt;span class=&quot;keyword&quot;&gt;namespace&lt;/span&gt; &lt;span class=&quot;built_in&quot;&gt;std&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;function&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;main&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;()&lt;/span&gt; &lt;/span&gt;&amp;#123;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;keyword&quot;&gt;int&lt;/span&gt; a = &lt;span class=&quot;number&quot;&gt;10&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;built_in&quot;&gt;cout&lt;/span&gt; &amp;lt;&amp;lt; &lt;span class=&quot;string&quot;&gt;&quot;a = &quot;&lt;/span&gt; &amp;lt;&amp;lt; a &amp;lt;&amp;lt; &lt;span class=&quot;built_in&quot;&gt;endl&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	system(&lt;span class=&quot;string&quot;&gt;&quot;pause&quot;&lt;/span&gt;);&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;	&lt;span class=&quot;keyword&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;number&quot;&gt;0&lt;/span&gt;;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&amp;#125;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;comment&quot;&gt;// 注意：C++在创建变量时，必须给变量一个初始值，否则会报错&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;/li&gt;
&lt;/ol&gt;
    
    </summary>
    
    
      <category term="C++30天学习打卡" scheme="https://xiaoliaozi.com/categories/C-30%E5%A4%A9%E5%AD%A6%E4%B9%A0%E6%89%93%E5%8D%A1/"/>
    
    
      <category term="C++" scheme="https://xiaoliaozi.com/tags/C/"/>
    
  </entry>
  
  <entry>
    <title>关于Hive</title>
    <link href="https://xiaoliaozi.com/2020/02/13/%E5%85%B3%E4%BA%8EHive/"/>
    <id>https://xiaoliaozi.com/2020/02/13/%E5%85%B3%E4%BA%8EHive/</id>
    <published>2020-02-13T07:03:26.000Z</published>
    <updated>2020-02-28T12:05:46.006Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Hive简介"><a href="#Hive简介" class="headerlink" title="Hive简介"></a>Hive简介</h4><h5 id="什么是Hive"><a href="#什么是Hive" class="headerlink" title="什么是Hive"></a>什么是Hive</h5><ul><li>Hive 由 Facebook 实现并开源，是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据映射为一张数据库表，并提供 HQL(Hive SQL)查询功能，底层数据是存储在 HDFS 上。</li><li>Hive 本质: 将 SQL 语句转换为 MapReduce 任务运行，使不熟悉 MapReduce 的用户很方便地利用 HQL 处理和计算 HDFS 上的结构化的数据，<strong>是一款基于HDFS 的 MapReduce 计算框架</strong></li><li>主要用途：用来做离线数据分析，比直接用 MapReduce 开发效率更高。</li></ul><a id="more"></a><h5 id="为什么使用Hive"><a href="#为什么使用Hive" class="headerlink" title="为什么使用Hive"></a>为什么使用Hive</h5><ul><li>直接使用 Hadoop MapReduce 处理数据所面临的问题：<ul><li>人员学习成本太高</li><li>MapReduce 实现复杂查询逻辑开发难度太大</li></ul></li><li>使用 Hive<ul><li>操作接口采用类 SQL 语法，提供快速开发的能力</li><li>避免了去写 MapReduce，减少开发人员的学习成本</li><li>功能扩展很方便</li></ul></li></ul><h4 id="Hive架构"><a href="#Hive架构" class="headerlink" title="Hive架构"></a>Hive架构</h4><h5 id="Hive-架构图"><a href="#Hive-架构图" class="headerlink" title="Hive 架构图"></a>Hive 架构图</h5><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200221213219.jpg"/></p><h5 id="Hive-组件"><a href="#Hive-组件" class="headerlink" title="Hive 组件"></a>Hive 组件</h5><ul><li>用户接口：包括 CLI、JDBC/ODBC、WebGUI。<ul><li>CLI(command line interface)为 shell 命令行</li><li>JDBC/ODBC 是 Hive 的 JAVA 实现，与传统数据库JDBC 类似</li><li>WebGUI 是通过浏览器访问 Hive。</li><li>HiveServer2基于Thrift，允许远程客户端使用多种编程语言如Java、Python向Hive提交请求</li></ul></li><li>元数据存储：通常是存储在关系数据库如 mysql/derby 中。<ul><li>Hive 将元数据存储在数据库中。</li><li>Hive 中的元数据包括<ul><li>表的名字</li><li>表的列</li><li>分区及其属性</li><li>表的属性（是否为外部表等）</li><li>表的数据所在目录等。</li></ul></li></ul></li><li>解释器、编译器、优化器、执行器：完成 HQL 查询语句从词法分析、语法分析、编译、优化以及查询计划的生成。生成的查询计划存储在 HDFS 中，并在随后由 MapReduce 调用执行</li></ul><h5 id="Hive与Hadoop的关系"><a href="#Hive与Hadoop的关系" class="headerlink" title="Hive与Hadoop的关系"></a>Hive与Hadoop的关系</h5><ul><li>Hive 利用 HDFS 存储数据，利用 MapReduce 查询分析数据。</li><li>Hive是数据仓库工具，没有集群的概念，</li><li>如果想提交Hive作业只需要在hadoop集群 Master节点上装Hive就可以了</li></ul><h4 id="Hive与传统数据库对比"><a href="#Hive与传统数据库对比" class="headerlink" title="Hive与传统数据库对比"></a>Hive与传统数据库对比</h4><ul><li>hive 用于海量数据的离线数据分析 </li></ul><div class="table-container"><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">Hive</th><th style="text-align:center">关系型数据库</th></tr></thead><tbody><tr><td style="text-align:center">ANSI SQL</td><td style="text-align:center">不完全支持</td><td style="text-align:center">支持</td></tr><tr><td style="text-align:center">更新</td><td style="text-align:center">INSERT OVERWRITE\INTO TABLE(默认)</td><td style="text-align:center">UPDATE\INSERT\DELETE</td></tr><tr><td style="text-align:center">事务</td><td style="text-align:center">不支持(默认)</td><td style="text-align:center">支持</td></tr><tr><td style="text-align:center">模式</td><td style="text-align:center">读模式</td><td style="text-align:center">写模式</td></tr><tr><td style="text-align:center">查询语言</td><td style="text-align:center">HQL</td><td style="text-align:center">SQL</td></tr><tr><td style="text-align:center">数据存储</td><td style="text-align:center">HDFS</td><td style="text-align:center">Raw Device or Local FS</td></tr><tr><td style="text-align:center">执行</td><td style="text-align:center">MapReduce</td><td style="text-align:center">Executor</td></tr><tr><td style="text-align:center">执行延迟</td><td style="text-align:center">高</td><td style="text-align:center">低</td></tr><tr><td style="text-align:center">子查询</td><td style="text-align:center">只能用在From子句中</td><td style="text-align:center">完全支持</td></tr><tr><td style="text-align:center">处理数据规模</td><td style="text-align:center">大</td><td style="text-align:center">小</td></tr><tr><td style="text-align:center">可扩展性</td><td style="text-align:center">高</td><td style="text-align:center">低</td></tr><tr><td style="text-align:center">索引</td><td style="text-align:center">0.8版本后加入位图索引</td><td style="text-align:center">有复杂的索引</td></tr></tbody></table></div><ul><li>hive支持的数据类型<ul><li>原子数据类型<ul><li>TINYINT SMALLINT INT BIGINT BOOLEAN FLOAT DOUBLE STRING BINARY TIMESTAMP DECIMAL CHAR VARCHAR DATE</li></ul></li><li>复杂数据类型<ul><li>ARRAY  MAP  STRUCT</li></ul></li></ul></li><li>hive中表的类型<ul><li>托管表 (managed table) (内部表)</li><li>外部表</li></ul></li></ul><h4 id="Hive-数据模型"><a href="#Hive-数据模型" class="headerlink" title="Hive 数据模型"></a>Hive 数据模型</h4><ul><li>Hive 中所有的数据都存储在 HDFS 中，没有专门的数据存储格式</li><li>在创建表时指定数据中的分隔符，Hive 就可以映射成功，解析数据。</li><li>Hive 中包含以下数据模型：<ul><li>db：在 hdfs 中表现为 hive.metastore.warehouse.dir 目录下一个文件夹</li><li>table：在 hdfs 中表现所属 db 目录下一个文件夹</li><li>external table：数据存放位置可以在 HDFS 任意指定路径</li><li>partition：在 hdfs 中表现为 table 目录下的子目录</li><li>bucket：在 hdfs 中表现为同一个表目录下根据 hash 散列之后的多个文件</li></ul></li></ul><h4 id="Hive安装部署"><a href="#Hive安装部署" class="headerlink" title="Hive安装部署"></a>Hive安装部署</h4><ul><li><p>Hive 安装前需要安装好 JDK 和 Hadoop。配置好环境变量。 </p></li><li><p>下载Hive的安装包 <a href="http://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/</a> 并解压 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf hive-1.1.0-cdh5.7.0.tar.gz  -C ~/app/</span><br></pre></td></tr></table></figure></li><li><p>进入到 解压后的hive目录 找到 conf目录, 修改配置文件 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cp hive-env.sh.template hive-env.sh</span><br><span class="line">vi hive-env.sh</span><br><span class="line"><span class="meta">#</span><span class="bash"> 在hive-env.sh中指定hadoop的路径</span></span><br><span class="line">HADOOP_HOME=/root/bigdata/hadoop</span><br></pre></td></tr></table></figure></li><li><p>配置环境变量 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">vi ~/.bash_profile</span><br><span class="line"></span><br><span class="line">export HIVE_HOME=/root/bigdata/hive</span><br><span class="line">export PATH=$HIVE_HOME/bin:$PATH</span><br><span class="line"></span><br><span class="line">source ~/.bash_profile</span><br></pre></td></tr></table></figure></li><li><p>根据元数据存储的介质不同，分为下面两个版本，其中 derby 属于内嵌模式。实际生产环境中则使用 mysql 来进行元数据的存储。</p><ul><li><p>内置 derby 版： bin/hive 启动即可使用 缺点：不同路径启动 hive，每一个 hive 拥有一套自己的元数据，无法共享 </p></li><li><p>mysql 版 </p><ul><li>上传 mysql驱动到 hive安装目录的lib目录下   mysql-connector-java-5.*.jar </li><li>vi conf/hive-site.xml 配置 Mysql 元数据库信息(MySql安装见文档) </li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;</span><br><span class="line">&lt;?xml-stylesheet type="text/xsl" href="configuration.xsl"?&gt;</span><br><span class="line">&lt;configuration&gt;</span><br><span class="line">&lt;!-- 插入以下代码 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;root&lt;/value&gt;&lt;!-- 指定mysql用户名 --&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;password&lt;/value&gt;&lt;!-- 指定mysql密码 --&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">   &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;mysql</span><br><span class="line">        &lt;value&gt;jdbc:mysql://127.0.0.1:3306/hive&lt;/value&gt;</span><br><span class="line">    &lt;/property&gt;&lt;!-- 指定mysql数据库地址 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;</span><br><span class="line">        &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;&lt;!-- 指定mysql驱动 --&gt;</span><br><span class="line">    &lt;/property&gt;</span><br><span class="line">        &lt;!-- 到此结束代码 --&gt;</span><br><span class="line">  &lt;property&gt;</span><br><span class="line">    &lt;name&gt;hive.exec.script.wrapper&lt;/name&gt;</span><br><span class="line">    &lt;value/&gt;</span><br><span class="line">    &lt;description/&gt;</span><br><span class="line">  &lt;/property&gt;</span><br><span class="line">&lt;/configuration&gt;</span><br></pre></td></tr></table></figure></li></ul></li><li><p>hive启动 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> 启动docker</span></span><br><span class="line">service docker start</span><br><span class="line"><span class="meta">#</span><span class="bash"> 通过docker 启动mysql</span></span><br><span class="line">docker start mysql</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动 hive的metastore元数据服务</span></span><br><span class="line">hive --service metastore</span><br><span class="line"><span class="meta">#</span><span class="bash"> 启动hive</span></span><br><span class="line">hive</span><br></pre></td></tr></table></figure></li></ul><h4 id="Hive基本操作"><a href="#Hive基本操作" class="headerlink" title="Hive基本操作"></a>Hive基本操作</h4><ul><li><p>创建数据库 </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">DATABASE</span> <span class="keyword">test</span>;</span><br></pre></td></tr></table></figure></li><li><p>显示所有数据库 </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">SHOW</span> <span class="keyword">DATABASES</span>;</span><br></pre></td></tr></table></figure></li><li><p>创建表 </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">CREATE</span> <span class="keyword">TABLE</span> student(classNo <span class="keyword">string</span>, stuNo <span class="keyword">string</span>, score <span class="built_in">int</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span>;</span><br></pre></td></tr></table></figure><ul><li>row format delimited fields terminated by ‘,’ 指定了字段的分隔符为逗号</li><li>所以load数据的时候，load的文本也要为逗号，否则加载后为NULL。</li><li>hive只支持单个字符的分隔符，hive默认的分隔符是\001 </li></ul></li><li><p>将数据load到表中 </p><ul><li>在本地文件系统创建一个如下的文本文件：/home/hadoop/tmp/student.txt </li></ul><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="attr"># 数据</span></span><br><span class="line"><span class="attr">C01</span>,<span class="symbol">N0101</span>,<span class="number">82</span></span><br><span class="line">C<span class="number">01</span>,<span class="symbol">N0102</span>,<span class="number">59</span></span><br><span class="line">C<span class="number">01</span>,<span class="symbol">N0103</span>,<span class="number">65</span></span><br><span class="line">C<span class="number">02</span>,<span class="symbol">N0201</span>,<span class="number">81</span></span><br><span class="line">C<span class="number">02</span>,<span class="symbol">N0202</span>,<span class="number">82</span></span><br><span class="line">C<span class="number">02</span>,<span class="symbol">N0203</span>,<span class="number">79</span></span><br><span class="line">C<span class="number">03</span>,<span class="symbol">N0301</span>,<span class="number">56</span></span><br><span class="line">C<span class="number">03</span>,<span class="symbol">N0302</span>,<span class="number">92</span></span><br><span class="line">C<span class="number">03</span>,<span class="symbol">N0306</span>,<span class="number">72</span></span><br></pre></td></tr></table></figure><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/home/hadoop/tmp/student.txt'</span>overwrite <span class="keyword">into</span> <span class="keyword">table</span> student;</span><br></pre></td></tr></table></figure><ul><li>这个命令将student.txt文件复制到hive的warehouse目录中</li><li>这个目录由hive.metastore.warehouse.dir配置项设置，默认值为/user/hive/warehouse。</li><li>Overwrite选项将导致Hive事先删除student目录下所有的文件，并将文件内容映射到表中。</li><li>Hive不会对student.txt做任何格式处理，因为Hive本身并不强调数据的存储格式。 </li></ul></li><li><p>查询表中的数据 跟SQL类似 </p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;select * from student;</span><br></pre></td></tr></table></figure></li><li><p>分组查询group by和统计 count </p><figure class="highlight"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hive&gt;select classNo,count(score) from student where score&gt;=60 group by classNo;</span><br></pre></td></tr></table></figure></li></ul><h4 id="Hive的内部表和外部表"><a href="#Hive的内部表和外部表" class="headerlink" title="Hive的内部表和外部表"></a>Hive的内部表和外部表</h4><div class="table-container"><table><thead><tr><th></th><th>内部表(managed table)</th><th>外部表(external table)</th></tr></thead><tbody><tr><td>概念</td><td>创建表时无external修饰</td><td>创建表时被external修饰</td></tr><tr><td>数据管理</td><td>由Hive自身管理</td><td>由HDFS管理</td></tr><tr><td>数据保存位置</td><td>hive.metastore.warehouse.dir （默认：/user/hive/warehouse）</td><td>hdfs中任意位置</td></tr><tr><td>删除时影响</td><td>直接删除元数据（metadata）及存储数据</td><td>仅会删除元数据，HDFS上的文件并不会被删除</td></tr><tr><td>表结构修改时影响</td><td>修改会将修改直接同步给元数据</td><td>表结构和分区进行修改，则需要修复（MSCK REPAIR TABLE table_name;）</td></tr></tbody></table></div><p><strong>小案例</strong></p><ul><li><p>创建一个外部表student2 </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建外部表</span></span><br><span class="line"><span class="keyword">CREATE</span> <span class="keyword">EXTERNAL</span> <span class="keyword">TABLE</span> student2 (classNo <span class="keyword">string</span>, stuNo <span class="keyword">string</span>, score <span class="built_in">int</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">','</span> location <span class="string">'/tmp/student'</span>;</span><br><span class="line"><span class="comment"># 装载数据</span></span><br><span class="line"><span class="keyword">load</span> <span class="keyword">data</span> <span class="keyword">local</span> inpath <span class="string">'/root/tmp/student.txt'</span> overwrite <span class="keyword">into</span> <span class="keyword">table</span> student2;</span><br></pre></td></tr></table></figure></li><li><p>显示表信息 </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">desc formatted table_name;</span><br></pre></td></tr></table></figure></li><li><p>删除表查看结果 </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">drop</span> <span class="keyword">table</span> student;</span><br></pre></td></tr></table></figure></li><li><p>再次创建外部表 student2 </p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不插入数据直接查询查看结果</span></span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> student2;</span><br><span class="line"><span class="comment"># 能查到数据吗？能</span></span><br><span class="line"><span class="comment"># 外部表只删除元数据，不会删除储存数据</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="Hive分区表"><a href="#Hive分区表" class="headerlink" title="Hive分区表"></a>Hive分区表</h4><ul><li><p>什么是分区表 </p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span>随着表的不断增大，对于新纪录的增加，查找，删除等(DML)的维护也更加困难。对于数据库中的超大型表，可以通过把它的数据分成若干个小表，从而简化数据库的管理活动，对于每一个简化后的小表，我们称为一个单个的分区。</span><br><span class="line"><span class="number">2.</span>hive中分区表实际就是对应hdfs文件系统上独立的文件夹，该文件夹内的文件是该分区所有数据文件。</span><br><span class="line"><span class="number">3.</span>分区可以理解为分类，通过分类把不同类型的数据放到不同的目录下。</span><br><span class="line"><span class="number">4.</span>分类的标准就是分区字段，可以一个，也可以多个。</span><br><span class="line"><span class="number">5.</span>分区表的意义在于优化查询。查询时尽量利用分区字段。如果不使用分区字段，就会全部扫描。</span><br></pre></td></tr></table></figure></li><li><p>创建分区表</p></li></ul><ul><li><p>查看表的分区</p></li><li><p>添加分区</p></li><li><p>加载数据到分区</p></li><li><p>如果重复加载同名文件，不会报错，会自动创建一个*_copy_1.txt </p></li><li><p>外部分区表即使有分区的目录结构, 也必须要通过hql添加分区, 才能看到相应的数据 </p></li></ul><p><strong>总结</strong></p><blockquote><p>利用分区表方式减少查询时需要扫描的数据量</p><ul><li>分区字段不是表中的列, 数据文件中没有对应的列</li><li>分区仅仅是一个目录名</li><li>查看数据时, hive会自动添加分区列</li><li>支持多级分区, 多级子目录</li></ul></blockquote><h4 id="Hive动态分区"><a href="#Hive动态分区" class="headerlink" title="Hive动态分区"></a>Hive动态分区</h4><ul><li>在写入数据时自动创建分区(包括目录结构)</li><li>创建表 </li><li>导入数据</li><li>使用动态分区需要设置参数</li></ul><h4 id="Hive函数"><a href="#Hive函数" class="headerlink" title="Hive函数"></a>Hive函数</h4><h5 id="内置运算符"><a href="#内置运算符" class="headerlink" title="内置运算符"></a>内置运算符</h5><h5 id="内置函数"><a href="#内置函数" class="headerlink" title="内置函数"></a>内置函数</h5><h5 id="Hive-自定义函数和-Transform"><a href="#Hive-自定义函数和-Transform" class="headerlink" title="Hive 自定义函数和 Transform"></a>Hive 自定义函数和 Transform</h5><h4 id="Hive综合案例"><a href="#Hive综合案例" class="headerlink" title="Hive综合案例"></a>Hive综合案例</h4><h4 id="拓展-ANSI-SQL"><a href="#拓展-ANSI-SQL" class="headerlink" title="拓展-ANSI SQL"></a>拓展-ANSI SQL</h4><p>了解一下即可，一般做ORM的需要深入学习，对象关系映射（Object Relational Mapping，简称ORM）模式是一种为了解决面向对象与关系数据库存在的互不匹配的现象的技术。简单的说，ORM是通过使用描述对象和数据库之间映射的元数据，将程序中的对象自动持久化到关系数据库中。 </p><ul><li><p>ANSI：美国国家标准化组织，是一个核准多种行业标准的组织。</p></li><li><p>SQL</p><ul><li>结构化查询语言</li><li>是与关系型数据库进行通信的标准语言</li><li>最初由IBM公司的E.F.Codd博士论文为原型开发出来的。</li></ul></li><li><p>ANSI SQL：国际标准SQL语法</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;Hive简介&quot;&gt;&lt;a href=&quot;#Hive简介&quot; class=&quot;headerlink&quot; title=&quot;Hive简介&quot;&gt;&lt;/a&gt;Hive简介&lt;/h4&gt;&lt;h5 id=&quot;什么是Hive&quot;&gt;&lt;a href=&quot;#什么是Hive&quot; class=&quot;headerlink&quot; title=&quot;什么是Hive&quot;&gt;&lt;/a&gt;什么是Hive&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;Hive 由 Facebook 实现并开源，是基于 Hadoop 的一个数据仓库工具，可以将结构化的数据映射为一张数据库表，并提供 HQL(Hive SQL)查询功能，底层数据是存储在 HDFS 上。&lt;/li&gt;
&lt;li&gt;Hive 本质: 将 SQL 语句转换为 MapReduce 任务运行，使不熟悉 MapReduce 的用户很方便地利用 HQL 处理和计算 HDFS 上的结构化的数据，&lt;strong&gt;是一款基于HDFS 的 MapReduce 计算框架&lt;/strong&gt;&lt;/li&gt;
&lt;li&gt;主要用途：用来做离线数据分析，比直接用 MapReduce 开发效率更高。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="大数据推荐系统" scheme="https://xiaoliaozi.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="Hive" scheme="https://xiaoliaozi.com/tags/Hive/"/>
    
      <category term="Hive架构" scheme="https://xiaoliaozi.com/tags/Hive%E6%9E%B6%E6%9E%84/"/>
    
      <category term="Hive与Hadoop的关系" scheme="https://xiaoliaozi.com/tags/Hive%E4%B8%8EHadoop%E7%9A%84%E5%85%B3%E7%B3%BB/"/>
    
      <category term="Hive与传统数据库对比" scheme="https://xiaoliaozi.com/tags/Hive%E4%B8%8E%E4%BC%A0%E7%BB%9F%E6%95%B0%E6%8D%AE%E5%BA%93%E5%AF%B9%E6%AF%94/"/>
    
      <category term="Hive基本操作" scheme="https://xiaoliaozi.com/tags/Hive%E5%9F%BA%E6%9C%AC%E6%93%8D%E4%BD%9C/"/>
    
      <category term="Hive函数" scheme="https://xiaoliaozi.com/tags/Hive%E5%87%BD%E6%95%B0/"/>
    
  </entry>
  
  <entry>
    <title>HADOOP概念扩展</title>
    <link href="https://xiaoliaozi.com/2020/02/12/HADOOP%E6%A6%82%E5%BF%B5%E6%89%A9%E5%B1%95/"/>
    <id>https://xiaoliaozi.com/2020/02/12/HADOOP%E6%A6%82%E5%BF%B5%E6%89%A9%E5%B1%95/</id>
    <published>2020-02-12T01:45:09.000Z</published>
    <updated>2020-02-27T12:37:53.482Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Hadoop生态系统"><a href="#Hadoop生态系统" class="headerlink" title="Hadoop生态系统"></a>Hadoop生态系统</h4><h5 id="狭义的Hadoop-VS-广义的Hadoop"><a href="#狭义的Hadoop-VS-广义的Hadoop" class="headerlink" title="狭义的Hadoop VS 广义的Hadoop"></a>狭义的Hadoop VS 广义的Hadoop</h5><p> 广义的Hadoop：指的是Hadoop生态系统，Hadoop生态系统是一个很庞大的概念，hadoop是其中最重要最基础的一个部分，生态系统中每一子系统只解决某一个特定的问题域（甚至可能更窄），不搞统一型的全能系统，而是小而精的多个小系统。</p><a id="more"></a><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227203527.png"/> </p><ul><li>Hive：数据仓库</li><li>R：数据分析</li><li>Mahout：机器学习库</li><li>pig：脚本语言，跟Hive类似</li><li>Oozie：工作流引擎，管理作业执行顺序</li><li>Zookeeper：用户无感知，主节点挂掉选择从节点作为主的</li><li>Flume：日志收集框架</li><li>Sqoop：数据交换框架，例如：关系型数据库与HDFS之间的数据交换</li><li>Hbase : 海量数据中的查询，相当于分布式文件系统中的数据库</li><li><p>Spark: 分布式的计算框架基于内存</p><ul><li>spark core</li><li>spark sql</li><li>spark streaming 准实时 不算是一个标准的流式计算</li><li>spark ML spark MLlib</li></ul></li><li><p>Kafka: 消息队列</p></li><li>Storm: 分布式的流式计算框架 python操作storm</li><li>Flink: 分布式的流式计算框架</li></ul><h5 id="Hadoop生态系统的特点"><a href="#Hadoop生态系统的特点" class="headerlink" title="Hadoop生态系统的特点"></a><strong>Hadoop生态系统的特点</strong></h5><ul><li>开源、社区活跃</li><li>囊括了大数据处理的方方面面</li><li>成熟的生态圈</li></ul><h4 id="HDFS-读写流程-amp-高可用"><a href="#HDFS-读写流程-amp-高可用" class="headerlink" title="HDFS 读写流程&amp; 高可用"></a>HDFS 读写流程&amp; 高可用</h4><h5 id="HDFS读写流程"><a href="#HDFS读写流程" class="headerlink" title="HDFS读写流程"></a>HDFS读写流程</h5><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227203608.jpg"/> </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227203557.jpg"/> </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227203543.jpg"/> </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227203536.jpg"/> </p><ul><li>客户端向NameNode发出写文件请求。 </li><li>检查是否已存在文件、检查权限。若通过检查，直接先将操作写入EditLog，并返回输出流对象。 （注：WAL，write ahead log，先写Log，再写内存，因为EditLog记录的是最新的HDFS客户端执行所有的写操作。如果后续真实写操作失败了，由于在真实写操作之前，操作就被写入EditLog中了，故EditLog中仍会有记录，我们不用担心后续client读不到相应的数据块，因为在第5步中DataNode收到块后会有一返回确认信息，若没写成功，发送端没收到确认信息，会一直重试，直到成功）</li><li>client端按128MB的块切分文件。</li><li>client将NameNode返回的分配的可写的DataNode列表和Data数据一同发送给最近的第一个DataNode节点，此后client端和NameNode分配的多个DataNode构成pipeline管道，client端向输出流对象中写数据。client每向第一个DataNode写入一个packet，这个packet便会直接在pipeline里传给第二个、第三个…DataNode。 （注：并不是写好一个块或一整个文件后才向后分发）</li><li>每个DataNode写完一个块后，会返回确认信息。 （注：并不是每写完一个packet后就返回确认信息，个人觉得因为packet中的每个chunk都携带校验信息，没必要每写一个就汇报一下，这样效率太慢。正确的做法是写完一个block块后，对校验信息进行汇总分析，就能得出是否有块写错的情况发生）</li><li>写完数据，关闭输输出流。</li><li>发送完成信号给NameNode。（注：发送完成信号的时机取决于集群是强一致性还是最终一致性，强一致性则需要所有DataNode写完后才向NameNode汇报。最终一致性则其中任意一个DataNode写完后就能单独向NameNode汇报，HDFS一般情况下都是强调强一致性）。</li></ul><h5 id="HDFS如何实现高可用-HA"><a href="#HDFS如何实现高可用-HA" class="headerlink" title="HDFS如何实现高可用(HA)"></a>HDFS如何实现高可用(HA)</h5><ul><li><strong>数据存储故障容错</strong> <ul><li>磁盘介质在存储过程中受环境或者老化影响，数据可能错乱</li><li>对于存储在 DataNode 上的数据块，计算并存储校验和（CheckSum)</li><li>读取数据的时候，重新计算读取出来的数据校验和， 校验不正确抛出异常，从其它DataNode上读取备份数据</li></ul></li><li><strong>磁盘故障容错</strong><ul><li>DataNode 监测到本机的某块磁盘损坏</li><li>将该块磁盘上存储的所有 BlockID 报告给 NameNode</li><li>NameNode 检查这些数据块在哪些DataNode上有备份,</li><li>通知相应DataNode, 将数据复制到其他服务器上</li></ul></li><li><strong>DataNode故障容错</strong><ul><li>通过心跳和NameNode保持通讯</li><li>超时未发送心跳， NameNode会认为这个DataNode已经宕机</li><li>NameNode查找这个DataNode上有哪些数据块，以及这些数据在其它DataNode服务器上的存储情况</li><li>从其它DataNode服务器上复制数据</li></ul></li><li><strong>NameNode故障容错</strong><ul><li>主从热备 secondary namenode</li><li>zookeeper配合 master节点选举</li></ul></li></ul><h4 id="Hadoop发行版的选择"><a href="#Hadoop发行版的选择" class="headerlink" title="Hadoop发行版的选择"></a>Hadoop发行版的选择</h4><h5 id="社区版-Apache-Hadoop"><a href="#社区版-Apache-Hadoop" class="headerlink" title="社区版 Apache Hadoop"></a>社区版 Apache Hadoop</h5><ul><li><p>开源，技术最新；</p></li><li><p>最新的Hadoop版本都是从Apache Hadoop发布的，可以在 xxx.apache.org上进行软件的下载；</p></li><li><p>当涉及到的大数据工具比较多的时候，比较容易出现兼容性的问题，所以我们以后在选择发行版本的时候，<strong>一般选择CDH的版本</strong>，如果选择社区版可能因为兼容问题，耽误大量时间，并且可能还解决不了问题。</p></li></ul><h5 id="CDH"><a href="#CDH" class="headerlink" title="CDH"></a>CDH</h5><ul><li>部分内容没有开源，技术会有滞后</li><li>Cloudera 在社区版的基础上做了一些修改</li><li>在<a href="http://archive.cloudera.com/cdh5/cdh/5/进行下载" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/进行下载</a></li><li>hadoop-2.6.0-cdh-5.7.0 和 Flume<strong>*</strong>-cdh5.7.0 cdh版本一致 的各个组件配合是有不会有兼容性问题</li><li>一般情况下建议使用CDH版本</li></ul><h5 id="HDP"><a href="#HDP" class="headerlink" title="HDP"></a>HDP</h5><ul><li>商用的版本，用得比较少</li></ul><h4 id="大数据产品与互联网产品结合"><a href="#大数据产品与互联网产品结合" class="headerlink" title="大数据产品与互联网产品结合"></a>大数据产品与互联网产品结合</h4><p>1、分布式系统执行任务瓶颈: 延迟高 MapReduce 几分钟 Spark几秒钟</p><p>2、互联网产品要求</p><ul><li>毫秒级响应(1秒以内完成)</li><li>需要通过大数据实现 统计分析 数据挖掘 关联推荐 用户画像</li></ul><p>3、大数据平台</p><ul><li>整合网站应用和大数据系统之间的差异, 将应用产生的数据导入到大数据系统, 经过处理计算后再导出给应用程序使用</li></ul><p>4、互联网大数据平台架构</p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227203551.png"/> </p><p>5、数据采集</p><ul><li>App/Web 产生的数据&amp;日志同步到大数据系统</li><li>数据库同步:Sqoop，日志同步:Flume， 打点: Kafka</li><li>不同数据源产生的数据质量可能差别很大<ul><li>数据库 也许可以直接用</li><li>日志 爬虫 大量的清洗,转化处理</li></ul></li></ul><p>6、数据处理</p><ul><li>大数据存储与计算的核心</li><li>数据同步后导入HDFS</li><li>MapReduce Hive Spark 读取数据进行计算 结果再保存到HDFS</li><li>MapReduce Hive Spark 离线计算，HDFS 离线存储<ul><li>离线计算通常针对(某一类别)全体数据，比如 历史上所有订单</li><li>离线计算特点: 数据规模大, 运行时间长</li></ul></li><li>流式计算<ul><li>淘宝双11 每秒产生订单数 监控宣传</li><li>Storm(毫秒) SparkStreaming(秒)</li></ul></li></ul><p>7、数据输出与展示</p><ul><li>HDFS需要把数据导出交给应用程序，让用户实时展示 ECharts<ul><li>淘宝卖家量子魔方</li></ul></li><li>给运营和决策层提供各种统计报告，数据需要写入数据库<ul><li>很多运营管理人员，上班后就会登陆后台数据系统</li></ul></li></ul><p>8、任务调度系统</p><ul><li>将上面三个部分整合起来</li></ul><h4 id="拓展：大数据应用-数据分析"><a href="#拓展：大数据应用-数据分析" class="headerlink" title="拓展：大数据应用-数据分析"></a>拓展：大数据应用-数据分析</h4><p>1、通过数据分析指标监控企业运营状态，及时调整运营和产品策略,是大数据技术的关键价值之一</p><p>2、大数据平台(互联网企业)运行的绝大多数大数据计算都是关于数据分析的</p><ul><li>统计指标</li><li>关联分析,</li><li>汇总报告,</li></ul><p>3、运营数据是公司管理的基础</p><ul><li>了解公司目前发展的状况</li><li>数据驱动运营: 调节指标对公司进行管理</li></ul><p>4、运营数据的获取需要大数据平台的支持</p><ul><li>埋点采集数据</li><li>数据库，日志 三方采集数据</li><li>对数据清洗 转换 存储</li><li>利用SQL进行数据统计 汇总 分析</li><li>得到需要的运营数据报告</li></ul><p>5、运营常用数据指标</p><ul><li><p>新增用户数 UG user growth 用户增长</p><ul><li>产品增长性的关键指标</li><li>新增访问网站(新下载APP)的用户数</li></ul></li><li><p>用户留存率</p><ul><li>用户留存率 = 留存用户数 / 当期新增用户数</li><li>3日留存 5日留存 7日留存</li></ul></li><li><p>活跃用户数</p><ul><li>打开使用产品的用户</li><li>日活</li><li>月活</li><li>提升活跃是网站运营的重要目标</li></ul></li><li><p>PV Page View</p><ul><li>打开产品就算活跃</li><li>打开以后是否频繁操作就用PV衡量, 每次点击, 页面跳转都记一次PV</li></ul></li><li><p>GMV</p><ul><li>成交总金额(Gross Merchandise Volume) 电商网站统计营业额, 反应网站应收能力的重要指标</li><li>GMV相关的指标: 订单量 客单价</li></ul></li><li><p>转化率</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">转化率 = 有购买行为的用户数 / 总访问用户数</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;Hadoop生态系统&quot;&gt;&lt;a href=&quot;#Hadoop生态系统&quot; class=&quot;headerlink&quot; title=&quot;Hadoop生态系统&quot;&gt;&lt;/a&gt;Hadoop生态系统&lt;/h4&gt;&lt;h5 id=&quot;狭义的Hadoop-VS-广义的Hadoop&quot;&gt;&lt;a href=&quot;#狭义的Hadoop-VS-广义的Hadoop&quot; class=&quot;headerlink&quot; title=&quot;狭义的Hadoop VS 广义的Hadoop&quot;&gt;&lt;/a&gt;狭义的Hadoop VS 广义的Hadoop&lt;/h5&gt;&lt;p&gt; 广义的Hadoop：指的是Hadoop生态系统，Hadoop生态系统是一个很庞大的概念，hadoop是其中最重要最基础的一个部分，生态系统中每一子系统只解决某一个特定的问题域（甚至可能更窄），不搞统一型的全能系统，而是小而精的多个小系统。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据推荐系统" scheme="https://xiaoliaozi.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="Hadoop生态系统" scheme="https://xiaoliaozi.com/tags/Hadoop%E7%94%9F%E6%80%81%E7%B3%BB%E7%BB%9F/"/>
    
      <category term="HDFS 读写流程&amp;高可用" scheme="https://xiaoliaozi.com/tags/HDFS-%E8%AF%BB%E5%86%99%E6%B5%81%E7%A8%8B-%E9%AB%98%E5%8F%AF%E7%94%A8/"/>
    
      <category term="Hadoop发行版的选择" scheme="https://xiaoliaozi.com/tags/Hadoop%E5%8F%91%E8%A1%8C%E7%89%88%E7%9A%84%E9%80%89%E6%8B%A9/"/>
    
  </entry>
  
  <entry>
    <title>YARN和MAPREDUCE</title>
    <link href="https://xiaoliaozi.com/2020/02/12/YARN%E5%92%8CMAPREDUCE/"/>
    <id>https://xiaoliaozi.com/2020/02/12/YARN%E5%92%8CMAPREDUCE/</id>
    <published>2020-02-12T01:42:27.000Z</published>
    <updated>2020-02-27T12:24:04.354Z</updated>
    
    <content type="html"><![CDATA[<h4 id="资源调度框架-YARN"><a href="#资源调度框架-YARN" class="headerlink" title="资源调度框架 YARN"></a>资源调度框架 YARN</h4><h5 id="什么是YARN"><a href="#什么是YARN" class="headerlink" title="什么是YARN"></a>什么是YARN</h5><ul><li>Yet Another Resource Negotiator, 另一种资源协调者</li><li>通用资源管理系统</li><li>为上层应用提供统一的资源管理和调度，为集群在利用率、资源统一管理和数据共享等方面带来了巨大好处</li></ul><a id="more"></a><h5 id="YARN产生背景"><a href="#YARN产生背景" class="headerlink" title="YARN产生背景"></a>YARN产生背景</h5><ul><li><p>通用资源管理系统</p><ul><li>Hadoop数据分布式存储（数据分块，冗余存储）</li><li>当多个MapReduce任务要用到相同的hdfs数据， 需要进行资源调度管理</li><li>Hadoop1.x时并没有YARN，MapReduce 既负责进行计算作业又处理服务器集群资源调度管理</li></ul></li><li><p>服务器集群资源调度管理和MapReduce执行过程耦合在一起带来的问题</p><ul><li>Hadoop早期, 技术只有Hadoop, 这个问题不明显</li><li>随着大数据技术的发展，Spark Storm … 计算框架都要用到服务器集群资源</li><li><p>如果没有通用资源管理系统，只能为多个集群分别提供数据</p><ul><li>资源利用率低 运维成本高</li></ul></li><li><p>Yarn (Yet Another Resource Negotiator) 另一种资源调度器</p><ul><li>Mesos 大数据资源管理产品</li></ul></li></ul><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227171624.png"/></p></li><li><p>不同计算框架可以共享同一个HDFS集群上的数据，享受整体的资源调度 </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227171624.png"/></p></li></ul><h5 id="YARN的架构和执行流程"><a href="#YARN的架构和执行流程" class="headerlink" title="YARN的架构和执行流程"></a>YARN的架构和执行流程</h5><ul><li>ResourceManager: RM 资源管理器 整个集群同一时间提供服务的RM只有一个，负责集群资源的统一管理和调度 处理客户端的请求： submit, kill 监控我们的NM，一旦某个NM挂了，那么该NM上运行的任务需要告诉我们的AM来如何进行处理</li><li>NodeManager: NM 节点管理器 整个集群中有多个，负责自己本身节点资源管理和使用 定时向RM汇报本节点的资源使用情况 接收并处理来自RM的各种命令：启动Container 处理来自AM的命令</li><li>ApplicationMaster: AM 每个应用程序对应一个：MR、Spark，负责应用程序的管理 为应用程序向RM申请资源（core、memory），分配给内部task 需要与NM通信：启动/停止task，task是运行在container里面，AM也是运行在container里面</li><li>Container 容器: 封装了CPU、Memory等资源的一个容器,是一个任务运行环境的抽象</li><li><p>Client: 提交作业，查询作业的运行进度，杀死作业</p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227165923.png"/></p></li></ul><blockquote><p> 1.Client提交作业请求</p><p>2.ResourceManager 进程和 NodeManager 进程通信，根据集群资源，为用户程序分配第一个Container(容器)，并将 ApplicationMaster 分发到这个容器上面</p><p>3.在启动的Container中创建ApplicationMaster</p><p>4.ApplicationMaster启动后向ResourceManager注册进程,申请资源</p><p>5.ApplicationMaster申请到资源后，向对应的NodeManager申请启动Container，将要执行的程序分发到NodeManager上</p><p>6.Container启动后，执行对应的任务</p><p>7.Tast执行完毕之后，向ApplicationMaster返回结果</p><p>8.ApplicationMaster向ResourceManager 请求kill</p></blockquote><h5 id="YARN环境搭建"><a href="#YARN环境搭建" class="headerlink" title="YARN环境搭建"></a>YARN环境搭建</h5><p>1）mapred-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>2）yarn-site.xml</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><p>3) 启动YARN相关的进程 sbin/start-yarn.sh</p><p>4）验证 jps ResourceManager NodeManager <a href="http://192,168.19.137:8088" target="_blank" rel="noopener">http://192,168.19.137:8088</a></p><p>5）停止YARN相关的进程 sbin/stop-yarn.sh</p><h4 id="分布式处理框架-MapReduce"><a href="#分布式处理框架-MapReduce" class="headerlink" title="分布式处理框架 MapReduce"></a>分布式处理框架 MapReduce</h4><h5 id="什么是MapReduce"><a href="#什么是MapReduce" class="headerlink" title="什么是MapReduce"></a>什么是MapReduce</h5><ul><li>定义： 分布式计算框架</li><li>作用： 海量数据的离线处理</li><li>MapReduce优点: 海量数据离线处理&amp;易开发</li><li>MapReduce缺点: 实时流式计算</li></ul><h5 id="MapReduce编程模型"><a href="#MapReduce编程模型" class="headerlink" title="MapReduce编程模型"></a>MapReduce编程模型</h5><ul><li>MapReduce分而治之的思想 </li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">数钱实例：一堆钞票，各种面值分别是多少</span><br><span class="line">单点策略：一个人数所有的钞票，数出各种面值有多少张</span><br><span class="line">分治策略：</span><br><span class="line">每个人分得一堆钞票，数出各种面值有多少张</span><br><span class="line">汇总，每个人负责统计一种面值</span><br><span class="line">解决数据可以切割进行计算的应用</span><br></pre></td></tr></table></figure><ul><li>MapReduce编程分Map和Reduce阶段 <ul><li>将作业拆分成Map阶段和Reduce阶段</li><li>Map阶段 Map Tasks 分：把复杂的问题分解为若干”简单的任务”</li><li>Reduce阶段: Reduce Tasks 合：reduce</li></ul></li><li><p>MapReduce编程执行步骤 </p><ul><li>准备MapReduce的输入数据</li><li>准备Mapper数据</li><li>Shuffle</li><li>Reduce处理</li><li>结果输出</li></ul></li><li><p>编程模型 </p></li></ul><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">借鉴函数式编程方式</span><br><span class="line">用户只需要实现两个函数接口：</span><br><span class="line"><span class="constructor">Map(<span class="params">in_key</span>,<span class="params">in_value</span>)</span></span><br><span class="line">---&gt;(out_key,intermediate_value) <span class="built_in">list</span></span><br><span class="line"><span class="constructor">Reduce(<span class="params">out_key</span>,<span class="params">intermediate_value</span>)</span> <span class="built_in">list</span></span><br><span class="line">---&gt;out_value <span class="built_in">list</span></span><br></pre></td></tr></table></figure><ul><li>Word Count 词频统计案例 </li></ul><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227171715.png"/> </p><h4 id="MapReduce实战"><a href="#MapReduce实战" class="headerlink" title="MapReduce实战"></a>MapReduce实战</h4><h5 id="MRJob编写和运行MapReduce代码"><a href="#MRJob编写和运行MapReduce代码" class="headerlink" title="MRJob编写和运行MapReduce代码"></a>MRJob编写和运行MapReduce代码</h5><p>1.<strong>mrjob 简介</strong></p><ul><li>使用python开发在Hadoop上运行的程序, mrjob是最简单的方式</li><li>mrjob程序可以在本地测试运行也可以部署到Hadoop集群上运行</li><li>如果不想成为hadoop专家, 但是需要利用Hadoop写MapReduce代码，mrJob是很好的选择</li></ul><p>2.<strong>mrjob 安装</strong></p><ul><li>使用pip安装：pip install mrjob</li></ul><p>3.<strong>mrjob实现WordCount</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> mrjob.job <span class="keyword">import</span> MRJob</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MRWordCount</span><span class="params">(MRJob)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#每一行从line中输入</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mapper</span><span class="params">(self, _, line)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> word <span class="keyword">in</span> line.split():</span><br><span class="line">            <span class="keyword">yield</span> word,<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># word相同的 会走到同一个reduce</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reducer</span><span class="params">(self, word, counts)</span>:</span>python</span><br><span class="line">        <span class="keyword">yield</span> word, sum(counts)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    MRWordCount.run()</span><br></pre></td></tr></table></figure><p>4.<strong>运行WordCount代码</strong> </p><p>打开命令行, 找到一篇文本文档, 敲如下命令:</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python mr_word_count.py my_file.txt</span><br></pre></td></tr></table></figure><h5 id="运行MRJOB的不同方式"><a href="#运行MRJOB的不同方式" class="headerlink" title="运行MRJOB的不同方式"></a>运行MRJOB的不同方式</h5><p>1.<strong>内嵌(-r inline)方式</strong></p><ul><li>特点是调试方便</li><li>启动单一进程模拟任务执行状态和结果，默认(-r inline)可以省略</li><li>输出文件使用 &gt; output-file 或-o output-file</li><li><p>比如下面两种运行方式是等价的</p><ul><li>python word_count.py -r inline input.txt &gt; output.txt </li><li>python word_count.py input.txt &gt; output.txt</li></ul><p>2.<strong>本地(-r local)方式</strong> </p></li><li><p>用于本地模拟Hadoop调试</p></li><li>与内嵌(inline)方式的区别是启动了多进程执行每一个任务</li><li><p>如：python word_count.py -r local input.txt &gt; output1.txt</p><p>3.<strong>Hadoop(-r hadoop)方式</strong> </p></li><li><p>用于hadoop环境，支持Hadoop运行调度控制参数</p></li><li><p>如：指定Hadoop任务调度优先级(VERY_HIGH|HIGH)</p><ul><li>—jobconf mapreduce.job.priority=VERY_HIGH。</li></ul></li><li><p>如：Map及Reduce任务个数限制</p><ul><li>—jobconf mapreduce.map.tasks=2 —jobconf mapreduce.reduce.tasks=5</li></ul></li><li><p>python word_count.py -r hadoop hdfs:///test.txt -o hdfs:///output</p></li></ul><h5 id="mrjob-实现-topN统计"><a href="#mrjob-实现-topN统计" class="headerlink" title="mrjob 实现 topN统计"></a>mrjob 实现 topN统计</h5><p> 统计数据中出现次数最多的前n个数据 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> mrjob.job <span class="keyword">import</span> MRJob,MRStep</span><br><span class="line"><span class="keyword">import</span> heapq</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TopNWords</span><span class="params">(MRJob)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">mapper</span><span class="params">(self, _, line)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> line.strip() != <span class="string">""</span>:</span><br><span class="line">            <span class="keyword">for</span> word <span class="keyword">in</span> line.strip().split():</span><br><span class="line">                <span class="keyword">yield</span> word,<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#介于mapper和reducer之间，用于临时的将mapper输出的数据进行统计</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">combiner</span><span class="params">(self, word, counts)</span>:</span></span><br><span class="line">        <span class="keyword">yield</span> word,sum(counts)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">reducer_sum</span><span class="params">(self, word, counts)</span>:</span></span><br><span class="line">        <span class="keyword">yield</span> <span class="literal">None</span>,(sum(counts),word)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#利用heapq将数据进行排序，将最大的2个取出</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">top_n_reducer</span><span class="params">(self,_,word_cnts)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> cnt,word <span class="keyword">in</span> heapq.nlargest(<span class="number">2</span>,word_cnts):</span><br><span class="line">            <span class="keyword">yield</span> word,cnt</span><br><span class="line"></span><br><span class="line">    <span class="comment">#实现steps方法用于指定自定义的mapper，comnbiner和reducer方法</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">steps</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="comment">#传入两个step 定义了执行的顺序</span></span><br><span class="line">        <span class="keyword">return</span> [</span><br><span class="line">            MRStep(mapper=self.mapper,</span><br><span class="line">                   combiner=self.combiner,</span><br><span class="line">                   reducer=self.reducer_sum),</span><br><span class="line">            MRStep(reducer=self.top_n_reducer)</span><br><span class="line">        ]</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    TopNWords.run()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><h4 id="MapReduce原理"><a href="#MapReduce原理" class="headerlink" title="MapReduce原理"></a>MapReduce原理</h4><h5 id="单机程序计算流程"><a href="#单机程序计算流程" class="headerlink" title="单机程序计算流程"></a>单机程序计算流程</h5><p>输入数据—-&gt;读取数据—-&gt;处理数据—-&gt;写入数据—-&gt;输出数据</p><h5 id="Hadoop计算流程"><a href="#Hadoop计算流程" class="headerlink" title="Hadoop计算流程"></a>Hadoop计算流程</h5><p>input data：输入数据</p><p>InputFormat：对数据进行切分，格式化处理</p><p>map：将前面切分的数据做map处理(将数据进行分类，输出(k,v)键值对数据)</p><p>shuffle&amp;sort:将相同的数据放在一起，并对数据进行排序处理</p><p>reduce：将map输出的数据进行hash计算，对每个map数据进行统计计算</p><p>OutputFormat：格式化输出数据</p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227171452.png"/></p><p>  <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227171230.png"/></p><p> <img src="YARN%E5%92%8CMAPREDUCE/mp5.png" alt="img"></p><p>  <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227165931.png"/></p><p>  <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227171503.png"/></p><p> map：将数据进行处理</p><p>buffer in memory：达到80%数据时，将数据锁在内存上，将这部分输出到磁盘上</p><p>partitions：在磁盘上有很多”小的数据”，将这些数据进行归并排序。</p><p>merge on disk：将所有的”小的数据”进行合并。</p><p>reduce：不同的reduce任务，会从map中对应的任务中copy数据</p><p> 在reduce中同样要进行merge操作</p><h4 id="MapReduce架构"><a href="#MapReduce架构" class="headerlink" title="MapReduce架构"></a>MapReduce架构</h4><h5 id="MapReduce架构-1-X"><a href="#MapReduce架构-1-X" class="headerlink" title="MapReduce架构 1.X"></a>MapReduce架构 1.X</h5><ul><li>JobTracker:负责接收客户作业提交，负责任务到作业节点上运行，检查作业的状态</li><li><p>TaskTracker：由JobTracker指派任务，定期向JobTracker汇报状态，在每一个工作节点上永远只会有一个TaskTracker</p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227171707.png"/></p></li></ul><h5 id="MapReduce2-X架构"><a href="#MapReduce2-X架构" class="headerlink" title="MapReduce2.X架构"></a>MapReduce2.X架构</h5><ul><li>ResourceManager：负责资源的管理，负责提交任务到NodeManager所在的节点运行，检查节点的状态</li><li>NodeManager：由ResourceManager指派任务，定期向ResourceManager汇报状态</li></ul><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227171651.png"/></p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;资源调度框架-YARN&quot;&gt;&lt;a href=&quot;#资源调度框架-YARN&quot; class=&quot;headerlink&quot; title=&quot;资源调度框架 YARN&quot;&gt;&lt;/a&gt;资源调度框架 YARN&lt;/h4&gt;&lt;h5 id=&quot;什么是YARN&quot;&gt;&lt;a href=&quot;#什么是YARN&quot; class=&quot;headerlink&quot; title=&quot;什么是YARN&quot;&gt;&lt;/a&gt;什么是YARN&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;Yet Another Resource Negotiator, 另一种资源协调者&lt;/li&gt;
&lt;li&gt;通用资源管理系统&lt;/li&gt;
&lt;li&gt;为上层应用提供统一的资源管理和调度，为集群在利用率、资源统一管理和数据共享等方面带来了巨大好处&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="大数据推荐系统" scheme="https://xiaoliaozi.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="资源调度框架YARN" scheme="https://xiaoliaozi.com/tags/%E8%B5%84%E6%BA%90%E8%B0%83%E5%BA%A6%E6%A1%86%E6%9E%B6YARN/"/>
    
      <category term="分布式处理框架MapReduce" scheme="https://xiaoliaozi.com/tags/%E5%88%86%E5%B8%83%E5%BC%8F%E5%A4%84%E7%90%86%E6%A1%86%E6%9E%B6MapReduce/"/>
    
      <category term="MapReduce实战" scheme="https://xiaoliaozi.com/tags/MapReduce%E5%AE%9E%E6%88%98/"/>
    
      <category term="MapReduce原理" scheme="https://xiaoliaozi.com/tags/MapReduce%E5%8E%9F%E7%90%86/"/>
    
      <category term="MapReduce架构" scheme="https://xiaoliaozi.com/tags/MapReduce%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>分布式文件系统HDFS</title>
    <link href="https://xiaoliaozi.com/2020/02/11/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9FHDFS/"/>
    <id>https://xiaoliaozi.com/2020/02/11/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9FHDFS/</id>
    <published>2020-02-11T07:30:04.000Z</published>
    <updated>2020-02-27T08:00:50.234Z</updated>
    
    <content type="html"><![CDATA[<h4 id="HDFS概念"><a href="#HDFS概念" class="headerlink" title="HDFS概念"></a>HDFS概念</h4><p>Hadoop 附带了一个名为 HDFS(Hadoop分布式文件系统)的分布式文件系统，基于 Hadoop 的应用程序使用 HDFS 。HDFS 是专为存储超大数据文件，运行在集群的商品硬件上。它是容错的，可伸缩的，并且非常易于扩展。</p><a id="more"></a><h4 id="HDFS的使用"><a href="#HDFS的使用" class="headerlink" title="HDFS的使用"></a>HDFS的使用</h4><p><strong>启动HDFS</strong> </p><ul><li><p>来到$HADOOP_HOME/sbin目录下</p></li><li><p>执行start-dfs.sh</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop00 sbin]$ ./start-dfs.sh</span><br></pre></td></tr></table></figure></li><li><p>可以看到 namenode和 datanode启动的日志信息 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Starting namenodes on [hadoop00]</span><br><span class="line">hadoop00: starting namenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-namenode-hadoop00.out</span><br><span class="line">localhost: starting datanode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-datanode-hadoop00.out</span><br><span class="line">Starting secondary namenodes [0.0.0.0]</span><br><span class="line">0.0.0.0: starting secondarynamenode, logging to /home/hadoop/app/hadoop-2.6.0-cdh5.7.0/logs/hadoop-hadoop-secondarynamenode-hadoop00.out</span><br></pre></td></tr></table></figure></li><li><p>通过jps命令查看当前运行的进程 </p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">[hadoop@hadoop00 sbin]$ jps</span><br><span class="line">4416 DataNode</span><br><span class="line">4770 Jps</span><br><span class="line">4631 SecondaryNameNode</span><br><span class="line">4251 NameNode</span><br></pre></td></tr></table></figure></li><li><p>可以看到 NameNode DataNode 以及 SecondaryNameNode 说明启动成功 </p></li></ul><p><strong>通过可视化界面查看HDFS的运行情况</strong> </p><p>  <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227155955.png"/> </p><h4 id="HDFS-shell操作"><a href="#HDFS-shell操作" class="headerlink" title="HDFS shell操作"></a>HDFS shell操作</h4><h5 id="HDFS-shell常见操作"><a href="#HDFS-shell常见操作" class="headerlink" title="HDFS shell常见操作"></a>HDFS shell常见操作</h5><p><strong>ls</strong></p><ul><li><p>hadoop fs -ls </p></li><li><p>如果是文件，则按照如下格式返回文件信息： 文件名 &lt;副本数&gt; 文件大小 修改日期 修改时间 权限 用户ID 组ID </p></li><li>如果是目录，则返回它直接子文件的一个列表，就像在Unix中一样。目录返回列表的信息如下： 目录名  修改日期  修改时间  权限  用户ID 组ID </li><li>示例<ul><li>hadoop fs -ls /user/hadoop/file1 /user/hadoop/file2</li><li>hadoop fs -ls hdfs://host:port/user/hadoop/dir1 /nonexistentfile </li><li>返回值： 成功返回0，失败返回-1</li></ul></li></ul><h5 id="text"><a href="#text" class="headerlink" title="text"></a><strong>text</strong></h5><ul><li>使用方法：hadoop fs -text</li><li>将源文件输出为文本格式。允许的格式是zip和TextRecordInputStream</li></ul><p><strong>mv</strong></p><ul><li>使用方法：hadoop fs -mv URI [URI …]</li><li>将文件从源路径移动到目标路径。这个命令允许有多个源路径，此时目标路径必须是一个目录。不允许在不同的文件系统间移动文件。 示例：<ul><li>hadoop fs -mv /user/hadoop/file1 /user/hadoop/file2</li><li>hadoop fs -mv hdfs://host:port/file1 hdfs://host:port/file2 hdfs://host:port/file3 hdfs://host:port/dir1</li><li>返回值：成功返回0，失败返回-1</li></ul></li></ul><p><strong>put</strong></p><ul><li>使用方法：hadoop fs -put …</li><li>从本地文件系统中复制单个或多个源路径到目标文件系统。也支持从标准输入中读取输入写入目标文件系统。<ul><li>hadoop fs -put localfile /user/hadoop/hadoopfile</li><li>hadoop fs -put localfile1 localfile2 /user/hadoop/hadoopdir</li><li>hadoop fs -put localfile hdfs://host:port/hadoop/hadoopfile</li><li>hadoop fs -put - hdfs://host:port/hadoop/hadoopfile 从标准输入中读取输入。</li><li>返回值：成功返回0，失败返回-1</li></ul></li></ul><p>参考网址： <a href="http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html" target="_blank" rel="noopener">http://hadoop.apache.org/docs/r1.0.4/cn/hdfs_shell.html</a> </p><h5 id="HDFS-shell操作练习"><a href="#HDFS-shell操作练习" class="headerlink" title="HDFS shell操作练习"></a>HDFS shell操作练习</h5><ul><li>在centos 中创建 test.txt</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">touch test.txt</span><br></pre></td></tr></table></figure><ul><li>在centos中为test.txt 添加文本内容 </li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vi test.txt</span><br></pre></td></tr></table></figure><ul><li>在HDFS中创建 hadoop001/test 文件夹 </li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -mkdir -p /hadoop001/test</span><br></pre></td></tr></table></figure><ul><li>把text.txt文件上传到HDFS中 </li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -put test.txt /hadoop001/test/</span><br></pre></td></tr></table></figure><ul><li>查看hdfs中 hadoop001/test/test.txt 文件内容 </li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -cat /hadoop001/test/test.txt</span><br></pre></td></tr></table></figure><ul><li>将hdfs中 hadoop001/test/test.txt文件下载到centos </li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -get /hadoop001/test/test.txt test.txt</span><br></pre></td></tr></table></figure><ul><li>删除HDFS中 hadoop001/test/ </li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hadoop fs -rm -r /hadoop001</span><br></pre></td></tr></table></figure><h4 id="HDFS设计思路"><a href="#HDFS设计思路" class="headerlink" title="HDFS设计思路"></a>HDFS设计思路</h4><ul><li><p>分布式文件系统的设计思路：</p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227155952.png"/></p></li><li><p>HDFS的设计目标 </p><ul><li>适合运行在通用硬件(commodity hardware)上的分布式文件系统</li><li>高度容错性的系统，适合部署在廉价的机器上</li><li>HDFS能提供高吞吐量的数据访问，非常适合大规模数据集上的应用</li><li>容易扩展，为用户提供性能不错的文件存储服务</li></ul></li></ul><h4 id="HDFS架构"><a href="#HDFS架构" class="headerlink" title="HDFS架构"></a>HDFS架构</h4><ul><li>1个NameNode/NN(Master) 带 DataNode/DN(Slaves) (Master-Slave结构)</li><li>1个文件会被拆分成多个Block</li><li>NameNode(NN)<ul><li>负责客户端请求的响应</li><li>负责元数据（文件的名称、副本系数、Block存放的DN）的管理<ul><li>元数据 MetaData 描述数据的数据</li></ul></li><li>监控DataNode健康状况 10分钟没有收到DataNode报告认为Datanode死掉了</li></ul></li><li>DataNode(DN)<ul><li>存储用户的文件对应的数据块(Block)</li><li>要定期向NN发送心跳信息，汇报本身及其所有的block信息，健康状况</li></ul></li><li><p>分布式集群NameNode和DataNode部署在不同机器上</p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227160002.jpg"/> </p></li></ul><h4 id="HDFS优缺点"><a href="#HDFS优缺点" class="headerlink" title="HDFS优缺点"></a>HDFS优缺点</h4><ul><li>优点<ul><li>数据冗余 硬件容错</li><li>适合存储大文件</li><li>处理流式数据</li><li>可构建在廉价机器上</li></ul></li><li>缺点<ul><li>低延迟的数据访问</li><li>小文件存储</li></ul></li></ul><h4 id="HDFS环境搭建"><a href="#HDFS环境搭建" class="headerlink" title="HDFS环境搭建"></a>HDFS环境搭建</h4><ul><li><p>下载jdk 和 hadoop 放到 ~/software目录下 然后解压到 ~/app目录下</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -zxvf 压缩包名字 -C ~/app/</span><br></pre></td></tr></table></figure></li><li><p>配置环境变量</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">vi ~/.bash_profile</span><br><span class="line">export JAVA_HOME=/root/bigdata/jdk</span><br><span class="line">export PATH=$JAVA_HOME/bin:$PATH</span><br><span class="line">export HADOOP_HOME=/root/bigdata/hadoop</span><br><span class="line">export PATH=$HADOOP_HOME/bin:$PATH</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">保存退出后</span></span><br><span class="line">source ~/.bash_profile</span><br></pre></td></tr></table></figure></li><li><p>进入到解压后的hadoop目录 修改配置文件</p><ul><li>配置文件作用<ul><li>core-site.xml 指定hdfs的访问方式</li><li>hdfs-site.xml 指定namenode 和 datanode 的数据存储位置</li><li>mapred-site.xml 配置mapreduce</li><li>yarn-site.xml 配置yarn</li></ul></li><li>修改hadoop-env.sh</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">cd etc/hadoop</span><br><span class="line">vi hadoop-env.sh</span><br><span class="line"><span class="meta">#</span><span class="bash">找到下面内容添加java home</span></span><br><span class="line">export_JAVA_HOME=/root/bigdata/jdk</span><br></pre></td></tr></table></figure><ul><li>修改 core-site.xml 在 节点中添加</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">configuration</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>file:/root/bigdata/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">name</span>&gt;</span>fs.defaultFS<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">value</span>&gt;</span>hdfs://hadoop-master:9000<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>修改hdfs-site.xml 在 configuration节点中添加</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.namenode.name.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/root/bigdata/hadoop/hdfs/name<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.datanode.data.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>/root/bigdata/hadoop/hdfs/data<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>dfs.replication<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>1<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>修改 mapred-site.xml</li><li>默认没有这个 从模板文件复制</li></ul><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cp mapred-site.xml.template mapred-site.xml</span><br></pre></td></tr></table></figure><p> 在mapred-site.xml 的configuration 节点中添加</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>mapreduce.framework.name<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>yarn<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><ul><li>修改yarn-site.xml configuration 节点中添加</li></ul><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">name</span>&gt;</span>yarn.nodemanager.aux-services<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">value</span>&gt;</span>mapreduce_shuffle<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure></li><li><p>来到hadoop的bin目录格式化namenode</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./hadoop namenode -format (这个命令只运行一次)</span><br></pre></td></tr></table></figure></li><li><p>启动hdfs 进入到 sbin</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./start-dfs.sh</span><br></pre></td></tr></table></figure></li><li><p>启动启动yarn 在sbin中</p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;HDFS概念&quot;&gt;&lt;a href=&quot;#HDFS概念&quot; class=&quot;headerlink&quot; title=&quot;HDFS概念&quot;&gt;&lt;/a&gt;HDFS概念&lt;/h4&gt;&lt;p&gt;Hadoop 附带了一个名为 HDFS(Hadoop分布式文件系统)的分布式文件系统，基于 Hadoop 的应用程序使用 HDFS 。HDFS 是专为存储超大数据文件，运行在集群的商品硬件上。它是容错的，可伸缩的，并且非常易于扩展。&lt;/p&gt;
    
    </summary>
    
    
      <category term="大数据推荐系统" scheme="https://xiaoliaozi.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="HDFS" scheme="https://xiaoliaozi.com/tags/HDFS/"/>
    
      <category term="HDFS概念" scheme="https://xiaoliaozi.com/tags/HDFS%E6%A6%82%E5%BF%B5/"/>
    
      <category term="HDFS shell操作" scheme="https://xiaoliaozi.com/tags/HDFS-shell%E6%93%8D%E4%BD%9C/"/>
    
      <category term="HDFS架构" scheme="https://xiaoliaozi.com/tags/HDFS%E6%9E%B6%E6%9E%84/"/>
    
      <category term="HDFS环境搭建" scheme="https://xiaoliaozi.com/tags/HDFS%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
    
      <category term="HDFS优缺点" scheme="https://xiaoliaozi.com/tags/HDFS%E4%BC%98%E7%BC%BA%E7%82%B9/"/>
    
  </entry>
  
  <entry>
    <title>HADOOP概述</title>
    <link href="https://xiaoliaozi.com/2020/02/11/HADOOP%E6%A6%82%E8%BF%B0/"/>
    <id>https://xiaoliaozi.com/2020/02/11/HADOOP%E6%A6%82%E8%BF%B0/</id>
    <published>2020-02-11T07:29:44.000Z</published>
    <updated>2020-02-27T07:59:38.132Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Hadoop的概念"><a href="#Hadoop的概念" class="headerlink" title="Hadoop的概念"></a>Hadoop的概念</h4><ul><li>Apache™ Hadoop® 是一个开源的，<strong>可靠的</strong>(reliable)，<strong>可扩展</strong>的(scalable)<strong>分布式计算框架</strong> <ul><li>允许使用简单的编程模型跨计算机集群分布式处理大型数据集 </li><li><strong>可扩展</strong>: 从单个服务器扩展到数千台计算机，每台计算机都提供本地计算和存储</li><li><strong>可靠的</strong>: 不依靠硬件来提供高可用性(high-availability)，而是在应用层检测和处理故障，从而在计算机集群之上提供高可用服务</li></ul></li></ul><a id="more"></a><h4 id="Hadoop能做什么"><a href="#Hadoop能做什么" class="headerlink" title="Hadoop能做什么?"></a>Hadoop能做什么?</h4><ul><li><p>搭建大型数据仓库</p></li><li><p>PB级数据的存储 处理 分析 统计等业务</p><ul><li><p>搜索引擎</p></li><li><p>日志分析</p></li><li><p>数据挖掘</p></li><li><p>商业智能(Business Intelligence，简称：BI)</p><blockquote><p>商业智能通常被理解为将企业中现有的数据(订单、库存、交易账目、客户和供应商等数据)转化为知识，帮助企业做出明智的业务经营决策的工具。从技术层面上讲，是数据仓库、数据挖掘等技术的综合运用。</p></blockquote></li></ul></li></ul><h4 id="Hadoop核心组件"><a href="#Hadoop核心组件" class="headerlink" title="Hadoop核心组件"></a>Hadoop核心组件</h4><ul><li><p>Hadoop是所有搜索引擎的共性问题的廉价解决方案 </p><ul><li>如何存储持续增长的海量网页: 单节点 V.S. 分布式存储</li><li>如何对持续增长的海量网页进行排序: 超算 V.S. 分布式计算</li><li>HDFS 解决分布式存储问题</li><li>MapReduce 解决分布式计算问题</li></ul></li><li><p>Hadoop Common：协调其他组件的通用工具</p></li><li><p>HDFS：一个基于网络的分布式文件存储系统</p><ul><li>Hadoop Distributed File System (HDFS™) </li><li>HDFS的特点:扩展性&amp;容错性&amp;海量数量存储</li><li>将文件切分成指定大小的数据块, 并在多台机器上保存多个副本</li><li>数据切分、多副本、容错等操作对用户是透明的</li></ul><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">下面这张图是数据块多份复制存储的示意</span><br><span class="line">图中对于文件 /users/sameerp/data/part<span class="number">-0</span>，其复制备份数设置为<span class="number">2</span>, 存储的BlockID分别为<span class="number">1</span>、<span class="number">3</span>。</span><br><span class="line">Block1的两个备份存储在DataNode0和DataNode2两个服务器上</span><br><span class="line">Block3的两个备份存储在DataNode4和DataNode6两个服务器上</span><br></pre></td></tr></table></figure><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227155845.png"/> </p></li><li><p>Hadoop MapReduce ： 基于yarn的大数据集并行处理系统 </p><ul><li>分布式计算框架</li><li>MapReduce是GoogleMapReduce的开源实现</li><li>MapReduce特点:扩展性&amp;容错性&amp;海量数据离线处理</li></ul><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227155815.png"/> </p></li><li><p>YARN： 作业调度和集群资源管理的框架 </p><ul><li>YARN: Yet Another Resource Negotiator</li><li>负责整个集群资源的管理和调度</li><li>YARN特点:扩展性&amp;容错性&amp;多框架资源统一调度</li></ul></li></ul><h4 id="Hadoop优势"><a href="#Hadoop优势" class="headerlink" title="Hadoop优势"></a>Hadoop优势</h4><ul><li>高可靠<ul><li>数据存储: 数据块多副本</li><li>数据计算: 某个节点崩溃, 会自动重新调度作业计算</li></ul></li><li>高扩展性<ul><li>存储/计算资源不够时，可以横向的线性扩展机器</li><li>一个集群中可以包含数以千计的节点</li><li>集群可以使用廉价机器，成本低</li></ul></li><li>Hadoop生态系统成熟</li></ul><h4 id="拓展-数据仓库"><a href="#拓展-数据仓库" class="headerlink" title="拓展-数据仓库"></a>拓展-数据仓库</h4><h5 id="数据库两大基本类型"><a href="#数据库两大基本类型" class="headerlink" title="数据库两大基本类型"></a>数据库两大基本类型</h5><blockquote><p> <strong>操作型数据库</strong> ： 主要用于业务支撑。一个公司往往会使用并维护若干个数据库，这些数据库保存着公司的日常操作数据，比如商品购买、酒店预订、学生成绩录入等； </p><p> <strong>分析型数据库</strong> ： 主要用于历史数据分析。这类数据库作为公司的单独数据存储，负责利用历史数据对公司各主题域进行统计分析； </p></blockquote><h5 id="数据仓库定义"><a href="#数据仓库定义" class="headerlink" title="数据仓库定义"></a>数据仓库定义</h5><p>数据仓库是决策支持系统和联机分析应用数据源的结构化数据环境。数据仓库研究和解决从数据库中获取信息的问题。数据仓库的特征在于面向主题、集成性、稳定性和时变性 </p><h5 id="数据仓库特点"><a href="#数据仓库特点" class="headerlink" title="数据仓库特点"></a>数据仓库特点</h5><ol><li><strong>面向主题</strong></li></ol><p>​    面向主题特性是数据仓库和操作型数据库的根本区别。操作型数据库是为了支撑各种业务而建立，而分析型数据库则是为了对从各种繁杂业务中抽象出来的分析主题(如用户、成本、商品等)进行分析而建立；</p><ol><li><strong>集成性</strong></li></ol><p>​    集成性是指数据仓库会将不同源数据库中的数据汇总到一起；</p><ol><li><strong>企业范围</strong></li></ol><p>​    数据仓库内的数据是面向公司全局的。比如某个主题域为成本，则全公司和成本有关的信息都会被汇集进来；</p><ol><li><strong>历史性</strong></li></ol><p>​    较之操作型数据库，数据仓库的时间跨度通常比较长。前者通常保存几个月，后者可能几年甚至几十年；</p><ol><li><strong>时变性</strong></li></ol><p>​    时变性是指数据仓库包含来自其时间范围不同时间段的数据快照。有了这些数据快照以后，用户便可将其汇总，生成各历史阶段的数据分析报告；</p><h5 id="数据仓库组件"><a href="#数据仓库组件" class="headerlink" title="数据仓库组件"></a>数据仓库组件</h5><p> 数据仓库的核心组件有四个：各源数据库，ETL，数据仓库，前端应用。如下图所示： </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227155904.jpg"/> </p><ol><li><p><strong>业务系统</strong></p><p> 业务系统包含各种源数据库，这些源数据库既为业务系统提供数据支撑，同时也作为数据仓库的数据源(注：除了业务系统，数据仓库也可从其他外部数据源获取数据)； </p></li><li><p><strong>ETL</strong></p><p> ETL分别代表：提取extraction、转换transformation、加载load。其中提取过程表示操作型数据库搜集指定数据，转换过程表示将数据转化为指定格式并进行数据清洗保证数据质量，加载过程表示将转换过后满足指定格式的数据加载进数据仓库。数据仓库会周期不断地从源数据库提取清洗好了的数据，因此也被称为”目标系统”； </p></li><li><p><strong>前端应用</strong></p><p>  和操作型数据库一样，数据仓库通常提供具有直接访问数据仓库功能的前端应用，这些应用也被称为BI(商务智能)应用； </p></li></ol><h5 id="数据仓库开发流程"><a href="#数据仓库开发流程" class="headerlink" title="数据仓库开发流程"></a>数据仓库开发流程</h5><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227155858.jpg"/> </p><p> 最后：在大数据时代，数据仓库的重要性更胜以往。Hadoop平台下的Hive，Spark平台下的Spark SQL都是各自生态圈内应用最热门的配套工具，而它们的本质就是开源分布式数据仓库。</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;Hadoop的概念&quot;&gt;&lt;a href=&quot;#Hadoop的概念&quot; class=&quot;headerlink&quot; title=&quot;Hadoop的概念&quot;&gt;&lt;/a&gt;Hadoop的概念&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;Apache™ Hadoop® 是一个开源的，&lt;strong&gt;可靠的&lt;/strong&gt;(reliable)，&lt;strong&gt;可扩展&lt;/strong&gt;的(scalable)&lt;strong&gt;分布式计算框架&lt;/strong&gt; &lt;ul&gt;
&lt;li&gt;允许使用简单的编程模型跨计算机集群分布式处理大型数据集 &lt;/li&gt;
&lt;li&gt;&lt;strong&gt;可扩展&lt;/strong&gt;: 从单个服务器扩展到数千台计算机，每台计算机都提供本地计算和存储&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;可靠的&lt;/strong&gt;: 不依靠硬件来提供高可用性(high-availability)，而是在应用层检测和处理故障，从而在计算机集群之上提供高可用服务&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="大数据推荐系统" scheme="https://xiaoliaozi.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="Hadoop的概念" scheme="https://xiaoliaozi.com/tags/Hadoop%E7%9A%84%E6%A6%82%E5%BF%B5/"/>
    
      <category term="Hadoop核心组件" scheme="https://xiaoliaozi.com/tags/Hadoop%E6%A0%B8%E5%BF%83%E7%BB%84%E4%BB%B6/"/>
    
      <category term="Hadoop优势" scheme="https://xiaoliaozi.com/tags/Hadoop%E4%BC%98%E5%8A%BF/"/>
    
      <category term="数据仓库" scheme="https://xiaoliaozi.com/tags/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93/"/>
    
  </entry>
  
  <entry>
    <title>基于模型的协同过滤推荐</title>
    <link href="https://xiaoliaozi.com/2020/02/11/%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E6%8E%A8%E8%8D%90/"/>
    <id>https://xiaoliaozi.com/2020/02/11/%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E6%8E%A8%E8%8D%90/</id>
    <published>2020-02-11T06:43:28.000Z</published>
    <updated>2020-02-27T07:57:55.714Z</updated>
    
    <content type="html"><![CDATA[<h4 id="Model-Based-协同过滤算法"><a href="#Model-Based-协同过滤算法" class="headerlink" title="Model-Based 协同过滤算法"></a>Model-Based 协同过滤算法</h4><p>随着机器学习技术的逐渐发展与完善，推荐系统也逐渐运用机器学习的思想来进行推荐。将机器学习应用到推荐系统中的方案真是不胜枚举。以下对Model-Based CF算法做一个大致的分类：</p><ul><li>基于分类算法、回归算法、聚类算法</li><li>基于矩阵分解的推荐</li><li>基于神经网络算法</li><li>基于图模型算法</li></ul><p>几种应用较多的方案：</p><ul><li><strong>基于回归模型的协同过滤推荐</strong></li><li><strong>基于矩阵分解的协同过滤推荐</strong></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h4 id=&quot;Model-Based-协同过滤算法&quot;&gt;&lt;a href=&quot;#Model-Based-协同过滤算法&quot; class=&quot;headerlink&quot; title=&quot;Model-Based 协同过滤算法&quot;&gt;&lt;/a&gt;Model-Based 协同过滤算法&lt;/h4&gt;&lt;p&gt;随着机器学习
      
    
    </summary>
    
    
      <category term="大数据推荐系统" scheme="https://xiaoliaozi.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="基于模型的推荐算法" scheme="https://xiaoliaozi.com/tags/%E5%9F%BA%E4%BA%8E%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统的冷启动问题</title>
    <link href="https://xiaoliaozi.com/2020/02/10/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%86%B7%E5%90%AF%E5%8A%A8%E9%97%AE%E9%A2%98/"/>
    <id>https://xiaoliaozi.com/2020/02/10/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%9A%84%E5%86%B7%E5%90%AF%E5%8A%A8%E9%97%AE%E9%A2%98/</id>
    <published>2020-02-10T02:30:55.000Z</published>
    <updated>2020-02-27T07:57:29.280Z</updated>
    
    <content type="html"><![CDATA[<h4 id="推荐系统冷启动概念"><a href="#推荐系统冷启动概念" class="headerlink" title="推荐系统冷启动概念"></a>推荐系统冷启动概念</h4><ul><li>⽤户冷启动：如何为新⽤户做个性化推荐</li><li>物品冷启动：如何将新物品推荐给⽤户（协同过滤）</li><li>系统冷启动：⽤户冷启动+物品冷启动</li><li>本质是推荐系统依赖历史数据，没有历史数据⽆法预测⽤户偏好</li></ul><a id="more"></a><h4 id="处理推荐系统冷启动问题的常用方法"><a href="#处理推荐系统冷启动问题的常用方法" class="headerlink" title="处理推荐系统冷启动问题的常用方法"></a>处理推荐系统冷启动问题的常用方法</h4><h5 id="用户冷启动"><a href="#用户冷启动" class="headerlink" title="用户冷启动"></a>用户冷启动</h5><ul><li><p>收集⽤户特征</p><ul><li>⽤户注册信息：性别、年龄、地域</li><li>设备信息：定位、⼿机型号、app列表</li><li>社交信息、推⼴素材、安装来源</li></ul><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227155703.png"/></p></li><li><p>引导用户填写兴趣 </p></li><li>使用其它站点的行为数据, 例如腾讯视频&amp;QQ音乐 今日头条&amp;抖音</li><li>新老用户推荐策略的差异<ul><li>新⽤户在冷启动阶段更倾向于热门排⾏榜，⽼⽤户会更加需要长尾推荐</li><li>Explore Exploit⼒度</li><li>使⽤单独的特征和模型预估</li></ul></li></ul><h5 id="物品冷启动"><a href="#物品冷启动" class="headerlink" title="物品冷启动"></a>物品冷启动</h5><ul><li>给物品打标签</li><li><p>利用物品的内容信息，将新物品先投放给曾经喜欢过和它内容相似的其他物品的用户。</p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227155708.png"/> </p></li></ul><h5 id="系统冷启动"><a href="#系统冷启动" class="headerlink" title="系统冷启动"></a>系统冷启动</h5><ul><li>基于内容的推荐 系统早期</li><li>基于内容的推荐逐渐过渡到协同过滤</li><li>基于内容的推荐和协同过滤的推荐结果都计算出来 加权求和得到最终推荐结果</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;推荐系统冷启动概念&quot;&gt;&lt;a href=&quot;#推荐系统冷启动概念&quot; class=&quot;headerlink&quot; title=&quot;推荐系统冷启动概念&quot;&gt;&lt;/a&gt;推荐系统冷启动概念&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;⽤户冷启动：如何为新⽤户做个性化推荐&lt;/li&gt;
&lt;li&gt;物品冷启动：如何将新物品推荐给⽤户（协同过滤）&lt;/li&gt;
&lt;li&gt;系统冷启动：⽤户冷启动+物品冷启动&lt;/li&gt;
&lt;li&gt;本质是推荐系统依赖历史数据，没有历史数据⽆法预测⽤户偏好&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="大数据推荐系统" scheme="https://xiaoliaozi.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="推荐系统冷启动" scheme="https://xiaoliaozi.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E5%86%B7%E5%90%AF%E5%8A%A8/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统评估</title>
    <link href="https://xiaoliaozi.com/2020/02/10/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AF%84%E4%BC%B0/"/>
    <id>https://xiaoliaozi.com/2020/02/10/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AF%84%E4%BC%B0/</id>
    <published>2020-02-10T02:30:27.000Z</published>
    <updated>2020-02-27T07:56:54.509Z</updated>
    
    <content type="html"><![CDATA[<h3 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>了解推荐系统的常用评估指标</li><li>了解推荐系统的评估方法</li></ul><a id="more"></a><h4 id="推荐系统的评估指标"><a href="#推荐系统的评估指标" class="headerlink" title="推荐系统的评估指标"></a>推荐系统的评估指标</h4><ul><li><p>好的推荐系统可以实现用户, 服务提供方, 内容提供方的共赢 </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227155643.png"/> </p></li><li><p>评估数据来源显示反馈和隐式反馈 </p></li></ul><div class="table-container"><table><thead><tr><th></th><th>显式反馈</th><th>隐式反馈</th></tr></thead><tbody><tr><td>例子</td><td>电影/书籍评分 是否喜欢这个推荐</td><td>播放/点击 评论 下载 购买</td></tr><tr><td>准确性</td><td>高</td><td>低</td></tr><tr><td>数量</td><td>少</td><td>多</td></tr><tr><td>获取成本</td><td>高</td><td>低</td></tr></tbody></table></div><ul><li><p>常用评估指标 </p><blockquote><p>• 准确性 • 信任度 • 满意度 • 实时性 • 覆盖率 • 鲁棒性<br>• 多样性 • 可扩展性 • 新颖性 • 商业⽬标 • 惊喜度 • ⽤户留存 </p></blockquote><figure class="highlight markdown"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="bullet">- </span>准确性 (理论角度) Netflix 美国录像带租赁</span><br><span class="line"><span class="bullet">  - </span>评分预测</span><br><span class="line"><span class="bullet">    - </span>RMSE MAE</span><br><span class="line"><span class="bullet">  - </span>topN推荐</span><br><span class="line"><span class="bullet">    - </span>召回率 精准率</span><br><span class="line"><span class="bullet">- </span>准确性 (业务角度)</span><br><span class="line"><span class="bullet">- </span>覆盖度</span><br><span class="line"><span class="bullet">  - </span>信息熵 对于推荐越大越好</span><br><span class="line"><span class="bullet">  - </span>覆盖率</span><br><span class="line"><span class="bullet">- </span>多样性&amp;新颖性&amp;惊喜性</span><br><span class="line"><span class="bullet">  - </span>多样性：推荐列表中两两物品的不相似性。（相似性如何度量？</span><br><span class="line"><span class="bullet">  - </span>新颖性：未曾关注的类别、作者；推荐结果的平均流⾏度</span><br><span class="line"><span class="bullet">  - </span>惊喜性：历史不相似（惊）但很满意（喜）</span><br><span class="line"><span class="bullet">  - </span>往往需要牺牲准确性</span><br><span class="line"><span class="bullet">  - </span>使⽤历史⾏为预测⽤户对某个物品的喜爱程度</span><br><span class="line"><span class="bullet">  - </span>系统过度强调实时性</span><br><span class="line"><span class="bullet">- </span>Exploitation &amp; Exploration 探索与利用问题</span><br><span class="line"><span class="bullet">  - </span>Exploitation(开发 利用)：选择现在可能最佳的⽅案</span><br><span class="line"><span class="bullet">  - </span>Exploration(探测 搜索)：选择现在不确定的⼀些⽅案，但未来可能会有⾼收益的⽅案</span><br><span class="line"><span class="bullet">  - </span>在做两类决策的过程中，不断更新对所有决策的不确定性的认知，优化 长期的⽬标</span><br><span class="line"><span class="bullet">- </span>EE问题实践</span><br><span class="line"><span class="bullet">  - </span>兴趣扩展: 相似话题, 搭配推荐</span><br><span class="line"><span class="bullet">  - </span>人群算法: userCF 用户聚类</span><br><span class="line"><span class="bullet">  - </span>平衡个性化推荐和热门推荐比例</span><br><span class="line"><span class="bullet">  - </span>随机丢弃用户行为历史</span><br><span class="line"><span class="bullet">  - </span>随机扰动模型参数</span><br><span class="line"><span class="bullet">- </span>EE可能带来的问题</span><br><span class="line"><span class="bullet">  - </span>探索伤害用户体验, 可能导致用户流失</span><br><span class="line"><span class="bullet">  - </span>探索带来的长期收益(留存率)评估周期长, KPI压力大</span><br><span class="line"><span class="bullet">  - </span>如何平衡实时兴趣和长期兴趣</span><br><span class="line"><span class="bullet">  - </span>如何平衡短期产品体验和长期系统生态</span><br><span class="line"><span class="bullet">  - </span>如何平衡大众口味和小众需求</span><br></pre></td></tr></table></figure></li></ul><h4 id="推荐系统评估方法"><a href="#推荐系统评估方法" class="headerlink" title="推荐系统评估方法"></a>推荐系统评估方法</h4><p>评估方法</p><ul><li>问卷调查: 成本高</li><li>离线评估:<ul><li>只能在用户看到过的候选集上做评估, 且跟线上真实效果存在偏差</li><li>只能评估少数指标</li><li>速度快, 不损害用户体验</li></ul></li><li>在线评估: 灰度发布 &amp; A/B测试 50% 全量上线</li><li>实践: 离线评估和在线评估结合，定期做问卷调查</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;学习目标&quot;&gt;&lt;a href=&quot;#学习目标&quot; class=&quot;headerlink&quot; title=&quot;学习目标&quot;&gt;&lt;/a&gt;学习目标&lt;/h3&gt;&lt;ul&gt;
&lt;li&gt;了解推荐系统的常用评估指标&lt;/li&gt;
&lt;li&gt;了解推荐系统的评估方法&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="大数据推荐系统" scheme="https://xiaoliaozi.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="推荐系统的评估指标" scheme="https://xiaoliaozi.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%9A%84%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87/"/>
    
      <category term="推荐系统评估方法" scheme="https://xiaoliaozi.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E8%AF%84%E4%BC%B0%E6%96%B9%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>案例-基于协同过滤的电影推荐</title>
    <link href="https://xiaoliaozi.com/2020/02/09/%E6%A1%88%E4%BE%8B-%E5%9F%BA%E4%BA%8E%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%9A%84%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90/"/>
    <id>https://xiaoliaozi.com/2020/02/09/%E6%A1%88%E4%BE%8B-%E5%9F%BA%E4%BA%8E%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E7%9A%84%E7%94%B5%E5%BD%B1%E6%8E%A8%E8%8D%90/</id>
    <published>2020-02-09T02:30:03.000Z</published>
    <updated>2020-02-27T07:56:35.185Z</updated>
    
    <content type="html"><![CDATA[<h4 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h4><ul><li>应用基于用户的协同过滤实现电影评分预测</li><li>应用基于物品的协同过滤实现电影评分预测</li></ul><a id="more"></a><h4 id="User-Based-CF-预测电影评分"><a href="#User-Based-CF-预测电影评分" class="headerlink" title="User-Based CF 预测电影评分"></a>User-Based CF 预测电影评分</h4><ul><li><p>据集下载</p><ul><li>下载地址：<a href="https://grouplens.org/datasets/movielens/latest/" target="_blank" rel="noopener">MovieLens Latest Datasets Small</a></li><li>建议下载<a href="http://files.grouplens.org/datasets/movielens/ml-latest-small.zip" target="_blank" rel="noopener">ml-latest-small.zip</a>，数据量小，便于我们单机使用和运行</li></ul></li><li><p>加载ratings.csv，转换为用户-电影评分矩阵并计算用户之间相似度</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">DATA_PATH = <span class="string">"./datasets/ml-latest-small/ratings.csv"</span></span><br><span class="line"></span><br><span class="line">dtype = &#123;<span class="string">"userId"</span>: np.int32, <span class="string">"movieId"</span>: np.int32, <span class="string">"rating"</span>: np.float32&#125;</span><br><span class="line"><span class="comment"># 加载数据，我们只用前三列数据，分别是用户ID，电影ID，已经用户对电影的对应评分</span></span><br><span class="line">ratings = pd.read_csv(data_path, dtype=dtype, usecols=range(<span class="number">3</span>))</span><br><span class="line"><span class="comment"># 透视表，将电影ID转换为列名称，转换成为一个User-Movie的评分矩阵</span></span><br><span class="line">ratings_matrix = ratings.pivot_table(index=[<span class="string">"userId"</span>], columns=[<span class="string">"movieId"</span>],values=<span class="string">"rating"</span>)</span><br><span class="line"><span class="comment">#计算用户之间相似度</span></span><br><span class="line">user_similar = ratings_matrix.T.corr()</span><br></pre></td></tr></table></figure></li><li><p>预测用户对物品的评分 （以用户1对电影1评分为例） </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227155449.png"/></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 找出uid用户的相似用户</span></span><br><span class="line">similar_users = user_similar[<span class="number">1</span>].drop([<span class="number">1</span>]).dropna()</span><br><span class="line"><span class="comment"># 相似用户筛选规则：正相关的用户</span></span><br><span class="line">similar_users = similar_users.where(similar_users&gt;<span class="number">0</span>).dropna()</span><br><span class="line"><span class="comment"># 2. 从用户1的近邻相似用户中筛选出对物品1有评分记录的近邻用户</span></span><br><span class="line">ids = set(ratings_matrix[<span class="number">1</span>].dropna().index)&amp;set(similar_users.index)</span><br><span class="line">finally_similar_users = similar_users.ix[list(<span class="number">1</span>)]</span><br><span class="line"><span class="comment"># 3. 结合uid用户与其近邻用户的相似度预测uid用户对iid物品的评分</span></span><br><span class="line">numerator = <span class="number">0</span>    <span class="comment"># 评分预测公式的分子部分的值</span></span><br><span class="line">denominator = <span class="number">0</span>    <span class="comment"># 评分预测公式的分母部分的值</span></span><br><span class="line"><span class="keyword">for</span> sim_uid, similarity <span class="keyword">in</span> finally_similar_users.iteritems():</span><br><span class="line">    <span class="comment"># 近邻用户的评分数据</span></span><br><span class="line">    sim_user_rated_movies = ratings_matrix.ix[sim_uid].dropna()</span><br><span class="line">    <span class="comment"># 近邻用户对iid物品的评分</span></span><br><span class="line">    sim_user_rating_for_item = sim_user_rated_movies[<span class="number">1</span>]</span><br><span class="line">    <span class="comment"># 计算分子的值</span></span><br><span class="line">    numerator += similarity * sim_user_rating_for_item</span><br><span class="line">    <span class="comment"># 计算分母的值</span></span><br><span class="line">    denominator += similarity</span><br><span class="line"><span class="comment"># 4 计算预测的评分值</span></span><br><span class="line">predict_rating = numerator/denominator</span><br><span class="line">print(<span class="string">"预测出用户&lt;%d&gt;对电影&lt;%d&gt;的评分：%0.2f"</span> % (<span class="number">1</span>, <span class="number">1</span>, predict_rating))</span><br></pre></td></tr></table></figure></li><li><p>封装成方法 预测任意用户对任意电影的评分 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(uid, iid, ratings_matrix, user_similar)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    预测给定用户对给定物品的评分值</span></span><br><span class="line"><span class="string">    :param uid: 用户ID</span></span><br><span class="line"><span class="string">    :param iid: 物品ID</span></span><br><span class="line"><span class="string">    :param ratings_matrix: 用户-物品评分矩阵</span></span><br><span class="line"><span class="string">    :param user_similar: 用户两两相似度矩阵</span></span><br><span class="line"><span class="string">    :return: 预测的评分值</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(<span class="string">"开始预测用户&lt;%d&gt;对电影&lt;%d&gt;的评分..."</span>%(uid, iid))</span><br><span class="line">    <span class="comment"># 1. 找出uid用户的相似用户</span></span><br><span class="line">    similar_users = user_similar[uid].drop([uid]).dropna()</span><br><span class="line">    <span class="comment"># 相似用户筛选规则：正相关的用户</span></span><br><span class="line">    similar_users = similar_users.where(similar_users&gt;<span class="number">0</span>).dropna()</span><br><span class="line">    <span class="keyword">if</span> similar_users.empty <span class="keyword">is</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">"用户&lt;%d&gt;没有相似的用户"</span> % uid)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 从uid用户的近邻相似用户中筛选出对iid物品有评分记录的近邻用户</span></span><br><span class="line">    ids = set(ratings_matrix[iid].dropna().index)&amp;set(similar_users.index)</span><br><span class="line">    finally_similar_users = similar_users.ix[list(ids)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. 结合uid用户与其近邻用户的相似度预测uid用户对iid物品的评分</span></span><br><span class="line">    numerator = <span class="number">0</span>    <span class="comment"># 评分预测公式的分子部分的值</span></span><br><span class="line">    denominator = <span class="number">0</span>    <span class="comment"># 评分预测公式的分母部分的值</span></span><br><span class="line">    <span class="keyword">for</span> sim_uid, similarity <span class="keyword">in</span> finally_similar_users.iteritems():</span><br><span class="line">        <span class="comment"># 近邻用户的评分数据</span></span><br><span class="line">        sim_user_rated_movies = ratings_matrix.ix[sim_uid].dropna()</span><br><span class="line">        <span class="comment"># 近邻用户对iid物品的评分</span></span><br><span class="line">        sim_user_rating_for_item = sim_user_rated_movies[iid]</span><br><span class="line">        <span class="comment"># 计算分子的值</span></span><br><span class="line">        numerator += similarity * sim_user_rating_for_item</span><br><span class="line">        <span class="comment"># 计算分母的值</span></span><br><span class="line">        denominator += similarity</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算预测的评分值并返回</span></span><br><span class="line">    predict_rating = numerator/denominator</span><br><span class="line">    print(<span class="string">"预测出用户&lt;%d&gt;对电影&lt;%d&gt;的评分：%0.2f"</span> % (uid, iid, predict_rating))</span><br><span class="line">    <span class="keyword">return</span> round(predict_rating, <span class="number">2</span>)</span><br></pre></td></tr></table></figure></li><li><p>为某一用户预测所有电影评分 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_all</span><span class="params">(uid, ratings_matrix, user_similar)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    预测全部评分</span></span><br><span class="line"><span class="string">    :param uid: 用户id</span></span><br><span class="line"><span class="string">    :param ratings_matrix: 用户-物品打分矩阵</span></span><br><span class="line"><span class="string">    :param user_similar: 用户两两间的相似度</span></span><br><span class="line"><span class="string">    :return: 生成器，逐个返回预测评分</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 准备要预测的物品的id列表</span></span><br><span class="line">    item_ids = ratings_matrix.columns</span><br><span class="line">    <span class="comment"># 逐个预测</span></span><br><span class="line">    <span class="keyword">for</span> iid <span class="keyword">in</span> item_ids:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            rating = predict(uid, iid, ratings_matrix, user_similar)</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            print(e)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">yield</span> uid, iid, rating</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> predict_all(<span class="number">1</span>, ratings_matrix, user_similar):</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></li><li><p>根据评分为指定用户推荐topN个电影 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">top_k_rs_result</span><span class="params">(k)</span>:</span></span><br><span class="line">    results = predict_all(<span class="number">1</span>, ratings_matrix, user_similar)</span><br><span class="line">    <span class="keyword">return</span> sorted(results, key=<span class="keyword">lambda</span> x: x[<span class="number">2</span>], reverse=<span class="literal">True</span>)[:k]</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">from</span> pprint <span class="keyword">import</span> pprint</span><br><span class="line">    result = top_k_rs_result(<span class="number">20</span>)</span><br><span class="line">    pprint(result)</span><br></pre></td></tr></table></figure></li></ul><h4 id="Item-Based-CF-预测电影评分"><a href="#Item-Based-CF-预测电影评分" class="headerlink" title="Item-Based CF 预测电影评分"></a>Item-Based CF 预测电影评分</h4><ul><li><p>加载ratings.csv，转换为用户-电影评分矩阵并计算用户之间相似度 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">DATA_PATH = <span class="string">"./datasets/ml-latest-small/ratings.csv"</span></span><br><span class="line"></span><br><span class="line">dtype = &#123;<span class="string">"userId"</span>: np.int32, <span class="string">"movieId"</span>: np.int32, <span class="string">"rating"</span>: np.float32&#125;</span><br><span class="line"><span class="comment"># 加载数据，我们只用前三列数据，分别是用户ID，电影ID，已经用户对电影的对应评分</span></span><br><span class="line">ratings = pd.read_csv(data_path, dtype=dtype, usecols=range(<span class="number">3</span>))</span><br><span class="line"><span class="comment"># 透视表，将电影ID转换为列名称，转换成为一个User-Movie的评分矩阵</span></span><br><span class="line">ratings_matrix = ratings.pivot_table(index=[<span class="string">"userId"</span>], columns=[<span class="string">"movieId"</span>],values=<span class="string">"rating"</span>)</span><br><span class="line"><span class="comment">#计算用户之间相似度</span></span><br><span class="line">item_similar = ratings_matrix.corr()</span><br></pre></td></tr></table></figure></li><li><p>预测用户对物品的评分 （以用户1对电影1评分为例） </p></li><li><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227155455.png"/></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 找出iid物品的相似物品</span></span><br><span class="line">  similar_items = item_similar[<span class="number">1</span>].drop([<span class="number">1</span>]).dropna()</span><br><span class="line">  <span class="comment"># 相似物品筛选规则：正相关的物品</span></span><br><span class="line">  similar_items = similar_items.where(similar_items&gt;<span class="number">0</span>).dropna()</span><br><span class="line">  <span class="comment"># 2. 从iid物品的近邻相似物品中筛选出uid用户评分过的物品</span></span><br><span class="line">  ids = set(ratings_matrix.ix[<span class="number">1</span>].dropna().index)&amp;set(similar_items.index)</span><br><span class="line">  finally_similar_items = similar_items.ix[list(ids)]</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 3. 结合iid物品与其相似物品的相似度和uid用户对其相似物品的评分，预测uid对iid的评分</span></span><br><span class="line">  numerator = <span class="number">0</span>    <span class="comment"># 评分预测公式的分子部分的值</span></span><br><span class="line">  denominator = <span class="number">0</span>    <span class="comment"># 评分预测公式的分母部分的值</span></span><br><span class="line">  <span class="keyword">for</span> sim_iid, similarity <span class="keyword">in</span> finally_similar_items.iteritems():</span><br><span class="line">      <span class="comment"># 近邻物品的评分数据</span></span><br><span class="line">      sim_item_rated_movies = ratings_matrix[sim_iid].dropna()</span><br><span class="line">      <span class="comment"># 1用户对相似物品物品的评分</span></span><br><span class="line">      sim_item_rating_from_user = sim_item_rated_movies[<span class="number">1</span>]</span><br><span class="line">      <span class="comment"># 计算分子的值</span></span><br><span class="line">      numerator += similarity * sim_item_rating_from_user</span><br><span class="line">      <span class="comment"># 计算分母的值</span></span><br><span class="line">      denominator += similarity</span><br><span class="line">  </span><br><span class="line">  <span class="comment"># 计算预测的评分值并返回</span></span><br><span class="line">  predict_rating = sum_up/sum_down</span><br><span class="line">  print(<span class="string">"预测出用户&lt;%d&gt;对电影&lt;%d&gt;的评分：%0.2f"</span> % (uid, iid, predict_rating))</span><br></pre></td></tr></table></figure></li><li><p>封装成方法 预测任意用户对任意电影的评分 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(uid, iid, ratings_matrix, user_similar)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    预测给定用户对给定物品的评分值</span></span><br><span class="line"><span class="string">    :param uid: 用户ID</span></span><br><span class="line"><span class="string">    :param iid: 物品ID</span></span><br><span class="line"><span class="string">    :param ratings_matrix: 用户-物品评分矩阵</span></span><br><span class="line"><span class="string">    :param user_similar: 用户两两相似度矩阵</span></span><br><span class="line"><span class="string">    :return: 预测的评分值</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    print(<span class="string">"开始预测用户&lt;%d&gt;对电影&lt;%d&gt;的评分..."</span>%(uid, iid))</span><br><span class="line">    <span class="comment"># 1. 找出uid用户的相似用户</span></span><br><span class="line">    similar_users = user_similar[uid].drop([uid]).dropna()</span><br><span class="line">    <span class="comment"># 相似用户筛选规则：正相关的用户</span></span><br><span class="line">    similar_users = similar_users.where(similar_users&gt;<span class="number">0</span>).dropna()</span><br><span class="line">    <span class="keyword">if</span> similar_users.empty <span class="keyword">is</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">raise</span> Exception(<span class="string">"用户&lt;%d&gt;没有相似的用户"</span> % uid)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 2. 从uid用户的近邻相似用户中筛选出对iid物品有评分记录的近邻用户</span></span><br><span class="line">    ids = set(ratings_matrix[iid].dropna().index)&amp;set(similar_users.index)</span><br><span class="line">    finally_similar_users = similar_users.ix[list(ids)]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 3. 结合uid用户与其近邻用户的相似度预测uid用户对iid物品的评分</span></span><br><span class="line">    numerator = <span class="number">0</span>    <span class="comment"># 评分预测公式的分子部分的值</span></span><br><span class="line">    denominator = <span class="number">0</span>    <span class="comment"># 评分预测公式的分母部分的值</span></span><br><span class="line">    <span class="keyword">for</span> sim_uid, similarity <span class="keyword">in</span> finally_similar_users.iteritems():</span><br><span class="line">        <span class="comment"># 近邻用户的评分数据</span></span><br><span class="line">        sim_user_rated_movies = ratings_matrix.ix[sim_uid].dropna()</span><br><span class="line">        <span class="comment"># 近邻用户对iid物品的评分</span></span><br><span class="line">        sim_user_rating_for_item = sim_user_rated_movies[iid]</span><br><span class="line">        <span class="comment"># 计算分子的值</span></span><br><span class="line">        numerator += similarity * sim_user_rating_for_item</span><br><span class="line">        <span class="comment"># 计算分母的值</span></span><br><span class="line">        denominator += similarity</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 计算预测的评分值并返回</span></span><br><span class="line">    predict_rating = numerator/denominator</span><br><span class="line">    print(<span class="string">"预测出用户&lt;%d&gt;对电影&lt;%d&gt;的评分：%0.2f"</span> % (uid, iid, predict_rating))</span><br><span class="line">    <span class="keyword">return</span> round(predict_rating, <span class="number">2</span>)</span><br></pre></td></tr></table></figure></li><li><p>为某一用户预测所有电影评分 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_all</span><span class="params">(uid, ratings_matrix, item_similar)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    预测全部评分</span></span><br><span class="line"><span class="string">    :param uid: 用户id</span></span><br><span class="line"><span class="string">    :param ratings_matrix: 用户-物品打分矩阵</span></span><br><span class="line"><span class="string">    :param item_similar: 物品两两间的相似度</span></span><br><span class="line"><span class="string">    :return: 生成器，逐个返回预测评分</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="comment"># 准备要预测的物品的id列表</span></span><br><span class="line">    item_ids = ratings_matrix.columns</span><br><span class="line">    <span class="comment"># 逐个预测</span></span><br><span class="line">    <span class="keyword">for</span> iid <span class="keyword">in</span> item_ids:</span><br><span class="line">        <span class="keyword">try</span>:</span><br><span class="line">            rating = predict(uid, iid, ratings_matrix, item_similar)</span><br><span class="line">        <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">            print(e)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">yield</span> uid, iid, rating</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> predict_all(<span class="number">1</span>, ratings_matrix, item_similar):</span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure></li><li><p>根据评分为指定用户推荐topN个电影 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">top_k_rs_result</span><span class="params">(k)</span>:</span></span><br><span class="line">    results = predict_all(<span class="number">1</span>, ratings_matrix, item_similar)</span><br><span class="line">    <span class="keyword">return</span> sorted(results, key=<span class="keyword">lambda</span> x: x[<span class="number">2</span>], reverse=<span class="literal">True</span>)[:k]</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">from</span> pprint <span class="keyword">import</span> pprint</span><br><span class="line">    result = top_k_rs_result(<span class="number">20</span>)</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;学习目标&quot;&gt;&lt;a href=&quot;#学习目标&quot; class=&quot;headerlink&quot; title=&quot;学习目标&quot;&gt;&lt;/a&gt;学习目标&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;应用基于用户的协同过滤实现电影评分预测&lt;/li&gt;
&lt;li&gt;应用基于物品的协同过滤实现电影评分预测&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="大数据推荐系统" scheme="https://xiaoliaozi.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="基于用户的协同过滤" scheme="https://xiaoliaozi.com/tags/%E5%9F%BA%E4%BA%8E%E7%94%A8%E6%88%B7%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/"/>
    
      <category term="基于物品的协同过滤" scheme="https://xiaoliaozi.com/tags/%E5%9F%BA%E4%BA%8E%E7%89%A9%E5%93%81%E7%9A%84%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4/"/>
    
  </entry>
  
  <entry>
    <title>特征工程-特征降维</title>
    <link href="https://xiaoliaozi.com/2020/02/03/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4/"/>
    <id>https://xiaoliaozi.com/2020/02/03/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4/</id>
    <published>2020-02-03T04:52:16.000Z</published>
    <updated>2020-02-27T07:42:45.410Z</updated>
    
    <content type="html"><![CDATA[<h4 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h4><ul><li>了解降维的定义</li><li>知道通过低方差过滤实现降维过程</li><li>知道相关系数实现降维的过程</li><li>知道主成分分析法实现过程</li></ul><a id="more"></a><h4 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h4><h5 id="定义"><a href="#定义" class="headerlink" title="定义"></a>定义</h5><p> <strong>降维</strong>是指在某些限定条件下，<strong>降低随机变量(特征)个数</strong>，得到<strong>一组“不相关”主变量</strong>的过程 </p><ul><li><p>降低随机变量的个数 </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227154011.png"/></p></li><li><p>相关特征(correlated feature)</p><ul><li>相对湿度与降雨量之间的相关</li><li>等等</li></ul></li></ul><blockquote><p>正是因为在进行训练的时候，我们都是使用特征进行学习。如果特征本身存在问题或者特征之间相关性较强，对于算法学习预测会影响较大 </p></blockquote><h5 id="降维的两种方式"><a href="#降维的两种方式" class="headerlink" title="降维的两种方式"></a>降维的两种方式</h5><ul><li>特征选择</li><li>主成分分析（可以理解一种特征提取的方式）</li></ul><h4 id="特征选择"><a href="#特征选择" class="headerlink" title="特征选择"></a>特征选择</h4><h5 id="定义-1"><a href="#定义-1" class="headerlink" title="定义"></a>定义</h5><p> 数据中包含<strong>冗余或无关变量（或称特征、属性、指标等）</strong>，旨在从<strong>原有特征中找出主要特征</strong>。 </p><h5 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h5><ul><li>Filter(过滤式)：主要探究特征本身特点、特征与特征和目标值之间关联<ul><li><strong>方差选择法：低方差特征过滤</strong></li><li><strong>相关系数</strong></li></ul></li><li>Embedded (嵌入式)：算法自动选择特征（特征与目标值之间的关联）<ul><li><strong>决策树:信息熵、信息增益</strong></li><li><strong>正则化：L1、L2</strong></li><li><strong>深度学习：卷积等</strong></li></ul></li></ul><h5 id="低方差特征过滤"><a href="#低方差特征过滤" class="headerlink" title="低方差特征过滤"></a>低方差特征过滤</h5><p>1.<strong>定义</strong></p><figure class="highlight asciidoc"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">删除低方差的一些特征，前面讲过方差的意义。再结合方差的大小来考虑这个方式的角度。</span><br><span class="line"><span class="bullet">- </span>特征方差小：某个特征大多样本的值比较相近</span><br><span class="line"><span class="bullet">- </span>特征方差大：某个特征很多样本的值都有差别</span><br></pre></td></tr></table></figure><p>2.<strong>API</strong></p><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sklearn.feature_selection.<span class="constructor">VarianceThreshold(<span class="params">threshold</span> = 0.0)</span></span><br><span class="line">删除所有低方差特征</span><br><span class="line"><span class="module-access"><span class="module"><span class="identifier">Variance</span>.</span></span>fit<span class="constructor">_transform(X)</span></span><br><span class="line">X:numpy <span class="built_in">array</span>格式的数据<span class="literal">[<span class="identifier">n_samples</span>,<span class="identifier">n_features</span>]</span></span><br><span class="line">返回值：训练集差异低于threshold的特征将被删除。默认值是保留所有非零方差特征，即删除所有样本中具有相同值的特征。</span><br></pre></td></tr></table></figure><p>3.<strong>案例</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">“”“</span><br><span class="line">我们对某些股票的指标特征之间进行一个筛选，除去<span class="string">'index,'</span>date<span class="string">','</span><span class="keyword">return</span><span class="string">'列不考虑（这些类型不匹配，也不是所需要指标）</span></span><br><span class="line"><span class="string">”“”</span></span><br><span class="line"><span class="string">import pandas as pd</span></span><br><span class="line"><span class="string">from sklearn.feature_selection import VarianceThreshold</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">def var_thr():</span></span><br><span class="line"><span class="string">    """低方差过滤"""</span></span><br><span class="line"><span class="string">    data = pd.read_csv("./data/factor_returns.csv")</span></span><br><span class="line"><span class="string">    # print(data)</span></span><br><span class="line"><span class="string">    print(data.shape)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    # 实例化一个对象</span></span><br><span class="line"><span class="string">    tranfer = VarianceThreshold(threshold=1)</span></span><br><span class="line"><span class="string">    # 转换</span></span><br><span class="line"><span class="string">    tranfer_data = tranfer.fit_transform(data.iloc[:, 1:10])</span></span><br><span class="line"><span class="string">    # print(tranfer_data)</span></span><br><span class="line"><span class="string">    print(tranfer_data.shape)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">if __name__ == '</span>__main__<span class="string">':</span></span><br><span class="line"><span class="string">    var_thr()</span></span><br></pre></td></tr></table></figure><p>4.<strong>相关系数</strong></p><p>主要实现方式：</p><ul><li>皮尔逊相关系数</li><li>斯皮尔曼相关系数</li></ul><p>1.<strong>皮尔逊相关系数</strong>(Pearson Correlation Coefficient)</p><p><strong>作用</strong>： 反映变量之间相关关系密切程度的统计指标 </p><p><strong>公式</strong>： <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227154029.png"/> </p><p> <strong>举例</strong>:</p><p>  比如说我们计算年广告费投入与月均销售额 </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227154004.png"/></p><p> 那么之间的相关系数怎么计算</p><p> <img src="%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4/%E7%9B%B8%E5%85%B3%E7%B3%BB%E6%95%B01.png" alt="img"> </p><p> 最终计算： </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227154000.png"/> </p><p>= 0.9942</p><p><strong>所以我们最终得出结论是广告投入费与月平均销售额之间有高度的正相关关系。</strong></p><p><strong>特点</strong>：</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">相关系数的值介于–<span class="number">1</span>与+<span class="number">1</span>之间，即–<span class="number">1</span>≤ r ≤+<span class="number">1</span>。其性质如下：</span><br><span class="line">当r&gt;<span class="number">0</span>时，表示两变量正相关，r&lt;<span class="number">0</span>时，两变量为负相关</span><br><span class="line">当|r|=<span class="number">1</span>时，表示两变量为完全相关，当r=<span class="number">0</span>时，表示两变量间无相关关系</span><br><span class="line">当<span class="number">0</span>&lt;|r|&lt;<span class="number">1</span>时，表示两变量存在一定程度的相关。且|r|越接近<span class="number">1</span>，两变量间线性关系越密切；|r|越接近于<span class="number">0</span>，表示两变量的线性相关越弱</span><br><span class="line">一般可按三级划分：|r|&lt;<span class="number">0.4</span>为低度相关；<span class="number">0.4</span>≤|r|&lt;<span class="number">0.7</span>为显著性相关；<span class="number">0.7</span>≤|r|&lt;<span class="number">1</span>为高度线性相关</span><br></pre></td></tr></table></figure><p><strong>API</strong>：</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line">x : (N,) <span class="built_in">array</span>_like</span><br><span class="line">y : (N,) <span class="built_in">array</span>_like Returns: (Pearson’s correlation coefficient, p-value)</span><br></pre></td></tr></table></figure><p><strong>案例</strong>：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pea_demo</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""皮尔逊相关系数"""</span></span><br><span class="line">    <span class="comment"># 1.准备数据</span></span><br><span class="line">    x1 = [<span class="number">12.5</span>, <span class="number">15.3</span>, <span class="number">23.2</span>, <span class="number">26.4</span>, <span class="number">33.5</span>, <span class="number">34.4</span>, <span class="number">39.4</span>, <span class="number">45.2</span>, <span class="number">55.4</span>, <span class="number">60.9</span>]</span><br><span class="line">    x2 = [<span class="number">21.2</span>, <span class="number">23.9</span>, <span class="number">32.9</span>, <span class="number">34.1</span>, <span class="number">42.5</span>, <span class="number">43.2</span>, <span class="number">49.0</span>, <span class="number">52.8</span>, <span class="number">59.4</span>, <span class="number">63.5</span>]</span><br><span class="line">    <span class="comment"># 判断</span></span><br><span class="line">    ret = pearsonr(x1, x2)</span><br><span class="line">    print(<span class="string">"皮尔逊相关系数:\n"</span>, ret)</span><br><span class="line">     </span><br><span class="line">pea_demo()</span><br><span class="line"><span class="comment"># (0.9941983762371883, 4.9220899554573455e-09)</span></span><br></pre></td></tr></table></figure><p>2.<strong>斯皮尔曼相关系数</strong>(Rank IC)</p><p><strong>作用</strong>： 反映变量之间相关关系密切程度的统计指标 </p><p><strong>公式</strong>：  <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227154022.png"/> </p><p>n为等级个数，d为二列成对变量的等级差数 </p><p><strong>特点</strong>：</p><figure class="highlight tp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">斯皮尔曼相关系数表明 <span class="keyword">X</span> (自变量) 和 <span class="keyword">Y</span> (因变量)的相关方向。 如果当<span class="keyword">X</span>增加时， <span class="keyword">Y</span> 趋向于增加, 斯皮尔曼相关系数则为正</span><br><span class="line">与之前的皮尔逊相关系数大小性质一样，取值 [<span class="number">-1</span>, <span class="number">1</span>]之间</span><br><span class="line">注意：斯皮尔曼相关系数比皮尔逊相关系数应用更加广泛</span><br></pre></td></tr></table></figure><p><strong>API</strong></p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> spearmanr</span><br></pre></td></tr></table></figure><p><strong>案例</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> spearmanr</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">spear_demo</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""斯皮尔曼相关系数"""</span></span><br><span class="line">    <span class="comment"># 1.准备数据</span></span><br><span class="line">    x1 = [<span class="number">12.5</span>, <span class="number">15.3</span>, <span class="number">23.2</span>, <span class="number">26.4</span>, <span class="number">33.5</span>, <span class="number">34.4</span>, <span class="number">39.4</span>, <span class="number">45.2</span>, <span class="number">55.4</span>, <span class="number">60.9</span>]</span><br><span class="line">    x2 = [<span class="number">21.2</span>, <span class="number">23.9</span>, <span class="number">32.9</span>, <span class="number">34.1</span>, <span class="number">42.5</span>, <span class="number">43.2</span>, <span class="number">49.0</span>, <span class="number">52.8</span>, <span class="number">59.4</span>, <span class="number">63.5</span>]</span><br><span class="line">    <span class="comment"># 判断</span></span><br><span class="line">    ret = spearmanr(x1, x2)</span><br><span class="line">    print(<span class="string">"斯皮尔曼相关系数：\n"</span>, ret)</span><br><span class="line">    </span><br><span class="line">spear_demo()</span><br></pre></td></tr></table></figure><h4 id="主成分分析"><a href="#主成分分析" class="headerlink" title="主成分分析"></a>主成分分析</h4><h5 id="什么是主成分分析"><a href="#什么是主成分分析" class="headerlink" title="什么是主成分分析"></a>什么是主成分分析</h5><ul><li>定义：<strong>高维数据转化为低维数据的过程</strong>，在此过程中<strong>可能会舍弃原有数据、创造新的变量</strong></li><li>作用：<strong>是数据维数压缩，尽可能降低原数据的维数（复杂度），损失少量信息。</strong></li><li>应用：回归分析或者聚类分析当中</li><li><strong>图示</strong>： <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227154036.png"/> </li></ul><h5 id="API"><a href="#API" class="headerlink" title="API"></a>API</h5><figure class="highlight reasonml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sklearn.decomposition.<span class="constructor">PCA(<span class="params">n_components</span>=None)</span></span><br><span class="line">- 将数据分解为较低维数空间</span><br><span class="line">- n_components:</span><br><span class="line">  - 小数：表示保留百分之多少的信息</span><br><span class="line">  - 整数：减少到多少特征</span><br><span class="line">- <span class="module-access"><span class="module"><span class="identifier">PCA</span>.</span></span>fit<span class="constructor">_transform(X)</span> X:numpy <span class="built_in">array</span>格式的数据<span class="literal">[<span class="identifier">n_samples</span>,<span class="identifier">n_features</span>]</span></span><br><span class="line">- 返回值：转换后指定维度的<span class="built_in">array</span></span><br></pre></td></tr></table></figure><h5 id="数据计算"><a href="#数据计算" class="headerlink" title="数据计算"></a>数据计算</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pca_demo</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    对数据进行PCA降维</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    data = [[<span class="number">2</span>,<span class="number">8</span>,<span class="number">4</span>,<span class="number">5</span>], [<span class="number">6</span>,<span class="number">3</span>,<span class="number">0</span>,<span class="number">8</span>], [<span class="number">5</span>,<span class="number">4</span>,<span class="number">9</span>,<span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1、实例化PCA, 小数——保留多少信息</span></span><br><span class="line">    transfer = PCA(n_components=<span class="number">0.9</span>)</span><br><span class="line">    <span class="comment"># 2、调用fit_transform</span></span><br><span class="line">    data1 = transfer.fit_transform(data)</span><br><span class="line"></span><br><span class="line">    print(<span class="string">"保留90%的信息，降维结果为：\n"</span>, data1)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1、实例化PCA, 整数——指定降维到的维数</span></span><br><span class="line">    transfer2 = PCA(n_components=<span class="number">3</span>)</span><br><span class="line">    <span class="comment"># 2、调用fit_transform</span></span><br><span class="line">    data2 = transfer2.fit_transform(data)</span><br><span class="line">    print(<span class="string">"降维到3维的结果：\n"</span>, data2)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    pca_demo()</span><br><span class="line">    </span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">保留90%的信息，降维结果为：</span></span><br><span class="line"><span class="string"> [[ -3.13587302e-16   3.82970843e+00]</span></span><br><span class="line"><span class="string"> [ -5.74456265e+00  -1.91485422e+00]</span></span><br><span class="line"><span class="string"> [  5.74456265e+00  -1.91485422e+00]]</span></span><br><span class="line"><span class="string">降维到3维的结果：</span></span><br><span class="line"><span class="string"> [[ -3.13587302e-16   3.82970843e+00   4.59544715e-16]</span></span><br><span class="line"><span class="string"> [ -5.74456265e+00  -1.91485422e+00   4.59544715e-16]</span></span><br><span class="line"><span class="string"> [  5.74456265e+00  -1.91485422e+00   4.59544715e-16]]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">降维的定义：就是改变特征值，选择哪列保留，哪列删除，目标是得到一组”不相关“的主变量</span><br><span class="line">降维的两种方式：<span class="number">1.</span>特征选择，<span class="number">2.</span>主成分分析（可以理解一种特征提取的方式）</span><br><span class="line">特征选择</span><br><span class="line">定义：提出数据中的冗余变量</span><br><span class="line">方法：</span><br><span class="line">Filter(过滤式)：主要探究特征本身特点、特征与特征和目标值之间关联</span><br><span class="line">方差选择法：低方差特征过滤</span><br><span class="line">相关系数</span><br><span class="line">Embedded (嵌入式)：算法自动选择特征（特征与目标值之间的关联）</span><br><span class="line">决策树:信息熵、信息增益</span><br><span class="line">正则化：L1、L2</span><br><span class="line">低方差特征过滤:把方差比较小的某一列进行剔除</span><br><span class="line">api:sklearn.feature_selection.VarianceThreshold(threshold = <span class="number">0.0</span>)</span><br><span class="line">删除所有低方差特征</span><br><span class="line">注意，参数threshold一定要进行值的指定</span><br><span class="line">相关系数</span><br><span class="line">主要实现方式：</span><br><span class="line">皮尔逊相关系数</span><br><span class="line">通过具体值的大小进行计算</span><br><span class="line">相对复杂</span><br><span class="line">api:<span class="keyword">from</span> scipy.stats <span class="keyword">import</span> pearsonr</span><br><span class="line">返回值，越接近|<span class="number">1</span>|，相关性越强；越接近<span class="number">0</span>，相关性越弱</span><br><span class="line">斯皮尔曼相关系数</span><br><span class="line">通过等级差进行计算</span><br><span class="line">比上一个简单</span><br><span class="line">api:<span class="keyword">from</span> scipy.stats <span class="keyword">import</span> spearmanr</span><br><span class="line">返回值，越接近|<span class="number">1</span>|，相关性越强；越接近<span class="number">0</span>，相关性越弱</span><br><span class="line">主成分分析pca</span><br><span class="line">定义：高维数据转换为低维数据，然后产生了新的变量</span><br><span class="line">api:sklearn.decomposition.PCA(n_components=None)</span><br><span class="line">n_components</span><br><span class="line">整数 -- 表示降低到几维</span><br><span class="line">小数 -- 保留百分之多少的信息</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;学习目标&quot;&gt;&lt;a href=&quot;#学习目标&quot; class=&quot;headerlink&quot; title=&quot;学习目标&quot;&gt;&lt;/a&gt;学习目标&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;了解降维的定义&lt;/li&gt;
&lt;li&gt;知道通过低方差过滤实现降维过程&lt;/li&gt;
&lt;li&gt;知道相关系数实现降维的过程&lt;/li&gt;
&lt;li&gt;知道主成分分析法实现过程&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://xiaoliaozi.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://xiaoliaozi.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="特征工程" scheme="https://xiaoliaozi.com/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
      <category term="特征降维" scheme="https://xiaoliaozi.com/tags/%E7%89%B9%E5%BE%81%E9%99%8D%E7%BB%B4/"/>
    
      <category term="特征选择" scheme="https://xiaoliaozi.com/tags/%E7%89%B9%E5%BE%81%E9%80%89%E6%8B%A9/"/>
    
      <category term="主成分分析" scheme="https://xiaoliaozi.com/tags/%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90/"/>
    
  </entry>
  
  <entry>
    <title>聚类算法</title>
    <link href="https://xiaoliaozi.com/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    <id>https://xiaoliaozi.com/2020/02/01/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/</id>
    <published>2020-02-01T05:09:38.000Z</published>
    <updated>2020-02-27T07:43:11.565Z</updated>
    
    <content type="html"><![CDATA[<h4 id="聚类算法简介"><a href="#聚类算法简介" class="headerlink" title="聚类算法简介"></a>聚类算法简介</h4><p> <strong>使用不同的聚类准则，产生的聚类结果不同</strong>。 </p><a id="more"></a><ol><li><h5 id="聚类算法在现实中的应用"><a href="#聚类算法在现实中的应用" class="headerlink" title="聚类算法在现实中的应用"></a>聚类算法在现实中的应用</h5><ul><li>用户画像，广告推荐，Data Segmentation，搜索引擎的流量推荐，恶意流量识别</li><li>基于位置信息的商业推送，新闻聚类，筛选排序</li><li>图像分割，降维，识别；离群点检测；信用卡异常消费；发掘相同功能的基因片段</li></ul></li><li><h5 id="聚类算法的概念"><a href="#聚类算法的概念" class="headerlink" title="聚类算法的概念"></a>聚类算法的概念</h5><p>一种典型的<strong>无监督</strong>学习算法，主要用于将相似的样本自动归到一个类别中。在聚类算法中根据样本之间的相似性，将样本划分到不同的类别中，对于不同的相似度计算方法，会得到不同的聚类结果，常用的相似度计算方法有欧式距离法。</p></li><li><h5 id="聚类算法与分类算法最大的区别"><a href="#聚类算法与分类算法最大的区别" class="headerlink" title="聚类算法与分类算法最大的区别"></a>聚类算法与分类算法最大的区别</h5><p> 聚类算法是无监督的学习算法，而分类算法属于监督的学习算法。 </p></li></ol><h4 id="聚类算法API"><a href="#聚类算法API" class="headerlink" title="聚类算法API"></a>聚类算法API</h4><ol><li><h5 id="API介绍"><a href="#API介绍" class="headerlink" title="API介绍"></a>API介绍</h5><figure class="highlight gml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sklearn.cluster.KMeans(n_clusters=<span class="number">8</span>)</span><br><span class="line">参数:</span><br><span class="line">n_clusters:开始的聚类中心数量</span><br><span class="line">整型，缺省值=<span class="number">8</span>，生成的聚类数，即产生的质心（centroids）数。</span><br><span class="line">方法:</span><br><span class="line">estimator.fit(<span class="symbol">x</span>)</span><br><span class="line">estimator.predict(<span class="symbol">x</span>)</span><br><span class="line">estimator.fit_predict(<span class="symbol">x</span>)</span><br><span class="line">计算聚类中心并预测每个样本属于哪个类别,相当于先调用fit(<span class="symbol">x</span>),然后再调用predict(<span class="symbol">x</span>)</span><br></pre></td></tr></table></figure></li><li><h5 id="小案例"><a href="#小案例" class="headerlink" title="小案例"></a>小案例</h5><p> 随机创建不同二维数据集作为训练集，并结合k-means算法将其聚类，你可以尝试分别聚类不同数量的簇，并观察聚类效果： </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227112324.png"/> </p><p> 聚类参数n_cluster传值不同，得到的聚类结果不同 </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227141148.png"/> </p><p>1.<strong>流程分析</strong></p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227112232.png"/> </p><p>2.<strong>代码实现</strong></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets.samples_generator <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> calinski_harabaz_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.创建数据集</span></span><br><span class="line"><span class="comment"># X为样本特征，Y为样本簇类别， 共1000个样本，每个样本4个特征，共4个簇，</span></span><br><span class="line"><span class="comment"># 簇中心在[-1,-1], [0,0],[1,1], [2,2]， 簇方差分别为[0.4, 0.2, 0.2, 0.2]</span></span><br><span class="line">X, y = make_blobs(n_samples=<span class="number">1000</span>, n_features=<span class="number">2</span>, centers=[[<span class="number">-1</span>, <span class="number">-1</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>]],</span><br><span class="line">                  cluster_std=[<span class="number">0.4</span>, <span class="number">0.2</span>, <span class="number">0.2</span>, <span class="number">0.2</span>],</span><br><span class="line">                  random_state=<span class="number">9</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据集可视化</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], marker=<span class="string">'o'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.使用k-means进行聚类,并使用CH方法评估</span></span><br><span class="line">y_pred = KMeans(n_clusters=<span class="number">2</span>, random_state=<span class="number">9</span>).fit_predict(X)</span><br><span class="line"><span class="comment"># 分别尝试n_cluses=2\3\4,然后查看聚类效果</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=y_pred)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用Calinski-Harabasz Index评估的聚类分数</span></span><br><span class="line">print(calinski_harabaz_score(X, y_pred))</span><br></pre></td></tr></table></figure></li></ol><h4 id="聚类算法实现流程"><a href="#聚类算法实现流程" class="headerlink" title="聚类算法实现流程"></a>聚类算法实现流程</h4><blockquote><p><strong>k-means其实包含两层内容：</strong></p><ul><li>K : 初始中心点个数（计划聚类数）</li><li>means：求中心点到其他数据点距离的平均值</li></ul></blockquote><h5 id="1-k-means聚类步骤"><a href="#1-k-means聚类步骤" class="headerlink" title="1. k-means聚类步骤"></a>1. k-means聚类步骤</h5><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>、随机设置K个特征空间内的点作为初始的聚类中心</span><br><span class="line"><span class="number">2</span>、对于其他每个点计算到K个中心的距离，未知的点选择最近的一个聚类中心点作为标记类别</span><br><span class="line"><span class="number">3</span>、接着对着标记的聚类中心之后，重新计算出每个聚类的新中心点（平均值）</span><br><span class="line"><span class="number">4</span>、如果计算得出的新中心点与原中心点一样（质心不再移动），那么结束，否则重新进行第二步过程</span><br></pre></td></tr></table></figure><p> 通过下图解释实现流程： </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227111050.png"/></p><p>  k聚类动态效果图 </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227141741.png"/></p><h5 id="2-案例练习"><a href="#2-案例练习" class="headerlink" title="2.案例练习"></a>2.案例练习</h5><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227112202.png"/></p><p>  1.随机设置K个特征空间内的点作为初始的聚类中心（本案例中设置p1和p2） </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227112154.png"/></p><p>  2.对于其他每个点计算到K个中心的距离，未知的点选择最近的一个聚类中心点作为标记类别 </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227112148.png"/></p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227112139.png"/></p><p>  3.接着对着标记的聚类中心之后，重新计算出每个聚类的新中心点（平均值） </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227112132.png"/></p><p> 4.如果计算得出的新中心点与原中心点一样（质心不再移动），那么结束，否则重新进行第二步过程【经过判断，需要重复上述步骤，开始新一轮迭代】 </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227112123.png"/></p><p>  <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227112119.png"/> </p><p>  5.当每次迭代结果不变时，认为算法收敛，聚类完成，<strong>K-Means一定会停下，不可能陷入一直选质心的过程。</strong> </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227112105.png"/></p><h4 id="聚类算法模型评估"><a href="#聚类算法模型评估" class="headerlink" title="聚类算法模型评估"></a>聚类算法模型评估</h4><h5 id="1-误差平方和-SSE-The-sum-of-squares-due-to-error"><a href="#1-误差平方和-SSE-The-sum-of-squares-due-to-error" class="headerlink" title="1.误差平方和(SSE \The sum of squares due to error)"></a>1.误差平方和(SSE \The sum of squares due to error)</h5><p> 举例:(下图中数据-0.2, 0.4, -0.8, 1.3, -0.7, 均为真实值和预测值的差) </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227110835.png"/></p><p>  在k-means中的应用: </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227110825.png"/></p><p>  <img src="%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/sse3.png" alt="image-20190219173610490"> </p><p> 公式各部分内容: </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227110811.png"/></p><p> 上图中: k=2</p><ul><li><p><strong>SSE图最终的结果，对图松散度的衡量.</strong>(eg: **SSE(左图))</p></li><li><p>SSE随着聚类迭代，其值会越来越小，直到最后趋于稳定</p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227110737.png"/> </p></li><li><p>如果质心的初始值选择不好，SSE只会达到一个不怎么好的局部最优解. </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227101738.png"/> </p></li></ul><h5 id="2-“肘”方法-Elbow-method-—-K值确定"><a href="#2-“肘”方法-Elbow-method-—-K值确定" class="headerlink" title="2.“肘”方法 (Elbow method) — K值确定"></a>2.“肘”方法 (Elbow method) — K值确定</h5><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227112225.png"/></p><p> （1）对于n个点的数据集，迭代计算k from 1 to n，每次聚类完成后计算每个点到其所属的簇中心的距离的平方和；</p><p>（2）平方和是会逐渐变小的，直到k==n时平方和为0，因为每个点都是它所在的簇中心本身。</p><p>（3）在这个平方和变化过程中，会出现一个拐点也即“肘”点，<strong>下降率突然变缓时即认为是最佳的k值</strong>。</p><p>在决定什么时候停止训练时，肘形判据同样有效，数据通常有更多的噪音，在<strong>增加分类无法带来更多回报时，我们停止增加类别</strong>。</p><h5 id="3-轮廓系数法（Silhouette-Coefficient）"><a href="#3-轮廓系数法（Silhouette-Coefficient）" class="headerlink" title="3.轮廓系数法（Silhouette Coefficient）"></a>3.轮廓系数法（Silhouette Coefficient）</h5><p> 结合了聚类的凝聚度（Cohesion）和分离度（Separation），用于评估聚类的效果： </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227111035.png"/></p><p> <strong>目的：</strong> 内部距离最小化，外部距离最大化</p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227110928.png"/> </p><p>计算样本$i$到同簇其他样本的平均距离$a_i$，$a_i$ 越小样本i的簇内不相似度越小，说明样本$i$越应该被聚类到该簇。</p><p>计算样本i到最近簇$C<em>j$ 的所有样本的平均距离$b</em>(ij)$，称样本$i$与最近簇$C_j$ 的不相似度，定义为样本i的簇间不相似度：</p><p>$b<em>i =min{b</em>(i1), b<em>(i2), …, b</em>(ik)}$，$b_i$越大，说明样本$i$越不属于其他簇。</p><p>求出所有样本的轮廓系数后再求平均值就得到了<strong>平均轮廓系数</strong>。</p><p>平均轮廓系数的取值范围为[-1,1]，系数越大，聚类效果越好。</p><p>簇内样本的距离越近，簇间样本距离越远</p><p><strong>案例：</strong></p><p>下图是500个样本含有2个feature的数据分布情况，我们对它进行SC系数效果衡量：</p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227110915.png"/> </p><p><strong>n_clusters = 2 The average silhouette_score is : 0.7049787496083262</strong></p><p>n_clusters = 3 The average silhouette_score is : 0.5882004012129721</p><p><strong>n_clusters = 4 The average silhouette_score is : 0.6505186632729437</strong></p><p>n_clusters = 5 The average silhouette_score is : 0.56376469026194</p><p>n_clusters = 6 The average silhouette_score is : 0.4504666294372765</p><p>n_clusters 分别为 2，3，4，5，6时，SC系数如下，是介于[-1,1]之间的度量指标：</p><p><strong>每次聚类后，每个样本都会得到一个轮廓系数，当它为1时，说明这个点与周围簇距离较远，结果非常好，当它为0，说明这个点可能处在两个簇的边界上，当值为负时，暗含该点可能被误分了。</strong></p><p>从平均SC系数结果来看，K取3，5，6是不好的，那么2和4呢？</p><p>k=2的情况：</p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227110855.png"/></p><p>  k=4的情况： </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227142806.png"/></p><p> n_clusters = 2时，第0簇的宽度远宽于第1簇；</p><p>n_clusters = 4时，所聚的簇宽度相差不大，因此选择K=4，作为最终聚类个数。</p><h5 id="4-CH系数（Calinski-Harabasz-Index）"><a href="#4-CH系数（Calinski-Harabasz-Index）" class="headerlink" title="4.CH系数（Calinski-Harabasz Index）"></a>4.CH系数（Calinski-Harabasz Index）</h5><p><strong>Calinski-Harabasz：</strong>类别内部数据的协方差越小越好，类别之间的协方差越大越好（换句话说：类别内部数据的距离平方和越小越好，类别之间的距离平方和越大越好），</p><p>这样的Calinski-Harabasz分数s会高，分数s高则聚类效果越好。</p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227112338.png"/> </p><p>tr为<strong>矩阵的迹</strong>, Bk为类别之间的协方差矩阵，Wk为类别内部数据的协方差矩阵;</p><p>m为训练集样本数，k为类别数。</p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227112330.png"/></p><p> 使用矩阵的迹进行求解的理解：</p><p>矩阵的对角线可以表示一个物体的相似性</p><p>在机器学习里，主要为了获取数据的特征值，那么就是说，在任何一个矩阵计算出来之后，都可以简单化，只要获取矩阵的迹，就可以表示这一块数据的最重要的特征了，这样就可以把很多无关紧要的数据删除掉，达到简化数据，提高处理速度。</p><p>CH需要达到的目的：<strong>用尽量少的类别聚类尽量多的样本，同时获得较好的聚类效果。</strong></p><h4 id="聚类算法算法优化"><a href="#聚类算法算法优化" class="headerlink" title="聚类算法算法优化"></a>聚类算法算法优化</h4><p><strong>k-means算法小结</strong></p><p><strong>优点：</strong></p><p> 1.原理简单（靠近中心点），实现容易</p><p> 2.聚类效果中上（依赖K的选择）</p><p> 3.空间复杂度o(N)，时间复杂度o(I<em>K</em>N)</p><blockquote><p>N为样本点个数，K为中心点个数，I为迭代次数</p></blockquote><p><strong>缺点：</strong></p><p> 1.对离群点，噪声敏感 （中心点易偏移）</p><p> 2.很难发现大小差别很大的簇及进行增量计算</p><p> 3.结果不一定是全局最优，只能保证局部最优（与K的个数及初值选取有关）</p><h5 id="1-Canopy算法配合初始聚类"><a href="#1-Canopy算法配合初始聚类" class="headerlink" title="1.Canopy算法配合初始聚类"></a>1.Canopy算法配合初始聚类</h5><p>Canopy算法配合初始聚类实现流程：</p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227112345.png"/></p><p> 优点：</p><ol><li><p>Kmeans对噪声抗干扰较弱，通过Canopy对比，将较小的NumPoint的Cluster直接去掉有利于抗干扰。</p></li><li><p>Canopy选择出来的每个Canopy的centerPoint作为K会更精确。</p></li><li><p>只是针对每个Canopy的内做Kmeans聚类，减少相似计算的数量。</p></li></ol><p>缺点：</p><p> 算法中 T1、T2的确定问题 ，依旧可能落入局部最优解</p><h5 id="2-K-means"><a href="#2-K-means" class="headerlink" title="2.K-means++"></a>2.K-means++</h5><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227111237.png"/> </p><p>  其中： <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227111057.png"/></p><p>  为方便后面表示，把其记为A </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227111217.png"/></p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227111212.png"/> </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227111104.png"/></p><p> kmeans++目的，让选择的质心尽可能的分散</p><p>如下图中，如果第一个质心选择在圆心，那么最优可能选择到的下一个点在P(A)这个区域（根据颜色进行划分）</p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227111230.png"/></p><h5 id="3-二分k-means"><a href="#3-二分k-means" class="headerlink" title="3.二分k-means"></a>3.二分k-means</h5><p>实现流程:</p><ol><li><p>所有点作为一个簇</p></li><li><p>将该簇一分为二</p></li><li><p>选择能最大限度降低聚类代价函数（也就是误差平方和）的簇划分为两个簇。</p></li><li><p>以此进行下去，直到簇的数目等于用户给定的数目k为止。</p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227112217.png"/> </p></li></ol><p><strong>隐含的一个原则</strong></p><p>因为聚类的误差平方和能够衡量聚类性能，该值越小表示数据点越接近于他们的质心，聚类效果就越好。所以需要对误差平方和最大的簇进行再一次划分，因为误差平方和越大，表示该簇聚类效果越不好，越有可能是多个簇被当成了一个簇，所以我们首先需要对这个簇进行划分。</p><p>二分K均值算法可以加速K-means算法的执行速度，因为它的相似度计算少了并且不受初始化问题的影响，因为这里不存在随机点的选取，且每一步都保证了误差最小</p><h5 id="4-k-medoids（k-中心聚类算法）"><a href="#4-k-medoids（k-中心聚类算法）" class="headerlink" title="4.k-medoids（k-中心聚类算法）"></a>4.k-medoids（k-中心聚类算法）</h5><p>K-medoids和K-means是有区别的，<strong>不一样的地方在于中心点的选取</strong></p><ul><li>K-means中，将中心点取为当前cluster中所有数据点的平均值，对异常点很敏感!</li><li><p>K-medoids中，将从当前cluster 中选取到其他所有（当前cluster中的）点的距离之和最小的点作为中心点。</p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227111042.png"/></p><p>算法流程：</p></li></ul><p>　　 ( 1 )总体n个样本点中任意选取k个点作为medoids</p><p>　　 ( 2 )按照与medoids最近的原则，将剩余的n-k个点分配到当前最佳的medoids代表的类中</p><p>　　 ( 3 )对于第i个类中除对应medoids点外的所有其他点，按顺序计算当其为新的medoids时，代价函数的值，遍历所有可能，选取代价函数最小时对应的点作为新的medoids</p><p>　　 ( 4 )重复2-3的过程，直到所有的medoids点不再发生变化或已达到设定的最大迭代次数</p><p>　　 ( 5 )产出最终确定的k个类</p><p><strong>k-medoids对噪声鲁棒性好。</strong></p><p>例：当一个cluster样本点只有少数几个，如（1,1）（1,2）（2,1）（1000,1000）。其中（1000,1000）是噪声。如果按照k-means质心大致会处在（1,1）（1000,1000）中间，这显然不是我们想要的。这时k-medoids就可以避免这种情况，他会在（1,1）（1,2）（2,1）（1000,1000）中选出一个样本点使cluster的绝对误差最小，计算可知一定会在前三个点中选取。</p><p>k-medoids只能对小样本起作用，样本大，速度就太慢了，当样本多的时候，少数几个噪音对k-means的质心影响也没有想象中的那么重，所以k-means的应用明显比k-medoids多。</p><h5 id="5-Kernel-k-means"><a href="#5-Kernel-k-means" class="headerlink" title="5.Kernel k-means"></a>5.Kernel k-means</h5><p> kernel k-means实际上，就是将每个样本进行一个投射到高维空间的处理，然后再将处理后的数据使用普通的k-means算法思想进行聚类。 </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227112209.png"/></p><h5 id="6-ISODATA"><a href="#6-ISODATA" class="headerlink" title="6.ISODATA"></a>6.ISODATA</h5><p>类别数目随着聚类过程而变化；</p><p>对类别数会进行合并，分裂，</p><p>“合并”：（当聚类结果某一类中样本数太少，或两个类间的距离太近时）</p><p>“分裂”：（当聚类结果中某一类的类内方差太大，将该类进行分裂）</p><h5 id="7-Mini-Batch-K-Means"><a href="#7-Mini-Batch-K-Means" class="headerlink" title="7.Mini Batch K-Means"></a>7.Mini Batch K-Means</h5><p>适合大数据的聚类算法</p><p>大数据量是什么量级？通常当样本量大于1万做聚类时，就需要考虑选用Mini Batch K-Means算法。</p><p>Mini Batch KMeans使用了Mini Batch（分批处理）的方法对数据点之间的距离进行计算。</p><p>Mini Batch计算过程中不必使用所有的数据样本，而是从不同类别的样本中抽取一部分样本来代表各自类型进行计算。由于计算样本量少，所以会相应的减少运行时间，但另一方面抽样也必然会带来准确度的下降。</p><p>该算法的迭代步骤有两步：</p><p>(1)从数据集中随机抽取一些数据形成小批量，把他们分配给最近的质心</p><p>(2)更新质心</p><p> 与Kmeans相比，数据的更新在每一个小的样本集上。对于每一个小批量，通过计算平均值得到更新质心，并把小批量里的数据分配给该质心，随着迭代次数的增加，这些质心的变化是逐渐减小的，直到质心稳定或者达到指定的迭代次数，停止计算。</p><p><strong>总结</strong></p><div class="table-container"><table><thead><tr><th><strong>优化方法</strong></th><th><strong>思路</strong></th></tr></thead><tbody><tr><td>Canopy+kmeans</td><td>Canopy粗聚类配合kmeans</td></tr><tr><td>kmeans++</td><td>距离越远越容易成为新的质心</td></tr><tr><td>二分k-means</td><td>拆除SSE最大的簇</td></tr><tr><td>k-medoids</td><td>和kmeans选取中心点的方式不同</td></tr><tr><td>kernel kmeans</td><td>映射到高维空间</td></tr><tr><td>ISODATA</td><td>动态聚类，可以更改K值大小</td></tr><tr><td>Mini-batch K-Means</td><td>大数据集分批聚类</td></tr></tbody></table></div><h4 id="案例-探究用户对物品类别的喜好细分"><a href="#案例-探究用户对物品类别的喜好细分" class="headerlink" title="案例-探究用户对物品类别的喜好细分"></a>案例-探究用户对物品类别的喜好细分</h4><h5 id="1-数据介绍"><a href="#1-数据介绍" class="headerlink" title="1.数据介绍"></a>1.数据介绍</h5><figure class="highlight mipsasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">数据如下：</span><br><span class="line"><span class="keyword">order_products__prior.csv：订单与商品信息</span></span><br><span class="line"><span class="keyword">字段：order_id, </span>product_id, <span class="keyword">add_to_cart_order, </span>reordered</span><br><span class="line">products.csv：商品信息</span><br><span class="line">字段：product_id, product_name, aisle_id, department_id</span><br><span class="line"><span class="keyword">orders.csv：用户的订单信息</span></span><br><span class="line"><span class="keyword">字段：order_id,user_id,eval_set,order_number,….</span></span><br><span class="line"><span class="keyword">aisles.csv：商品所属具体物品类别</span></span><br><span class="line"><span class="keyword">字段： </span>aisle_id, aisle</span><br></pre></td></tr></table></figure><h5 id="2-需求分析"><a href="#2-需求分析" class="headerlink" title="2.需求分析"></a>2.需求分析</h5><ol><li><p>获取数据</p></li><li><p>数据基本处理</p><p>2.1  合并表格</p><p>2.2 交叉表合并</p><p>2.3 数据截取</p></li><li><p>特征工程 — pca</p></li><li><p>机器学习（k-means）</p></li><li><p>模型评估</p><p>sklearn.metrics.silhouette_score(X, labels)</p><p>计算所有样本的平均轮廓系数</p><p>X：特征值</p><p>labels：被聚类标记的目标值</p></li></ol><h5 id="3-代码实现"><a href="#3-代码实现" class="headerlink" title="3.代码实现"></a>3.代码实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> silhouette_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.获取数据</span></span><br><span class="line">order_product = pd.read_csv(<span class="string">"./data/instacart/order_products__prior.csv"</span>)</span><br><span class="line">products = pd.read_csv(<span class="string">"./data/instacart/products.csv"</span>)</span><br><span class="line">orders = pd.read_csv(<span class="string">"./data/instacart/orders.csv"</span>)</span><br><span class="line">aisles = pd.read_csv(<span class="string">"./data/instacart/aisles.csv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.数据基本处理</span></span><br><span class="line"><span class="comment"># 2.1 合并表格</span></span><br><span class="line">table1 = pd.merge(order_product, products, on=[<span class="string">"product_id"</span>, <span class="string">"product_id"</span>])</span><br><span class="line">table2 = pd.merge(table1, orders, on=[<span class="string">"order_id"</span>, <span class="string">"order_id"</span>])</span><br><span class="line">table = pd.merge(table2, aisles, on=[<span class="string">"aisle_id"</span>, <span class="string">"aisle_id"</span>])</span><br><span class="line"><span class="comment"># 2.2 交叉表合并</span></span><br><span class="line">table = pd.crosstab(table[<span class="string">"user_id"</span>], table[<span class="string">"aisle"</span>])</span><br><span class="line"><span class="comment"># 2.3 数据截取</span></span><br><span class="line">table = table[:<span class="number">1000</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.特征工程 — pca</span></span><br><span class="line">transfer = PCA(n_components=<span class="number">0.9</span>)</span><br><span class="line">data = transfer.fit_transform(table)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.机器学习（k-means）</span></span><br><span class="line">estimator = KMeans(n_clusters=<span class="number">8</span>, random_state=<span class="number">22</span>)</span><br><span class="line">estimator.fit_predict(data)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.模型评估</span></span><br><span class="line">silhouette_score(data, y_predict)</span><br></pre></td></tr></table></figure><h4 id="拓展-算法选择指导"><a href="#拓展-算法选择指导" class="headerlink" title="拓展-算法选择指导"></a>拓展-算法选择指导</h4><p><strong>关于在计算的过程中，如何选择合适的算法进行计算，可以参考scikit learn官方给的指导意见：</strong></p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227110840.png"/> </p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;聚类算法简介&quot;&gt;&lt;a href=&quot;#聚类算法简介&quot; class=&quot;headerlink&quot; title=&quot;聚类算法简介&quot;&gt;&lt;/a&gt;聚类算法简介&lt;/h4&gt;&lt;p&gt; &lt;strong&gt;使用不同的聚类准则，产生的聚类结果不同&lt;/strong&gt;。 &lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习算法" scheme="https://xiaoliaozi.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="机器" scheme="https://xiaoliaozi.com/tags/%E6%9C%BA%E5%99%A8/"/>
    
      <category term="学习机器学习算法" scheme="https://xiaoliaozi.com/tags/%E5%AD%A6%E4%B9%A0%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
      <category term="聚类算法" scheme="https://xiaoliaozi.com/tags/%E8%81%9A%E7%B1%BB%E7%AE%97%E6%B3%95/"/>
    
      <category term="无监督学习算法" scheme="https://xiaoliaozi.com/tags/%E6%97%A0%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>推荐算法</title>
    <link href="https://xiaoliaozi.com/2020/01/15/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    <id>https://xiaoliaozi.com/2020/01/15/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/</id>
    <published>2020-01-15T03:12:49.000Z</published>
    <updated>2020-02-27T07:54:30.607Z</updated>
    
    <content type="html"><![CDATA[<h4 id="推荐模型构建流程"><a href="#推荐模型构建流程" class="headerlink" title="推荐模型构建流程"></a>推荐模型构建流程</h4><blockquote><p>Data(数据)-&gt;Features(特征)-&gt;ML Algorithm(选择算法训练模型)-&gt;Prediction Output(预测输出) </p></blockquote><a id="more"></a><ul><li><p>数据清洗/数据处理 </p><ul><li><p>数据来源</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">- 显性数据</span><br><span class="line">  - Rating 打分</span><br><span class="line">  - Comments 评论/评价</span><br><span class="line">- 隐形数据</span><br><span class="line">  -  Order history 历史订单</span><br><span class="line">  -  Cart events 加购物车</span><br><span class="line">  - <span class="built_in"> Page </span>views 页面浏览</span><br><span class="line">  -  Click-thru 点击</span><br><span class="line">  -  Search log 搜索记录</span><br></pre></td></tr></table></figure></li><li><p>数据量/数据能否满足要求</p></li></ul></li><li><p>特征工程 </p><ul><li><p>从数据中筛选特征</p><ul><li>一个给定的商品，可能被拥有类似品味或需求的用户购买</li><li>使用用户行为数据描述商品</li></ul></li><li><p>用数据表示特征</p><ul><li>将所有用户行为合并在一起 ，形成一个user-item 矩阵</li></ul></li></ul></li><li><p>选择合适的算法 </p><ul><li><p><strong>协同过滤</strong></p></li><li><p>基于内容</p></li></ul></li><li><p>产生推荐结果</p><ul><li>对推荐结果进行评估（评估方法后面章节介绍），评估通过后上线</li></ul></li></ul><h4 id="最经典的推荐算法：协同过滤推荐算法"><a href="#最经典的推荐算法：协同过滤推荐算法" class="headerlink" title="最经典的推荐算法：协同过滤推荐算法"></a>最经典的推荐算法：协同过滤推荐算法</h4><p>协同过滤推荐算法（Collaborative Filtering）</p><p>算法思想： <strong>物以类聚，人以群分</strong> </p><p> 基本的协同过滤推荐算法基于以下假设： </p><ul><li>“跟你喜好<strong>相似的人</strong>喜欢的东西你也很有可能喜欢” ：基于用户的协同过滤推荐（User-based CF）</li><li><p>“跟你喜欢的东西<strong>相似的东西</strong>你也很有可能喜欢 ”：基于物品的协同过滤推荐（Item-based CF）</p><p>实现协同过滤推荐步骤： </p></li></ul><p>1.<strong>找出最相似的人或物品：TOP-N相似的人或物品</strong></p><p>通过计算两两的相似度来进行排序，即可找出TOP-N相似的人或物品</p><p>2.<strong>根据相似的人或物品产生推荐结果</strong></p><p>利用TOP-N结果生成初始推荐结果，然后过滤掉用户已经有过记录的物品或明确表示不感兴趣的物品</p><p>简单例子：</p><p>  User-Based CF <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227154653.png"/></p><p> Item-Based CF   <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227154700.png"/> </p><h4 id="相似度计算-Similarity-Calculation"><a href="#相似度计算-Similarity-Calculation" class="headerlink" title="相似度计算(Similarity Calculation)"></a>相似度计算(Similarity Calculation)</h4><h5 id="相似度的计算方法"><a href="#相似度的计算方法" class="headerlink" title="相似度的计算方法"></a>相似度的计算方法</h5><p>1.<strong>欧氏距离</strong>， 是一个欧式空间下度量距离的方法. 两个物体， 都在同一个空间下表示为两个点,，假如叫做p，q， 分别都是n个坐标，那么欧式距离就是衡量这两个点之间的距离.。欧氏距离不适用于布尔向量之间。</p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227155047.png"/></p><p>  欧氏距离的值是一个非负数, 最大值正无穷, 通常计算相似度的结果希望是[-1,1]或[0,1]之间,一般可以使用</p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227154824.png"/></p><p>2.<strong>余弦相似度</strong> </p><ul><li>度量的是两个向量之间的夹角，用夹角的余弦值来度量相似的情况</li><li>两个向量的夹角为0是，余弦值为1，当夹角为90度是余弦值为0，为180度是余弦值为-1</li><li>余弦相似度在度量文本相似度, 用户相似度 物品相似度的时候较为常用</li><li><p>余弦相似度的特点， 与向量长度无关，余弦相似度计算要对向量长度归一化， 两个向量只要方向一致，无论程度强弱，都可以视为’相似’</p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227155116.png"/></p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227154815.png"/></p></li></ul><h5 id="皮尔逊相关系数Pearson"><a href="#皮尔逊相关系数Pearson" class="headerlink" title="皮尔逊相关系数Pearson"></a>皮尔逊相关系数Pearson</h5><ul><li>实际上也是余弦相似度，不过先对向量做了中心化，向量a，b各自减去向量的均值后, 再计算余弦相似度</li><li>皮尔逊相似度计算结果在-1，1之间 -1表示负相关， 1表示正相关</li><li>度量两个变量是不是同增同减</li><li><p>皮尔逊相关系数度量的是两个变量的变化趋势是否一致，<strong>不适合计算布尔值向量之间的相关度</strong></p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227154731.png"/> </p></li></ul><h5 id="杰卡德相似度-Jaccard"><a href="#杰卡德相似度-Jaccard" class="headerlink" title="杰卡德相似度 Jaccard"></a>杰卡德相似度 Jaccard</h5><ul><li>两个集合的交集元素个数在并集中所占的比例，非常适用于布尔向量表示</li><li>分子是两个布尔向量做点积计算, 得到的就是交集元素的个数</li><li><p>分母是两个布尔向量做或运算， 再求元素和</p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227154906.png"/> </p></li></ul><h5 id="如何选择余弦相似度"><a href="#如何选择余弦相似度" class="headerlink" title="如何选择余弦相似度"></a>如何选择余弦相似度</h5><ul><li>余弦相似度/皮尔逊相关系数适合用户评分数据(实数值)；</li><li>杰卡德相似度适用于隐式反馈数据(0，1布尔值 是否收藏，是否点击，是否加购物车)</li></ul><h4 id="协同过滤推荐算法代码案例"><a href="#协同过滤推荐算法代码案例" class="headerlink" title="协同过滤推荐算法代码案例"></a>协同过滤推荐算法代码案例</h4><p>1.构建数据集 </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">users = [<span class="string">"User1"</span>, <span class="string">"User2"</span>, <span class="string">"User3"</span>, <span class="string">"User4"</span>, <span class="string">"User5"</span>]</span><br><span class="line">items = [<span class="string">"Item A"</span>, <span class="string">"Item B"</span>, <span class="string">"Item C"</span>, <span class="string">"Item D"</span>, <span class="string">"Item E"</span>]</span><br><span class="line"><span class="comment"># 构建数据集</span></span><br><span class="line">datasets = [</span><br><span class="line">    [<span class="string">"buy"</span>,<span class="literal">None</span>,<span class="string">"buy"</span>,<span class="string">"buy"</span>,<span class="literal">None</span>],</span><br><span class="line">    [<span class="string">"buy"</span>,<span class="literal">None</span>,<span class="literal">None</span>,<span class="string">"buy"</span>,<span class="string">"buy"</span>],</span><br><span class="line">    [<span class="string">"buy"</span>,<span class="literal">None</span>,<span class="string">"buy"</span>,<span class="literal">None</span>,<span class="literal">None</span>],</span><br><span class="line">    [<span class="literal">None</span>,<span class="string">"buy"</span>,<span class="literal">None</span>,<span class="string">"buy"</span>,<span class="string">"buy"</span>],</span><br><span class="line">    [<span class="string">"buy"</span>,<span class="string">"buy"</span>,<span class="string">"buy"</span>,<span class="literal">None</span>,<span class="string">"buy"</span>],</span><br><span class="line">]</span><br></pre></td></tr></table></figure><p>2.计算时我们数据通常都需要对数据进行处理，或者编码，目的是为了便于我们对数据进行运算处理，比如这里是比较简单的情形，我们用1、0分别来表示用户的是否购买过该物品，则我们的数据集其实应该是这样的： </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">users = [<span class="string">"User1"</span>, <span class="string">"User2"</span>, <span class="string">"User3"</span>, <span class="string">"User4"</span>, <span class="string">"User5"</span>]</span><br><span class="line">items = [<span class="string">"Item A"</span>, <span class="string">"Item B"</span>, <span class="string">"Item C"</span>, <span class="string">"Item D"</span>, <span class="string">"Item E"</span>]</span><br><span class="line"><span class="comment"># 用户购买记录数据集</span></span><br><span class="line">datasets = [</span><br><span class="line">    [<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">    [<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">    [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],</span><br><span class="line">]</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(datasets,</span><br><span class="line">                  columns=items,</span><br><span class="line">                  index=users)</span><br><span class="line">print(df)</span><br></pre></td></tr></table></figure><p>3 .有了数据集，接下来我们就可以进行相似度的计算，不过对于相似度的计算其实是有很多专门的相似度计算方法的，比如余弦相似度、皮尔逊相关系数、杰卡德相似度等等。这里我们选择使用杰卡德相似系数[0,1] </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> jaccard_similarity_score</span><br><span class="line"><span class="comment"># 直接计算某两项的杰卡德相似系数</span></span><br><span class="line"><span class="comment"># 计算Item A 和Item B的相似度</span></span><br><span class="line">print(jaccard_similarity_score(df[<span class="string">"Item A"</span>], df[<span class="string">"Item B"</span>]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算所有的数据两两的杰卡德相似系数</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> pairwise_distances</span><br><span class="line"><span class="comment"># 计算用户间相似度</span></span><br><span class="line">user_similar = <span class="number">1</span> - pairwise_distances(df, metric=<span class="string">"jaccard"</span>)</span><br><span class="line">user_similar = pd.DataFrame(user_similar, columns=users, index=users)</span><br><span class="line">print(<span class="string">"用户之间的两两相似度："</span>)</span><br><span class="line">print(user_similar)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算物品间相似度</span></span><br><span class="line">item_similar = <span class="number">1</span> - pairwise_distances(df.T, metric=<span class="string">"jaccard"</span>)</span><br><span class="line">item_similar = pd.DataFrame(item_similar, columns=items, index=items)</span><br><span class="line">print(<span class="string">"物品之间的两两相似度："</span>)</span><br><span class="line">print(item_similar)</span><br></pre></td></tr></table></figure><p> 有了两两的相似度，接下来就可以筛选TOP-N相似结果，并进行推荐了 </p><p>4.User-Based CF </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> pprint <span class="keyword">import</span> pprint</span><br><span class="line"></span><br><span class="line">users = [<span class="string">"User1"</span>, <span class="string">"User2"</span>, <span class="string">"User3"</span>, <span class="string">"User4"</span>, <span class="string">"User5"</span>]</span><br><span class="line">items = [<span class="string">"Item A"</span>, <span class="string">"Item B"</span>, <span class="string">"Item C"</span>, <span class="string">"Item D"</span>, <span class="string">"Item E"</span>]</span><br><span class="line"><span class="comment"># 用户购买记录数据集</span></span><br><span class="line">datasets = [</span><br><span class="line">    [<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">    [<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">    [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(datasets,</span><br><span class="line">                  columns=items,</span><br><span class="line">                  index=users)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算所有的数据两两的杰卡德相似系数</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> pairwise_distances</span><br><span class="line"><span class="comment"># 计算用户间相似度  1-杰卡德距离=杰卡德相似度</span></span><br><span class="line">user_similar = <span class="number">1</span> - pairwise_distances(df, metric=<span class="string">"jaccard"</span>)</span><br><span class="line">user_similar = pd.DataFrame(user_similar, columns=users, index=users)</span><br><span class="line">print(<span class="string">"用户之间的两两相似度："</span>)</span><br><span class="line">print(user_similar)</span><br><span class="line"></span><br><span class="line">topN_users = &#123;&#125;</span><br><span class="line"><span class="comment"># 遍历每一行数据</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> user_similar.index:</span><br><span class="line">    <span class="comment"># 取出每一列数据，并删除自身，然后排序数据</span></span><br><span class="line">    _df = user_similar.loc[i].drop([i])</span><br><span class="line">    <span class="comment">#sort_values 排序 按照相似度降序排列</span></span><br><span class="line">    _df_sorted = _df.sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line">    <span class="comment"># 从排序之后的结果中切片 取出前两条（相似度最高的两个）</span></span><br><span class="line">    top2 = list(_df_sorted.index[:<span class="number">2</span>])</span><br><span class="line">    topN_users[i] = top2</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Top2相似用户："</span>)</span><br><span class="line">pprint(topN_users)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准备空白dict用来保存推荐结果</span></span><br><span class="line">rs_results = &#123;&#125;</span><br><span class="line"><span class="comment">#遍历所有的最相似用户</span></span><br><span class="line"><span class="keyword">for</span> user, sim_users <span class="keyword">in</span> topN_users.items():</span><br><span class="line">    rs_result = set()    <span class="comment"># 存储推荐结果</span></span><br><span class="line">    <span class="keyword">for</span> sim_user <span class="keyword">in</span> sim_users:</span><br><span class="line">        <span class="comment"># 构建初始的推荐结果</span></span><br><span class="line">        rs_result = rs_result.union(set(df.ix[sim_user].replace(<span class="number">0</span>,np.nan).dropna().index))</span><br><span class="line">    <span class="comment"># 过滤掉已经购买过的物品</span></span><br><span class="line">    rs_result -= set(df.ix[user].replace(<span class="number">0</span>,np.nan).dropna().index)</span><br><span class="line">    rs_results[user] = rs_result</span><br><span class="line">print(<span class="string">"最终推荐结果："</span>)</span><br><span class="line">pprint(rs_results)</span><br></pre></td></tr></table></figure><p>5.Item-Based CF </p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> pprint <span class="keyword">import</span> pprint</span><br><span class="line"></span><br><span class="line">users = [<span class="string">"User1"</span>, <span class="string">"User2"</span>, <span class="string">"User3"</span>, <span class="string">"User4"</span>, <span class="string">"User5"</span>]</span><br><span class="line">items = [<span class="string">"Item A"</span>, <span class="string">"Item B"</span>, <span class="string">"Item C"</span>, <span class="string">"Item D"</span>, <span class="string">"Item E"</span>]</span><br><span class="line"><span class="comment"># 用户购买记录数据集</span></span><br><span class="line">datasets = [</span><br><span class="line">    [<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>],</span><br><span class="line">    [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">    [<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>],</span><br><span class="line">    [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>],</span><br><span class="line">    [<span class="number">1</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>],</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(datasets,</span><br><span class="line">                  columns=items,</span><br><span class="line">                  index=users)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算所有的数据两两的杰卡德相似系数</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> pairwise_distances</span><br><span class="line"><span class="comment"># 计算物品间相似度</span></span><br><span class="line">item_similar = <span class="number">1</span> - pairwise_distances(df.T, metric=<span class="string">"jaccard"</span>)</span><br><span class="line">item_similar = pd.DataFrame(item_similar, columns=items, index=items)</span><br><span class="line">print(<span class="string">"物品之间的两两相似度："</span>)</span><br><span class="line">print(item_similar)</span><br><span class="line"></span><br><span class="line">topN_items = &#123;&#125;</span><br><span class="line"><span class="comment"># 遍历每一行数据</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> item_similar.index:</span><br><span class="line">    <span class="comment"># 取出每一列数据，并删除自身，然后排序数据</span></span><br><span class="line">    _df = item_similar.loc[i].drop([i])</span><br><span class="line">    _df_sorted = _df.sort_values(ascending=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    top2 = list(_df_sorted.index[:<span class="number">2</span>])</span><br><span class="line">    topN_items[i] = top2</span><br><span class="line"></span><br><span class="line">print(<span class="string">"Top2相似物品："</span>)</span><br><span class="line">pprint(topN_items)</span><br><span class="line"></span><br><span class="line">rs_results = &#123;&#125;</span><br><span class="line"><span class="comment"># 构建推荐结果</span></span><br><span class="line"><span class="keyword">for</span> user <span class="keyword">in</span> df.index:    <span class="comment"># 遍历所有用户</span></span><br><span class="line">    rs_result = set()</span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> df.ix[user].replace(<span class="number">0</span>,np.nan).dropna().index:   <span class="comment"># 取出每个用户当前已购物品列表</span></span><br><span class="line">        <span class="comment"># 根据每个物品找出最相似的TOP-N物品，构建初始推荐结果</span></span><br><span class="line">        rs_result = rs_result.union(topN_items[item])</span><br><span class="line">    <span class="comment"># 过滤掉用户已购的物品</span></span><br><span class="line">    rs_result -= set(df.ix[user].replace(<span class="number">0</span>,np.nan).dropna().index)</span><br><span class="line">    <span class="comment"># 添加到结果中</span></span><br><span class="line">    rs_results[user] = rs_result</span><br><span class="line"></span><br><span class="line">print(<span class="string">"最终推荐结果："</span>)</span><br><span class="line">pprint(rs_results)</span><br></pre></td></tr></table></figure><h4 id="关于协同过滤推荐算法使用的数据集"><a href="#关于协同过滤推荐算法使用的数据集" class="headerlink" title="关于协同过滤推荐算法使用的数据集"></a>关于协同过滤推荐算法使用的数据集</h4><p>在前面的demo中，我们只是使用用户对物品的一个购买记录，类似也可以是比如浏览点击记录、收听记录等等。这样数据我们预测的结果其实相当于是在预测用户是否对某物品感兴趣，对于喜好程度不能很好的预测。</p><p>因此在协同过滤推荐算法中其实会更多的利用用户对物品的“评分”数据来进行预测，通过评分数据集，我们可以预测用户对于他没有评分过的物品的评分。其实现原理和思想和都是一样的，只是使用的数据集是用户-物品的评分数据。</p><h5 id="关于用户-物品评分矩阵"><a href="#关于用户-物品评分矩阵" class="headerlink" title="关于用户-物品评分矩阵"></a>关于用户-物品评分矩阵</h5><p> 用户-物品的评分矩阵，根据评分矩阵的稀疏程度会有不同的解决方案 </p><ul><li><p><strong>稠密评分矩阵</strong> </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227154715.png"/> </p></li><li><p><strong>稀疏评分矩阵</strong> </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227154645.png"/> </p></li></ul><h4 id="使用协同过滤推荐算法对用户进行评分预测"><a href="#使用协同过滤推荐算法对用户进行评分预测" class="headerlink" title="使用协同过滤推荐算法对用户进行评分预测"></a>使用协同过滤推荐算法对用户进行评分预测</h4><ul><li><p>数据集 </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227154709.png"/></p><p> <strong>目的：预测用户1对物品E的评分</strong> </p></li><li><p>构建数据集：注意这里构建评分数据时，对于缺失的部分我们需要保留为None，如果设置为0那么会被当作评分值为0去对待</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">users = [<span class="string">"User1"</span>, <span class="string">"User2"</span>, <span class="string">"User3"</span>, <span class="string">"User4"</span>, <span class="string">"User5"</span>]</span><br><span class="line">items = [<span class="string">"Item A"</span>, <span class="string">"Item B"</span>, <span class="string">"Item C"</span>, <span class="string">"Item D"</span>, <span class="string">"Item E"</span>]</span><br><span class="line"><span class="comment"># 用户购买记录数据集</span></span><br><span class="line">datasets = [</span><br><span class="line">    [<span class="number">5</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">4</span>,<span class="literal">None</span>],</span><br><span class="line">    [<span class="number">3</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">3</span>],</span><br><span class="line">    [<span class="number">4</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">5</span>],</span><br><span class="line">    [<span class="number">3</span>,<span class="number">3</span>,<span class="number">1</span>,<span class="number">5</span>,<span class="number">4</span>],</span><br><span class="line">    [<span class="number">1</span>,<span class="number">5</span>,<span class="number">5</span>,<span class="number">2</span>,<span class="number">1</span>],</span><br><span class="line">]</span><br></pre></td></tr></table></figure></li><li><p>计算相似度：对于评分数据这里我们采用皮尔逊相关系数[-1,1]来计算，-1表示强负相关，+1表示强正相关</p><blockquote><p>pandas中corr方法可直接用于计算皮尔逊相关系数 </p></blockquote><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">df = pd.DataFrame(datasets,</span><br><span class="line">                  columns=items,</span><br><span class="line">                  index=users)</span><br><span class="line"></span><br><span class="line">print(<span class="string">"用户之间的两两相似度："</span>)</span><br><span class="line"><span class="comment"># 直接计算皮尔逊相关系数</span></span><br><span class="line"><span class="comment"># 默认是按列进行计算，因此如果计算用户间的相似度，当前需要进行转置</span></span><br><span class="line">user_similar = df.T.corr()</span><br><span class="line">print(user_similar.round(<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">print(<span class="string">"物品之间的两两相似度："</span>)</span><br><span class="line">item_similar = df.corr()</span><br><span class="line">print(item_similar.round(<span class="number">4</span>))</span><br></pre></td></tr></table></figure><figure class="highlight yaml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 运行结果：</span></span><br><span class="line"><span class="string">用户之间的两两相似度：</span></span><br><span class="line">        <span class="string">User1</span>   <span class="string">User2</span>   <span class="string">User3</span>   <span class="string">User4</span>   <span class="string">User5</span></span><br><span class="line"><span class="string">User1</span>  <span class="number">1.0000</span>  <span class="number">0.8528</span>  <span class="number">0.7071</span>  <span class="number">0.0000</span> <span class="number">-0.7921</span></span><br><span class="line"><span class="string">User2</span>  <span class="number">0.8528</span>  <span class="number">1.0000</span>  <span class="number">0.4677</span>  <span class="number">0.4900</span> <span class="number">-0.9001</span></span><br><span class="line"><span class="string">User3</span>  <span class="number">0.7071</span>  <span class="number">0.4677</span>  <span class="number">1.0000</span> <span class="number">-0.1612</span> <span class="number">-0.4666</span></span><br><span class="line"><span class="string">User4</span>  <span class="number">0.0000</span>  <span class="number">0.4900</span> <span class="number">-0.1612</span>  <span class="number">1.0000</span> <span class="number">-0.6415</span></span><br><span class="line"><span class="string">User5</span> <span class="number">-0.7921</span> <span class="number">-0.9001</span> <span class="number">-0.4666</span> <span class="number">-0.6415</span>  <span class="number">1.0000</span></span><br><span class="line"><span class="string">物品之间的两两相似度：</span></span><br><span class="line">        <span class="string">Item</span> <span class="string">A</span>  <span class="string">Item</span> <span class="string">B</span>  <span class="string">Item</span> <span class="string">C</span>  <span class="string">Item</span> <span class="string">D</span>  <span class="string">Item</span> <span class="string">E</span></span><br><span class="line"><span class="string">Item</span> <span class="string">A</span>  <span class="number">1.0000</span> <span class="number">-0.4767</span> <span class="number">-0.1231</span>  <span class="number">0.5322</span>  <span class="number">0.9695</span></span><br><span class="line"><span class="string">Item</span> <span class="string">B</span> <span class="number">-0.4767</span>  <span class="number">1.0000</span>  <span class="number">0.6455</span> <span class="number">-0.3101</span> <span class="number">-0.4781</span></span><br><span class="line"><span class="string">Item</span> <span class="string">C</span> <span class="number">-0.1231</span>  <span class="number">0.6455</span>  <span class="number">1.0000</span> <span class="number">-0.7206</span> <span class="number">-0.4276</span></span><br><span class="line"><span class="string">Item</span> <span class="string">D</span>  <span class="number">0.5322</span> <span class="number">-0.3101</span> <span class="number">-0.7206</span>  <span class="number">1.0000</span>  <span class="number">0.5817</span></span><br><span class="line"><span class="string">Item</span> <span class="string">E</span>  <span class="number">0.9695</span> <span class="number">-0.4781</span> <span class="number">-0.4276</span>  <span class="number">0.5817</span>  <span class="number">1.0000</span></span><br></pre></td></tr></table></figure><p>可以看到与用户1最相似的是用户2和用户3；与物品A最相似的物品分别是物品E和物品D。</p><p><strong>注意：</strong>我们在预测评分时，往往是通过与其有正相关的用户或物品进行预测，如果不存在正相关的情况，那么将无法做出预测。这一点尤其是在稀疏评分矩阵中尤为常见，因为稀疏评分矩阵中很难得出正相关系数。</p></li><li><p><strong>评分预测：</strong> </p><p><strong>User-Based CF 评分预测：使用用户间的相似度进行预测</strong> </p><p>关于评分预测的方法也有比较多的方案，下面介绍一种效果比较好的方案，该方案考虑了用户本身的评分评分以及近邻用户的加权平均相似度打分来进行预测： </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227155128.png"/></p><p>我们要预测用户1对物品E的评分，那么可以根据与用户1最近邻的用户2和用户3进行预测，计算如下：$pred(u_1,i_5)=(0.85∗3+0.71∗5)/(0.85+0.71)=3.91$最终预测出用户1对物品5的评分为3.91</p><p><strong>Item-Based CF 评分预测：使用物品间的相似度进行预测</strong> </p><p>这里利用物品相似度预测的计算同上，同样考虑了用户自身的平均打分因素，结合预测物品与相似物品的加权平均相似度打分进行来进行预测 </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img2/20200227155120.png"/></p><p>户1对物品E的评分，那么可以根据与物品E最近邻的物品A和物品D进行预测，计算如下： $pred(u1,i5)=(0.97∗5+0.58∗4)/(0.97+0.58)=4.63$ 对比可见，User-Based CF预测评分和Item-Based CF的评分结果也是存在差异的，因为严格意义上他们其实应当属于两种不同的推荐算法，各自在不同的领域不同场景下，都会比另一种的效果更佳，但具体哪一种更佳，必须经过合理的效果评估，因此在实现推荐系统时这两种算法往往都是需要去实现的，然后对产生的推荐效果进行评估分析选出更优方案。 </p></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;推荐模型构建流程&quot;&gt;&lt;a href=&quot;#推荐模型构建流程&quot; class=&quot;headerlink&quot; title=&quot;推荐模型构建流程&quot;&gt;&lt;/a&gt;推荐模型构建流程&lt;/h4&gt;&lt;blockquote&gt;
&lt;p&gt;Data(数据)-&amp;gt;Features(特征)-&amp;gt;ML Algorithm(选择算法训练模型)-&amp;gt;Prediction Output(预测输出) &lt;/p&gt;
&lt;/blockquote&gt;
    
    </summary>
    
    
      <category term="大数据推荐系统" scheme="https://xiaoliaozi.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="推荐模型构建流程" scheme="https://xiaoliaozi.com/tags/%E6%8E%A8%E8%8D%90%E6%A8%A1%E5%9E%8B%E6%9E%84%E5%BB%BA%E6%B5%81%E7%A8%8B/"/>
    
      <category term="推荐算法" scheme="https://xiaoliaozi.com/tags/%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
      <category term="协同过滤推荐算法" scheme="https://xiaoliaozi.com/tags/%E5%8D%8F%E5%90%8C%E8%BF%87%E6%BB%A4%E6%8E%A8%E8%8D%90%E7%AE%97%E6%B3%95/"/>
    
      <category term="相似度计算" scheme="https://xiaoliaozi.com/tags/%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97/"/>
    
  </entry>
  
  <entry>
    <title>推荐系统</title>
    <link href="https://xiaoliaozi.com/2020/01/15/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    <id>https://xiaoliaozi.com/2020/01/15/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/</id>
    <published>2020-01-15T01:33:02.000Z</published>
    <updated>2020-02-27T07:46:11.842Z</updated>
    
    <content type="html"><![CDATA[<h4 id="推荐系统概念"><a href="#推荐系统概念" class="headerlink" title="推荐系统概念"></a>推荐系统概念</h4><h5 id="什么是推荐系统"><a href="#什么是推荐系统" class="headerlink" title="什么是推荐系统"></a>什么是推荐系统</h5><p>没有明确需求的用户访问了我们的服务, 且服务的物品对用户构成了信息过载，系统通过一定的规则对物品进行排序，并将排在前面的物品展示给用户，这样的系统就是推荐系统 。</p><h5 id="信息过载-amp-用户需求不明确"><a href="#信息过载-amp-用户需求不明确" class="headerlink" title="信息过载 &amp; 用户需求不明确"></a>信息过载 &amp; 用户需求不明确</h5><ul><li>分类⽬录（1990s）：覆盖少量热门⽹站。典型应用：Hao123 Yahoo</li><li>搜索引擎（2000s）：通过搜索词明确需求。典型应用：Google Baidu</li><li>推荐系统（2010s）：不需要⽤户提供明确的需求，通过分析⽤ 户的历史⾏为给⽤户的兴趣进⾏建模，从⽽主动给⽤户推荐能 够满⾜他们兴趣和需求的信息。</li></ul><a id="more"></a><h5 id="推荐系统与搜索引擎"><a href="#推荐系统与搜索引擎" class="headerlink" title="推荐系统与搜索引擎"></a>推荐系统与搜索引擎</h5><div class="table-container"><table><thead><tr><th></th><th>搜索</th><th>推荐</th></tr></thead><tbody><tr><td>行为方式</td><td>主动</td><td>被动</td></tr><tr><td>意图</td><td>明确</td><td>模糊</td></tr><tr><td>个性化</td><td>弱</td><td>强</td></tr><tr><td>流量分布</td><td>马太效应</td><td>长尾效应</td></tr><tr><td>目标</td><td>快速满足</td><td>持续服务</td></tr><tr><td>评估指标</td><td>简明</td><td>复杂</td></tr></tbody></table></div><h4 id="推荐系统的工作原理"><a href="#推荐系统的工作原理" class="headerlink" title="推荐系统的工作原理"></a>推荐系统的工作原理</h4><ul><li><strong>社会化推荐</strong> 例如：向朋友咨询，社会化推荐，让好友给自己推荐物品</li><li><strong>基于内容的推荐</strong> 例如：打开搜索引擎，输入自己喜欢的演员的名字，然后看看返回结果中还有什么电影是自己没看过的</li><li><strong>基于流行度的推荐</strong> 例如：查看票房排行榜</li><li><strong>基于协同过滤的推荐</strong> 例如：找到和自己历史兴趣相似的用户，看看他们最近在看什么电影</li></ul><h4 id="推荐系统的作用"><a href="#推荐系统的作用" class="headerlink" title="推荐系统的作用"></a>推荐系统的作用</h4><ul><li>高效连接用户和物品</li><li>提高用户停留时间和用户活跃程度</li><li>有效的帮助产品实现其商业价值</li></ul><h4 id="推荐系统的应用场景"><a href="#推荐系统的应用场景" class="headerlink" title="推荐系统的应用场景"></a>推荐系统的应用场景</h4><ul><li>头条</li><li>淘宝京东</li><li>抖音</li></ul><h4 id="推荐系统和Web项目的区别"><a href="#推荐系统和Web项目的区别" class="headerlink" title="推荐系统和Web项目的区别"></a>推荐系统和Web项目的区别</h4><ul><li>通过信息过滤实现目标提升 V.S. 稳定的信息流通系统<ul><li>web项目: 处理复杂业务逻辑，处理高并发，为用户构建一个稳定的信息流通服务</li><li>推荐系统: 追求指标增长， 留存率/阅读时间/GMV (Gross Merchandise Volume电商网站成交金额)/视频网站VV (Video View)</li></ul></li><li>确定 V.S. 不确定思维<ul><li>web项目: 对结果有确定预期</li><li>推荐系统: 结果是概率问题</li></ul></li></ul><h4 id="推荐系统要素"><a href="#推荐系统要素" class="headerlink" title="推荐系统要素"></a>推荐系统要素</h4><ul><li>UI 和 UE(前端界面)</li><li>数据 (Lambda架构)</li><li>业务知识</li><li>算法</li></ul><h4 id="推荐系统架构"><a href="#推荐系统架构" class="headerlink" title="推荐系统架构"></a>推荐系统架构</h4><h5 id="推荐系统整体架构"><a href="#推荐系统整体架构" class="headerlink" title="推荐系统整体架构"></a>推荐系统整体架构</h5><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227154506.png"/></p><h5 id="大数据Lambda架构"><a href="#大数据Lambda架构" class="headerlink" title="大数据Lambda架构"></a>大数据Lambda架构</h5><ul><li>Lambda架构是由实时大数据处理框架Storm的作者Nathan Marz提出的一个实时大数据处理框架。</li><li>Lambda架构的将离线计算和实时计算整合，设计出一个能满足实时大数据系统关键特性的架构，包括有：高容错、低延时和可扩展等。</li><li>分层架构<ul><li>批处理层（离线计算）<ul><li>数据不可变, 可进行任何计算, 可水平扩展</li><li>高延迟 几分钟~几小时(计算量和数据量不同)</li><li>日志收集： Flume</li><li>分布式存储： Hadoop</li><li>分布式计算： Hadoop、Spark</li><li>视图存储数据库<ul><li>nosql(HBase/Cassandra)</li><li>Redis/memcache</li><li>MySQL</li></ul></li></ul></li><li>实时处理层<ul><li>流式处理, 持续计算</li><li>存储和分析某个窗口期内的数据（一段时间的热销排行，实时热搜等）</li><li>实时数据收集 flume-日志收集 &amp; kafka-消息队列（数据的实时收集）</li><li>实时数据分析 spark streaming/storm/flink</li></ul></li><li>服务层<ul><li>支持随机读</li><li>需要在非常短的时间内返回结果</li><li>读取批处理层和实时处理层结果并对其归并</li></ul></li></ul></li><li><p>Lambda架构图</p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227154322.png"/></p></li></ul><h5 id="推荐算法架构"><a href="#推荐算法架构" class="headerlink" title="推荐算法架构"></a>推荐算法架构</h5><ul><li>召回阶段 (海选)<ul><li>召回决定了最终推荐结果的天花板</li><li>常用算法:<ul><li>协同过滤</li><li>基于内容</li></ul></li></ul></li><li>排序阶段 （精选）<ul><li>召回决定了最终推荐结果的天花板, 排序逼近这个极限, 决定了最终的推荐效果</li><li>CTR预估 (点击率预估 使用LR算法) 估计用户是否会点击某个商品 需要用户的点击数据</li></ul></li><li>过滤<ul><li>法律规则</li><li>没有库存问题</li><li>反复曝光都没看</li></ul></li><li><p>规则调整</p><ul><li>商业合作</li><li>政策问题</li></ul><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227154421.jpeg"/></p></li></ul><h5 id="推荐系统的整体架构"><a href="#推荐系统的整体架构" class="headerlink" title="推荐系统的整体架构"></a>推荐系统的整体架构</h5><p> 推荐系统业务架构</p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227154452.png"/></p><p>  推荐系统技术架构</p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227154427.png"/> </p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;推荐系统概念&quot;&gt;&lt;a href=&quot;#推荐系统概念&quot; class=&quot;headerlink&quot; title=&quot;推荐系统概念&quot;&gt;&lt;/a&gt;推荐系统概念&lt;/h4&gt;&lt;h5 id=&quot;什么是推荐系统&quot;&gt;&lt;a href=&quot;#什么是推荐系统&quot; class=&quot;headerlink&quot; title=&quot;什么是推荐系统&quot;&gt;&lt;/a&gt;什么是推荐系统&lt;/h5&gt;&lt;p&gt;没有明确需求的用户访问了我们的服务, 且服务的物品对用户构成了信息过载，系统通过一定的规则对物品进行排序，并将排在前面的物品展示给用户，这样的系统就是推荐系统 。&lt;/p&gt;
&lt;h5 id=&quot;信息过载-amp-用户需求不明确&quot;&gt;&lt;a href=&quot;#信息过载-amp-用户需求不明确&quot; class=&quot;headerlink&quot; title=&quot;信息过载 &amp;amp; 用户需求不明确&quot;&gt;&lt;/a&gt;信息过载 &amp;amp; 用户需求不明确&lt;/h5&gt;&lt;ul&gt;
&lt;li&gt;分类⽬录（1990s）：覆盖少量热门⽹站。典型应用：Hao123 Yahoo&lt;/li&gt;
&lt;li&gt;搜索引擎（2000s）：通过搜索词明确需求。典型应用：Google Baidu&lt;/li&gt;
&lt;li&gt;推荐系统（2010s）：不需要⽤户提供明确的需求，通过分析⽤ 户的历史⾏为给⽤户的兴趣进⾏建模，从⽽主动给⽤户推荐能 够满⾜他们兴趣和需求的信息。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="大数据推荐系统" scheme="https://xiaoliaozi.com/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F/"/>
    
    
      <category term="推荐系统简介" scheme="https://xiaoliaozi.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E7%AE%80%E4%BB%8B/"/>
    
      <category term="推荐系统架构" scheme="https://xiaoliaozi.com/tags/%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/"/>
    
  </entry>
  
  <entry>
    <title>五大常用算法</title>
    <link href="https://xiaoliaozi.com/2020/01/13/%E4%BA%94%E5%A4%A7%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95/"/>
    <id>https://xiaoliaozi.com/2020/01/13/%E4%BA%94%E5%A4%A7%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95/</id>
    <published>2020-01-13T06:40:47.000Z</published>
    <updated>2020-01-14T06:30:47.546Z</updated>
    
    <content type="html"><![CDATA[<h4 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h4><p>据说有人归纳了计算机的五大常用算法，它们是贪婪算法，动态规划算法，分治算法，回溯算法以及分支限界算法。这五个算法是有很多应用场景的，最优化问题大多可以利用这些算法解决。算法的本质就是解决问题。当数据量比较小时，其实根本就不需要什么算法，写一些for循环完全就可以很快速的搞定了，但是当数据量比较大，场景比较复杂的时候，算法就尤为重要了，本文先归纳这几个算法及应用场景，随后在细细品味。</p><a id="more"></a><h4 id="穷举法"><a href="#穷举法" class="headerlink" title="穷举法"></a>穷举法</h4><h5 id="1-定义"><a href="#1-定义" class="headerlink" title="1.定义"></a>1.定义</h5><p>穷举法也叫枚举法， 在进行归纳推理时，如果逐个考察了某类事件的所有可能情况，因而得出一般结论，那么这结论是可靠的，这种归纳方法叫做枚举法。枚举法是利用计算机运算速度快、精确度高的特点，对要解决问题的所有可能情况，一个不漏地进行检验，从中找出符合要求的答案，因此枚举法是通过牺牲时间来换取答案的全面性 。穷举法属于暴力破解法， 暴力破解法，就是把所有条件，相关情况统统考虑进去，让计算机进行检索，指导得出与之所有条件符合的结果 。</p><h5 id="2-基本思想"><a href="#2-基本思想" class="headerlink" title="2.基本思想"></a>2.基本思想</h5><ol><li>确定枚举对象、枚举范围和判定条件</li><li>枚举可能的解，验证是否是问题的解</li></ol><h5 id="3-应用实例"><a href="#3-应用实例" class="headerlink" title="3.应用实例"></a>3.应用实例</h5><ol><li>百钱买鸡问题</li><li>鸡兔同笼问题</li><li>搬砖块问题</li><li>猜数字</li><li>韩信点兵 </li></ol><h4 id="贪婪算法"><a href="#贪婪算法" class="headerlink" title="贪婪算法"></a>贪婪算法</h4><h5 id="1-定义-1"><a href="#1-定义-1" class="headerlink" title="1.定义"></a>1.定义</h5><p>贪婪算法(贪心算法)是指在对问题进行求解时，在每一步选择中都采取最好或者最优(即最有利)的选择，从而希望能够导致结果是最好或者最优的算法。贪婪算法所得到的结果往往不是最优的结果(有时候会是最优解)，但是都是相对近似(接近)最优解的结果。</p><h5 id="2-基本思想-1"><a href="#2-基本思想-1" class="headerlink" title="2.基本思想"></a>2.基本思想</h5><ol><li><p>建立数学模型来描述问题</p></li><li><p>把求解的问题分成若干个子问题</p></li><li><p>对每一子问题求解，得到子问题的局部最优解</p></li><li><p>把子问题对应的局部最优解合成原来整个问题的一个近似最优解</p></li></ol><h5 id="3-应用实例-1"><a href="#3-应用实例-1" class="headerlink" title="3.应用实例"></a>3.应用实例</h5><ol><li>钱币找零问题 </li><li>区间调度问题</li><li>背包问题 </li><li>均分纸牌 </li><li>最大整数</li></ol><h4 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h4><h5 id="1-定义-2"><a href="#1-定义-2" class="headerlink" title="1.定义"></a>1.定义</h5><p> 动态规划过程是：每次决策依赖于当前状态，又随即引起状态的转移。一个决策序列就是在变化的状态中产生出来的，所以，这种多阶段最优化决策解决问题的过程就称为动态规划。 </p><h5 id="2-基本思想-2"><a href="#2-基本思想-2" class="headerlink" title="2.基本思想"></a>2.基本思想</h5><ol><li>将待求解的问题分解为若干个子问题（阶段）</li><li>按顺序求解子阶段，前一子问题的解，为后一子问题的求解提供了有用的信息</li><li>在求解任一子问题时，列出各种可能的局部解，通过决策保留那些有可能达到最优的局部解，丢弃其他局部解</li><li>依次解决各子问题，最后一个子问题就是初始问题的解。 </li></ol><h5 id="3-应用实例-2"><a href="#3-应用实例-2" class="headerlink" title="3.应用实例"></a>3.应用实例</h5><ol><li>数字三角形问题</li><li>找零钱问题</li><li>走方格问题</li><li>最长公共序列数</li></ol><h4 id="分治算法"><a href="#分治算法" class="headerlink" title="分治算法"></a>分治算法</h4><h5 id="1-定义-3"><a href="#1-定义-3" class="headerlink" title="1.定义"></a>1.定义</h5><p> 分治算法的基本思想是将一个规模为N的问题分解为K个规模较小的子问题，这些子问题相互独立且与原问题性质相同。求出子问题的解，就可得到原问题的解。即一种分目标完成程序算法，简单问题可用二分法完成。 </p><h5 id="2-基本思想-3"><a href="#2-基本思想-3" class="headerlink" title="2.基本思想"></a>2.基本思想</h5><ol><li>先把问题分解成几个子问题</li><li>求出这几个子问题的解法</li><li>再找到合适的方法，把它们组合成求整个问题的解法。</li><li>如果这些子问题还较大，难以解决，可以再把它们分成几个更小的子问题</li><li>以此类推，直至可以直接求出解为止</li></ol><h5 id="3-应用实例-3"><a href="#3-应用实例-3" class="headerlink" title="3.应用实例"></a>3.应用实例</h5><ol><li>找出伪币</li><li>二分搜索</li><li>汉诺塔</li><li>归并排序</li><li>快速排序</li><li>大整数乘法</li></ol><h4 id="回溯算法"><a href="#回溯算法" class="headerlink" title="回溯算法"></a>回溯算法</h4><h5 id="1-定义-4"><a href="#1-定义-4" class="headerlink" title="1.定义"></a>1.定义</h5><p> 回溯算法实际上一个类似枚举的搜索尝试过程，主要是在搜索尝试过程中寻找问题的解，当发现已不满足求解条件时，就“回溯”返回，尝试别的路径。回溯法是一种选优搜索法，按选优条件向前搜索，以达到目标。但当探索到某一步时，发现原先选择并不优或达不到目标，就退回一步重新选择，这种走不通就退回再走的技术为回溯法，而满足回溯条件的某个状态的点称为“回溯点”。许多复杂的，规模较大的问题都可以使用回溯法，有“通用解题方法”的美称。 </p><h5 id="2-基本思想-4"><a href="#2-基本思想-4" class="headerlink" title="2.基本思想"></a>2.基本思想</h5><p> 从一条路往前走，能进则进，不能进则退回来，换一条路再试 </p><h5 id="3-应用实例-4"><a href="#3-应用实例-4" class="headerlink" title="3.应用实例"></a>3.应用实例</h5><ol><li>八皇后问题</li><li>图的着色问题 </li><li>装载问题 </li><li>批处理作业调度问题 </li><li>背包问题 </li><li>最大团问题 </li></ol><h4 id="分支限界算法"><a href="#分支限界算法" class="headerlink" title="分支限界算法"></a>分支限界算法</h4><h5 id="1-定义-5"><a href="#1-定义-5" class="headerlink" title="1.定义"></a>1.定义</h5><p>分支限界算法是按照广度优先的方式对解空间树（状态空间树）进行搜索，从而求得最优解的算法。</p><h5 id="2-基本思想-5"><a href="#2-基本思想-5" class="headerlink" title="2.基本思想"></a>2.基本思想</h5><p>在搜索的过程中，采用<strong>限界函数</strong>（bound function）估算所有子节点的目标函数的可能取值，从而选择使目标函数取极值（极大值或者极小值）的节点作为扩展结点（如果限界值没有超过目前的最优解，则剪枝）进行下一步搜索（重复 BFS -&gt; 计算所有子节点限界 -&gt; 选择最优子节点作为扩展结点的过程），从而不断调整搜索的方向，尽快找到问题的最优解。分支限界的思想类似于：图的广度优先搜索，树的层序遍历。</p><h5 id="3-应用实例-5"><a href="#3-应用实例-5" class="headerlink" title="3.应用实例"></a>3.应用实例</h5><ol><li>单源最短路径问题</li><li>装载问题</li><li>布线问题</li><li>0-1背包问题</li><li>最大团问题</li><li>旅行售货员问题</li></ol><h4 id="总结说明"><a href="#总结说明" class="headerlink" title="总结说明"></a>总结说明</h4><p>对于一个应用实例可能会有多种算法解决，算法是一种解决问题的思想，任意一个算法绝对不是一两篇文章可以讲清楚的。当然也不是通过一两道题目可以完全学会。学习算法的关键是<strong>用算法的思想去想问题，去解决实际问题</strong>，多刷题是养成算法思维解决问题的基础。</p><h4 id="学习算法方法"><a href="#学习算法方法" class="headerlink" title="学习算法方法"></a>学习算法方法</h4><h5 id="1-书籍"><a href="#1-书籍" class="headerlink" title="1.书籍"></a>1.书籍</h5><ol><li>数据结构</li><li>数据结构与算法分析 </li><li>算法导论</li></ol><h5 id="2-刷题"><a href="#2-刷题" class="headerlink" title="2.刷题"></a>2.刷题</h5><ol><li>牛客</li><li>LeeCode</li></ol><h5 id="3-在线视频课程"><a href="#3-在线视频课程" class="headerlink" title="3.在线视频课程"></a>3.在线视频课程</h5><ol><li>慕课</li><li>网易云课程（强烈推荐）</li></ol><h5 id="4-可视化工具"><a href="#4-可视化工具" class="headerlink" title="4.可视化工具"></a>4.可视化工具</h5><ul><li><a href="https://visualgo.net/" target="_blank" rel="noopener">visualgo 网址 </a> </li></ul><p>最重要的是耐心，自律，有毅力，坚持。</p><p>作为才入门的程序员，我已经沉浸在知识海洋中无法自拔，深深感受到了自己的渺小。</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;前言&quot;&gt;&lt;a href=&quot;#前言&quot; class=&quot;headerlink&quot; title=&quot;前言&quot;&gt;&lt;/a&gt;前言&lt;/h4&gt;&lt;p&gt;据说有人归纳了计算机的五大常用算法，它们是贪婪算法，动态规划算法，分治算法，回溯算法以及分支限界算法。这五个算法是有很多应用场景的，最优化问题大多可以利用这些算法解决。算法的本质就是解决问题。当数据量比较小时，其实根本就不需要什么算法，写一些for循环完全就可以很快速的搞定了，但是当数据量比较大，场景比较复杂的时候，算法就尤为重要了，本文先归纳这几个算法及应用场景，随后在细细品味。&lt;/p&gt;
    
    </summary>
    
    
      <category term="计算机常用算法" scheme="https://xiaoliaozi.com/categories/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="计算机常用算法" scheme="https://xiaoliaozi.com/tags/%E8%AE%A1%E7%AE%97%E6%9C%BA%E5%B8%B8%E7%94%A8%E7%AE%97%E6%B3%95/"/>
    
      <category term="算法思想" scheme="https://xiaoliaozi.com/tags/%E7%AE%97%E6%B3%95%E6%80%9D%E6%83%B3/"/>
    
  </entry>
  
  <entry>
    <title>集成学习</title>
    <link href="https://xiaoliaozi.com/2020/01/13/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    <id>https://xiaoliaozi.com/2020/01/13/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/</id>
    <published>2020-01-13T03:11:47.000Z</published>
    <updated>2020-02-27T07:39:20.703Z</updated>
    
    <content type="html"><![CDATA[<h4 id="什么是集成学习"><a href="#什么是集成学习" class="headerlink" title="什么是集成学习"></a>什么是集成学习</h4><p> 集成学习通过建立几个模型来解决单一预测问题。它的工作原理是<strong>生成多个分类器/模型</strong>，各自独立地学习和作出预测。<strong>这些预测最后结合成组合预测，因此优于任何一个单分类的做出预测。</strong> </p><a id="more"></a><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227143703.png"/></p><h4 id="机器学习的两个核心任务"><a href="#机器学习的两个核心任务" class="headerlink" title="机器学习的两个核心任务"></a>机器学习的两个核心任务</h4><ul><li>任务一：<strong>如何优化训练数据</strong> —&gt; 主要用于<strong>解决欠拟合问题</strong></li><li>任务二：<strong>如何提升泛化性能</strong> —&gt; 主要用于<strong>解决过拟合问题</strong></li></ul><h4 id="集成学习中boosting和Bagging"><a href="#集成学习中boosting和Bagging" class="headerlink" title="集成学习中boosting和Bagging"></a>集成学习中boosting和Bagging</h4><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227143653.png"/> </p><p><strong>只要单分类器的表现不太差，集成学习的结果总是要好于单分类器的</strong></p><h4 id="Bagging集成原理"><a href="#Bagging集成原理" class="headerlink" title="Bagging集成原理"></a>Bagging集成原理</h4><p> 目标：把下面的圈和方块进行分类 </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227153011.png"/> </p><p>实现过程：</p><p>1.采样不同数据集</p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227152903.png"/></p><p> 2.训练分类器 </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227152657.png"/> </p><ol><li><p>平权投票，获取最终结果 </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227152648.png"/></p><p>4.主要实现过程小结 </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227152622.png"/> </p></li></ol><h4 id="随机森林构造过程"><a href="#随机森林构造过程" class="headerlink" title="随机森林构造过程"></a>随机森林构造过程</h4><p>在机器学习中，<strong>随机森林是一个包含多个决策树的分类器</strong>，并且其输出的类别是由个别树输出的类别的众数而定。</p><p><strong>随机森林</strong> <strong>= Bagging +</strong> <strong>决策树</strong></p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227143311.png"/></p><h4 id="随机森林api介绍"><a href="#随机森林api介绍" class="headerlink" title="随机森林api介绍"></a>随机森林api介绍</h4><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">sklearn.ensemble.RandomForestClassifier(<span class="attribute">n_estimators</span>=10, <span class="attribute">criterion</span>=’gini’, <span class="attribute">max_depth</span>=None, <span class="attribute">bootstrap</span>=<span class="literal">True</span>, <span class="attribute">random_state</span>=None, <span class="attribute">min_samples_split</span>=2)</span><br><span class="line">n_estimators：integer，optional（default = 10）森林里的树木数量120,200,300,500,800,1200</span><br><span class="line">Criterion：string，可选（default =“gini”）分割特征的测量方法</span><br><span class="line">max_depth：integer或None，可选（默认=无）树的最大深度 5,8,15,25,30</span><br><span class="line"><span class="attribute">max_features</span>=<span class="string">"auto”,每个决策树的最大特征数量</span></span><br><span class="line"><span class="string">If "</span>auto", then <span class="attribute">max_features</span>=sqrt(n_features).</span><br><span class="line"><span class="keyword">If</span> <span class="string">"sqrt"</span>, then <span class="attribute">max_features</span>=sqrt(n_features)(same as <span class="string">"auto"</span>).</span><br><span class="line"><span class="keyword">If</span> <span class="string">"log2"</span>, then <span class="attribute">max_features</span>=log2(n_features).</span><br><span class="line"><span class="keyword">If</span> None, then <span class="attribute">max_features</span>=n_features.</span><br><span class="line">bootstrap：boolean，optional（default = <span class="literal">True</span>）是否在构建树时使用放回抽样</span><br><span class="line">min_samples_split:节点划分最少样本数</span><br><span class="line">min_samples_leaf:叶子节点的最小样本数</span><br><span class="line">超参数：n_estimator, max_depth, min_samples_split,min_samples_leaf</span><br></pre></td></tr></table></figure><h4 id="随机森林预测案例"><a href="#随机森林预测案例" class="headerlink" title="随机森林预测案例"></a>随机森林预测案例</h4><ul><li>实例化随机森林</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 随机森林去进行预测</span></span><br><span class="line">rf = RandomForestClassifier()</span><br></pre></td></tr></table></figure><ul><li>定义超参数的选择列表</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">param = &#123;<span class="string">"n_estimators"</span>: [<span class="number">120</span>,<span class="number">200</span>,<span class="number">300</span>,<span class="number">500</span>,<span class="number">800</span>,<span class="number">1200</span>], <span class="string">"max_depth"</span>: [<span class="number">5</span>, <span class="number">8</span>, <span class="number">15</span>, <span class="number">25</span>, <span class="number">30</span>]&#125;</span><br></pre></td></tr></table></figure><ul><li>使用GridSearchCV进行网格搜索</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 超参数调优</span></span><br><span class="line">gc = GridSearchCV(rf, param_grid=param, cv=<span class="number">2</span>)</span><br><span class="line">gc.fit(x_train, y_train)</span><br><span class="line">print(<span class="string">"随机森林预测的准确率为："</span>, gc.score(x_test, y_test))</span><br></pre></td></tr></table></figure><blockquote><p>注意</p><ul><li>随机森林的建立过程</li><li>树的深度、树的个数等需要进行超参数调优</li></ul></blockquote><h4 id="bagging集成优点"><a href="#bagging集成优点" class="headerlink" title="bagging集成优点"></a>bagging集成优点</h4><p><strong>Bagging + 决策树/线性回归/逻辑回归/深度学习… = bagging集成学习方法</strong></p><p>经过上面方式组成的集成学习方法:</p><ol><li><strong>均可在原有算法上提高约2%左右的泛化正确率</strong></li><li><strong>简单, 方便, 通用</strong></li></ol><h4 id="boosting集成原理"><a href="#boosting集成原理" class="headerlink" title="boosting集成原理"></a>boosting集成原理</h4><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227152607.png"/> </p><p><strong>随着学习的积累从弱到强</strong></p><p><strong>简而言之：每新加入一个弱学习器，整体能力就会得到提升</strong></p><p>代表算法：Adaboost，GBDT，XGBoost</p><h4 id="boosting实现过程"><a href="#boosting实现过程" class="headerlink" title="boosting实现过程"></a>boosting实现过程</h4><p>1.<strong>训练第一个学习器</strong></p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227152557.png"/></p><p> 2.<strong>调整数据分布</strong></p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227143933.png"/></p><p> 3.<strong>训练第二个学习器</strong></p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227152614.png"/></p><p> 4.<strong>再次调整数据分布</strong></p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227143839.png"/></p><p> 5.<strong>依次训练学习器，调整数据分布</strong></p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227143802.png"/></p><p> 6.<strong>整体过程实现</strong></p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227143754.png"/> </p><blockquote><p>关键点：<br>如何确认投票权重？<br>如何调整数据分布？</p></blockquote><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227143743.png"/></p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227143717.png"/>  </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227143710.png"/> </p><h4 id="bagging与boosting比较"><a href="#bagging与boosting比较" class="headerlink" title="bagging与boosting比较"></a>bagging与boosting比较</h4><blockquote><p>区别一：数据方面</p><p>​    Bagging：对数据进行采样训练；</p><p>​    Boosting：根据前一轮学习结果调整数据的重要性。</p><p>区别二：投票方面</p><p>​    Bagging：所有学习器平权投票；</p><p>​    Boosting：对学习器进行加权投票。</p><p>区别三：学习顺序</p><p>​    Bagging的学习是并行的，每个学习器没有依赖关系；</p><p>​    Boosting学习是串行，学习有先后顺序。</p><p>区别四：主要作用</p><p>​    Bagging主要用于提高泛化性能（解决过拟合，也可以说降低方差）</p><p>​    Boosting主要用于提高训练精度 （解决欠拟合，也可以说降低偏差）</p></blockquote><h4 id="boostingAPI介绍"><a href="#boostingAPI介绍" class="headerlink" title="boostingAPI介绍"></a>boostingAPI介绍</h4><figure class="highlight elixir"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">from sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line">api链接<span class="symbol">:https</span><span class="symbol">://scikit-learn</span>.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html<span class="comment">#sklearn.ensemble.AdaBoostClassifier</span></span><br></pre></td></tr></table></figure><h4 id="梯度提升决策树-GBDT"><a href="#梯度提升决策树-GBDT" class="headerlink" title="梯度提升决策树 GBDT"></a>梯度提升决策树 GBDT</h4><h5 id="1-GBDT定义"><a href="#1-GBDT定义" class="headerlink" title="1.GBDT定义"></a>1.GBDT定义</h5><p>梯度提升决策树(GBDT Gradient Boosting Decision Tree) <strong>是一种迭代的决策树算法</strong>，<strong>该算法由多棵决策树组成，所有树的结论累加起来做最终答案。</strong>它在被提出之初就被认为是泛化能力（generalization)较强的算法。近些年更因为被用于搜索排序的机器学习模型而引起大家关注。</p><p><strong>GBDT = 梯度下降 + Boosting + 决策树</strong></p><h5 id="2-GBDT执行流程"><a href="#2-GBDT执行流程" class="headerlink" title="2.GBDT执行流程"></a>2.GBDT执行流程</h5><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227143646.png"/></p><p>如果上式中的$h_i(x)=$决策树模型,则上式就变为:</p><p><strong>GBDT = 梯度下降 + Boosting + 决策树</strong></p><h5 id="3-GBDT案例"><a href="#3-GBDT案例" class="headerlink" title="3.GBDT案例"></a>3.GBDT案例</h5><p>预测编号5的身高：<br>| 编号 | 年龄(岁) | 体重(KG) | 身高(M) |<br>| —— | ———— | ———— | ———- |<br>| 1    | 5        | 20       | 1.1     |<br>| 2    | 7        | 30       | 1.3     |<br>| 3    | 21       | 70       | 1.7     |<br>| 4    | 30       | 60       | 1.8     |<br>| 5    | 25       | 65       | ?       |</p><p>第一步：计算损失函数,并求出第一个预测值:</p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227143637.png"/> </p><p> 第二步：求解划分点 </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227143623.png"/></p><p> 得出:年龄21为划分点的方差=0.01+0.0025=0.0125</p><p>第三步：通过调整后目标值,求解得出h1(x)</p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227143605.png"/></p><p>  第四步：求解h2(x) </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227143346.png"/></p><p> 得出结果:</p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227143327.png"/> 编号5身高 = 1.475 + 0.03 + 0.275 = 1.78</p><h5 id="4-GBDT主要执行思想"><a href="#4-GBDT主要执行思想" class="headerlink" title="4.GBDT主要执行思想"></a>4.GBDT主要执行思想</h5><p>1.使用梯度下降法优化代价函数；</p><p>2.使用一层决策树作为弱学习器，负梯度作为目标值；</p><p>3.利用boosting思想进行集成。</p><h4 id="XGBoost"><a href="#XGBoost" class="headerlink" title="XGBoost"></a>XGBoost</h4><p> <strong>XGBoost= 二阶泰勒展开+boosting+决策树+正则化</strong> </p><p><strong>Boosting</strong>：XGBoost使用Boosting提升思想对多个弱学习器进行迭代式学习</p><p><strong>二阶泰勒展开</strong>：每一轮学习中，XGBoost对损失函数进行二阶泰勒展开，使用一阶和二阶梯度进行优化。</p><p><strong>决策树</strong>：在每一轮学习中，XGBoost使用决策树算法作为弱学习进行优化。</p><p><strong>正则化</strong>：在优化过程中XGBoost为防止过拟合，在损失函数中加入惩罚项，限制决策树的叶子节点个数以及决策树叶子节点的值。</p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img1/20200227143320.png"/> </p><p>泰勒展开越多，计算结果越精确</p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;什么是集成学习&quot;&gt;&lt;a href=&quot;#什么是集成学习&quot; class=&quot;headerlink&quot; title=&quot;什么是集成学习&quot;&gt;&lt;/a&gt;什么是集成学习&lt;/h4&gt;&lt;p&gt; 集成学习通过建立几个模型来解决单一预测问题。它的工作原理是&lt;strong&gt;生成多个分类器/模型&lt;/strong&gt;，各自独立地学习和作出预测。&lt;strong&gt;这些预测最后结合成组合预测，因此优于任何一个单分类的做出预测。&lt;/strong&gt; &lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习算法" scheme="https://xiaoliaozi.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="机器学习算法" scheme="https://xiaoliaozi.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
      <category term="机器学习" scheme="https://xiaoliaozi.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="监督学习算法" scheme="https://xiaoliaozi.com/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
      <category term="集成学习" scheme="https://xiaoliaozi.com/tags/%E9%9B%86%E6%88%90%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="欠拟合过拟合" scheme="https://xiaoliaozi.com/tags/%E6%AC%A0%E6%8B%9F%E5%90%88%E8%BF%87%E6%8B%9F%E5%90%88/"/>
    
      <category term="bagging集成" scheme="https://xiaoliaozi.com/tags/bagging%E9%9B%86%E6%88%90/"/>
    
      <category term="boosting集成" scheme="https://xiaoliaozi.com/tags/boosting%E9%9B%86%E6%88%90/"/>
    
  </entry>
  
  <entry>
    <title>特征工程-特征提取</title>
    <link href="https://xiaoliaozi.com/2020/01/11/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"/>
    <id>https://xiaoliaozi.com/2020/01/11/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B-%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/</id>
    <published>2020-01-11T02:40:27.000Z</published>
    <updated>2020-02-22T10:01:30.394Z</updated>
    
    <content type="html"><![CDATA[<h4 id="特征提取"><a href="#特征提取" class="headerlink" title="特征提取"></a>特征提取</h4><ul><li><p>定义</p><p><strong>将任意数据（如文本或图像）转换为可用于机器学习的数字特征</strong></p><blockquote><p>注：特征值化是为了计算机更好的去理解数据</p></blockquote><ul><li>特征提取分类:<ul><li>字典特征提取(特征离散化)</li><li>文本特征提取</li><li>图像特征提取（深度学习将介绍）</li></ul></li></ul></li></ul><a id="more"></a><ul><li><p>特征提取API</p><figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">sklearn</span><span class="selector-class">.feature_extraction</span></span><br></pre></td></tr></table></figure></li></ul><h4 id="字典特征提取"><a href="#字典特征提取" class="headerlink" title="字典特征提取"></a>字典特征提取</h4><p><strong>作用：对字典数据进行特征值化</strong></p><ul><li>sklearn.feature_extraction.DictVectorizer(sparse=True,…)<ul><li>DictVectorizer.fit_transform(X)<ul><li>X：字典或者包含字典的迭代器返回值</li><li>返回sparse矩阵</li></ul></li><li>DictVectorizer.get_feature_names() 返回类别名称</li></ul></li></ul><p><strong>应用：</strong></p><ul><li>实例化类DictVectorizer</li><li>调用fit_transform方法输入数据并转换（注意返回格式）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dict_demo</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    对字典类型的数据进行特征抽取</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    data = [&#123;<span class="string">'city'</span>: <span class="string">'北京'</span>,<span class="string">'temperature'</span>:<span class="number">100</span>&#125;, </span><br><span class="line">            &#123;<span class="string">'city'</span>: <span class="string">'上海'</span>,<span class="string">'temperature'</span>:<span class="number">60</span>&#125;,</span><br><span class="line">            &#123;<span class="string">'city'</span>: <span class="string">'深圳'</span>,<span class="string">'temperature'</span>:<span class="number">30</span>&#125;]</span><br><span class="line">    <span class="comment"># 1、实例化一个转换器类</span></span><br><span class="line">    transfer = DictVectorizer(sparse=<span class="literal">False</span>)</span><br><span class="line">    <span class="comment"># 2、调用fit_transform</span></span><br><span class="line">    data = transfer.fit_transform(data)</span><br><span class="line">    print(<span class="string">"返回的结果:\n"</span>, data)</span><br><span class="line">    <span class="comment"># 打印特征名字</span></span><br><span class="line">    print(<span class="string">"特征名字：\n"</span>, transfer.get_feature_names())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> Nonepython</span><br></pre></td></tr></table></figure><p> 注意观察没有加上sparse=False参数的结果 </p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">返回的结果:</span><br><span class="line">   (<span class="number">0</span>, <span class="number">1</span>)    <span class="number">1.0</span></span><br><span class="line">  (<span class="number">0</span>, <span class="number">3</span>)    <span class="number">100.0</span></span><br><span class="line">  (<span class="number">1</span>, <span class="number">0</span>)    <span class="number">1.0</span></span><br><span class="line">  (<span class="number">1</span>, <span class="number">3</span>)    <span class="number">60.0</span></span><br><span class="line">  (<span class="number">2</span>, <span class="number">2</span>)    <span class="number">1.0</span></span><br><span class="line">  (<span class="number">2</span>, <span class="number">3</span>)    <span class="number">30.0</span></span><br><span class="line">特征名字：</span><br><span class="line"> [<span class="string">'city=上海'</span>, <span class="string">'city=北京'</span>, <span class="string">'city=深圳'</span>, <span class="string">'temperature'</span>]</span><br></pre></td></tr></table></figure><p> 这个结果并不是我们想要看到的，所以加上参数，得到想要的结果： </p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">返回的结果:</span><br><span class="line"> [[   <span class="number">0.</span>    <span class="number">1.</span>    <span class="number">0.</span>  <span class="number">100.</span>]</span><br><span class="line"> [   <span class="number">1.</span>    <span class="number">0.</span>    <span class="number">0.</span>   <span class="number">60.</span>]</span><br><span class="line"> [   <span class="number">0.</span>    <span class="number">0.</span>    <span class="number">1.</span>   <span class="number">30.</span>]]</span><br><span class="line">特征名字：</span><br><span class="line"> [<span class="string">'city=上海'</span>, <span class="string">'city=北京'</span>, <span class="string">'city=深圳'</span>, <span class="string">'temperature'</span>]</span><br></pre></td></tr></table></figure><p>之前在学习pandas中的离散化的时候，也实现了类似的效果。我们把这个处理数据的技巧叫做”one-hot“编码</p><p> <strong>对于特征当中存在类别信息的我们都会做one-hot编码处理</strong> 。</p><h4 id="文本特征提取"><a href="#文本特征提取" class="headerlink" title="文本特征提取"></a>文本特征提取</h4><p><strong>作用：对文本数据进行特征值化</strong></p><ul><li><strong>sklearn.feature_extraction.text.CountVectorizer(stop_words=[])</strong><ul><li>返回词频矩阵</li><li>CountVectorizer.fit_transform(X)<ul><li>X:文本或者包含文本字符串的可迭代对象</li><li>返回值:返回sparse矩阵</li></ul></li><li>CountVectorizer.get_feature_names() 返回值:单词列表</li></ul></li><li><strong>sklearn.feature_extraction.text.TfidfVectorizer</strong></li></ul><p><strong>应用：</strong></p><ul><li>实例化类CountVectorizer</li><li>调用fit_transform方法输入数据并转换 （注意返回格式，利用toarray()进行sparse矩阵转换array数组）</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">text_count_demo</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    对文本进行特征抽取，countvetorizer</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    data = [<span class="string">"life is short,i like like python"</span>, <span class="string">"life is too long,i dislike python"</span>]</span><br><span class="line">    <span class="comment"># 1、实例化一个转换器类</span></span><br><span class="line">    <span class="comment"># transfer = CountVectorizer(sparse=False) # 注意,没有sparse这个参数</span></span><br><span class="line">    transfer = CountVectorizer()</span><br><span class="line">    <span class="comment"># 2、调用fit_transform</span></span><br><span class="line">    data = transfer.fit_transform(data)</span><br><span class="line">    print(<span class="string">"文本特征抽取的结果：\n"</span>, data.toarray())</span><br><span class="line">    print(<span class="string">"返回特征名字：\n"</span>, transfer.get_feature_names())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure><p>返回结果：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">文本特征抽取的结果：</span><br><span class="line"> [[<span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">2</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span>]]</span><br><span class="line">返回特征名字：</span><br><span class="line"> [<span class="string">'dislike'</span>, <span class="string">'is'</span>, <span class="string">'life'</span>, <span class="string">'like'</span>, <span class="string">'long'</span>, <span class="string">'python'</span>, <span class="string">'short'</span>, <span class="string">'too'</span>]</span><br></pre></td></tr></table></figure><p>不支持单个中文字，中文未分词，所以我们要对中文进行分词处理 </p><h4 id="jieba分词处理"><a href="#jieba分词处理" class="headerlink" title="jieba分词处理"></a>jieba分词处理</h4><ul><li>jieba.cut()：返回词语组成的生成器</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 需要安装下jieba库</span></span><br><span class="line">pip3 install jieba</span><br></pre></td></tr></table></figure><p><strong>案例</strong>：</p><ul><li>准备句子，利用jieba.cut进行分词</li><li>实例化CountVectorizer</li><li>将分词结果变成字符串当作fit_transform的输入值</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cut_word</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    对中文进行分词</span></span><br><span class="line"><span class="string">    "我爱北京天安门"————&gt;"我 爱 北京 天安门"</span></span><br><span class="line"><span class="string">    :param text:</span></span><br><span class="line"><span class="string">    :return: text</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 用结巴对中文字符串进行分词</span></span><br><span class="line">    text = <span class="string">" "</span>.join(list(jieba.cut(text)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">text_chinese_count_demo2</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    对中文进行特征抽取</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    data = [<span class="string">"一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。"</span>,</span><br><span class="line">            <span class="string">"我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。"</span>,</span><br><span class="line">            <span class="string">"如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"</span>]</span><br><span class="line">    <span class="comment"># 将原始数据转换成分好词的形式</span></span><br><span class="line">    text_list = []</span><br><span class="line">    <span class="keyword">for</span> sent <span class="keyword">in</span> data:</span><br><span class="line">        text_list.append(cut_word(sent))</span><br><span class="line">    print(text_list)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1、实例化一个转换器类</span></span><br><span class="line">    <span class="comment"># transfer = CountVectorizer(sparse=False)</span></span><br><span class="line">    transfer = CountVectorizer()</span><br><span class="line">    <span class="comment"># 2、调用fit_transform</span></span><br><span class="line">    data = transfer.fit_transform(text_list)</span><br><span class="line">    print(<span class="string">"文本特征抽取的结果：\n"</span>, data.toarray())</span><br><span class="line">    print(<span class="string">"返回特征名字：\n"</span>, transfer.get_feature_names())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure><p> 返回结果： </p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Building prefix dict <span class="keyword">from</span> the <span class="keyword">default</span> <span class="built_in">dictionary</span> ...</span><br><span class="line">Dumping model to file cache /var/folders/mz/tzf2l3sx4rgg6qpglfb035_r0000gn/T/jieba.cache</span><br><span class="line">Loading model cost <span class="number">1.032</span> seconds.</span><br><span class="line">[<span class="string">'一种 还是 一种 今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。'</span>, <span class="string">'我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。'</span>, <span class="string">'如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。'</span>]</span><br><span class="line">Prefix dict has been built succesfully.</span><br><span class="line">文本特征抽取的结果：</span><br><span class="line"> [[<span class="number">2</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">2</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">2</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span>]</span><br><span class="line"> [<span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">3</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">2</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span>]</span><br><span class="line"> [<span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">4</span> <span class="number">3</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">1</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span> <span class="number">2</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">1</span> <span class="number">0</span> <span class="number">0</span> <span class="number">0</span>]]</span><br><span class="line">返回特征名字：</span><br><span class="line"> [<span class="string">'一种'</span>, <span class="string">'不会'</span>, <span class="string">'不要'</span>, <span class="string">'之前'</span>, <span class="string">'了解'</span>, <span class="string">'事物'</span>, <span class="string">'今天'</span>, <span class="string">'光是在'</span>, <span class="string">'几百万年'</span>, <span class="string">'发出'</span>, <span class="string">'取决于'</span>, <span class="string">'只用'</span>, <span class="string">'后天'</span>, <span class="string">'含义'</span>, <span class="string">'大部分'</span>, <span class="string">'如何'</span>, <span class="string">'如果'</span>, <span class="string">'宇宙'</span>, <span class="string">'我们'</span>, <span class="string">'所以'</span>, <span class="string">'放弃'</span>, <span class="string">'方式'</span>, <span class="string">'明天'</span>, <span class="string">'星系'</span>, <span class="string">'晚上'</span>, <span class="string">'某样'</span>, <span class="string">'残酷'</span>, <span class="string">'每个'</span>, <span class="string">'看到'</span>, <span class="string">'真正'</span>, <span class="string">'秘密'</span>, <span class="string">'绝对'</span>, <span class="string">'美好'</span>, <span class="string">'联系'</span>, <span class="string">'过去'</span>, <span class="string">'还是'</span>, <span class="string">'这样'</span>]</span><br></pre></td></tr></table></figure><h4 id="Tf-idf文本特征提取"><a href="#Tf-idf文本特征提取" class="headerlink" title="Tf-idf文本特征提取"></a>Tf-idf文本特征提取</h4><ul><li>TF-IDF的主要思想是：如果<strong>某个词或短语在一篇文章中出现的概率高，并且在其他文章中很少出现</strong>，则认为此词或者短语具有很好的类别区分能力，能代表文章的主题，适合用来分类。</li><li><strong>TF-IDF作用：用以评估一字词对于一个文件集或一个语料库中的其中一份文件的重要程度。</strong>提取文章的标签（主题）。</li></ul><h5 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h5><ul><li>词频（term frequency，tf）指的是某一个给定的词语在该文件中出现的频率</li><li>逆向文档频率（inverse document frequency，idf）是一个词语普遍重要性的度量。某一特定词语的idf，可以<strong>由总文件数目除以包含该词语之文件的数目，再将得到的商取以10为底的对数得到</strong></li></ul><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200222180055.png"/></p><p> 最终得出结果可以理解为重要程度。 </p><p><strong>tfidf越大越能代表文章的主题</strong></p><figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">举例：</span><br><span class="line">假如一篇文章的总词语数是<span class="number">100</span>个，而词语<span class="string">"非常"</span>出现了<span class="number">5</span>次，那么<span class="string">"非常"</span>一词在该文件中的词频就是<span class="number">5</span>/<span class="number">100</span>=<span class="number">0.05</span>。</span><br><span class="line">而计算文件频率（IDF）的方法是以文件集的文件总数，除以出现<span class="string">"非常"</span>一词的文件数。</span><br><span class="line">所以，如果<span class="string">"非常"</span>一词在<span class="number">1</span>,<span class="number">0000</span>份文件出现过，而文件总数是<span class="number">10</span>,<span class="number">000</span>,<span class="number">000</span>份的话，</span><br><span class="line">其逆向文件频率就是lg（<span class="number">10</span>,<span class="number">000</span>,<span class="number">000</span> / <span class="number">1</span>,<span class="number">0000</span>）=<span class="number">3</span>。</span><br><span class="line">最后<span class="string">"非常"</span>对于这篇文档的tf-idf的分数为<span class="number">0.05</span> * <span class="number">3</span>=<span class="number">0.15</span></span><br></pre></td></tr></table></figure><h5 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">cut_word</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    对中文进行分词</span></span><br><span class="line"><span class="string">    "我爱北京天安门"————&gt;"我 爱 北京 天安门"</span></span><br><span class="line"><span class="string">    :param text:</span></span><br><span class="line"><span class="string">    :return: text</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 用结巴对中文字符串进行分词</span></span><br><span class="line">    text = <span class="string">" "</span>.join(list(jieba.cut(text)))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> text</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">text_chinese_tfidf_demo</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    对中文进行特征抽取</span></span><br><span class="line"><span class="string">    :return: None</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    data = [<span class="string">"一种还是一种今天很残酷，明天更残酷，后天很美好，但绝对大部分是死在明天晚上，所以每个人不要放弃今天。"</span>,</span><br><span class="line">            <span class="string">"我们看到的从很远星系来的光是在几百万年之前发出的，这样当我们看到宇宙时，我们是在看它的过去。"</span>,</span><br><span class="line">            <span class="string">"如果只用一种方式了解某样事物，你就不会真正了解它。了解事物真正含义的秘密取决于如何将其与我们所了解的事物相联系。"</span>]</span><br><span class="line">    <span class="comment"># 将原始数据转换成分好词的形式</span></span><br><span class="line">    text_list = []</span><br><span class="line">    <span class="keyword">for</span> sent <span class="keyword">in</span> data:</span><br><span class="line">        text_list.append(cut_word(sent))</span><br><span class="line">    print(text_list)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 1、实例化一个转换器类</span></span><br><span class="line">    <span class="comment"># transfer = CountVectorizer(sparse=False)</span></span><br><span class="line">    <span class="comment"># 剔除'一种', '不会', '不要'这些词没有说明代表意义，减少计算</span></span><br><span class="line">    transfer = TfidfVectorizer(stop_words=[<span class="string">'一种'</span>, <span class="string">'不会'</span>, <span class="string">'不要'</span>])</span><br><span class="line">    <span class="comment"># 2、调用fit_transform</span></span><br><span class="line">    data = transfer.fit_transform(text_list)</span><br><span class="line">    print(<span class="string">"文本特征抽取的结果：\n"</span>, data.toarray())</span><br><span class="line">    print(<span class="string">"返回特征名字：\n"</span>, transfer.get_feature_names())</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></table></figure><p>结果：</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">Building prefix dict <span class="keyword">from</span> the <span class="keyword">default</span> <span class="built_in">dictionary</span> ...</span><br><span class="line">Loading model <span class="keyword">from</span> cache /var/folders/mz/tzf2l3sx4rgg6qpglfb035_r0000gn/T/jieba.cache</span><br><span class="line">Loading model cost <span class="number">0.856</span> seconds.</span><br><span class="line">Prefix dict has been built succesfully.</span><br><span class="line">[<span class="string">'一种 还是 一种 今天 很 残酷 ， 明天 更 残酷 ， 后天 很 美好 ， 但 绝对 大部分 是 死 在 明天 晚上 ， 所以 每个 人 不要 放弃 今天 。'</span>, <span class="string">'我们 看到 的 从 很 远 星系 来 的 光是在 几百万年 之前 发出 的 ， 这样 当 我们 看到 宇宙 时 ， 我们 是 在 看 它 的 过去 。'</span>, <span class="string">'如果 只用 一种 方式 了解 某样 事物 ， 你 就 不会 真正 了解 它 。 了解 事物 真正 含义 的 秘密 取决于 如何 将 其 与 我们 所 了解 的 事物 相 联系 。'</span>]</span><br><span class="line">文本特征抽取的结果：</span><br><span class="line"> [[ <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.43643578</span>  <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span></span><br><span class="line">   <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.21821789</span>  <span class="number">0.</span>          <span class="number">0.21821789</span>  <span class="number">0.</span>          <span class="number">0.</span></span><br><span class="line">   <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.21821789</span>  <span class="number">0.21821789</span>  <span class="number">0.</span>          <span class="number">0.43643578</span></span><br><span class="line">   <span class="number">0.</span>          <span class="number">0.21821789</span>  <span class="number">0.</span>          <span class="number">0.43643578</span>  <span class="number">0.21821789</span>  <span class="number">0.</span>          <span class="number">0.</span></span><br><span class="line">   <span class="number">0.</span>          <span class="number">0.21821789</span>  <span class="number">0.21821789</span>  <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.21821789</span></span><br><span class="line">   <span class="number">0.</span>        ]</span><br><span class="line"> [ <span class="number">0.2410822</span>   <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.2410822</span>   <span class="number">0.2410822</span></span><br><span class="line">   <span class="number">0.2410822</span>   <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span></span><br><span class="line">   <span class="number">0.</span>          <span class="number">0.2410822</span>   <span class="number">0.55004769</span>  <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span></span><br><span class="line">   <span class="number">0.2410822</span>   <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.48216441</span></span><br><span class="line">   <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.2410822</span></span><br><span class="line">   <span class="number">0.</span>          <span class="number">0.2410822</span> ]</span><br><span class="line"> [ <span class="number">0.</span>          <span class="number">0.644003</span>    <span class="number">0.48300225</span>  <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span></span><br><span class="line">   <span class="number">0.16100075</span>  <span class="number">0.16100075</span>  <span class="number">0.</span>          <span class="number">0.16100075</span>  <span class="number">0.</span>          <span class="number">0.16100075</span></span><br><span class="line">   <span class="number">0.16100075</span>  <span class="number">0.</span>          <span class="number">0.12244522</span>  <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.16100075</span></span><br><span class="line">   <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.16100075</span>  <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.</span></span><br><span class="line">   <span class="number">0.3220015</span>   <span class="number">0.16100075</span>  <span class="number">0.</span>          <span class="number">0.</span>          <span class="number">0.16100075</span>  <span class="number">0.</span>          <span class="number">0.</span></span><br><span class="line">   <span class="number">0.</span>        ]]</span><br><span class="line">返回特征名字：</span><br><span class="line">[<span class="string">'之前'</span>, <span class="string">'了解'</span>, <span class="string">'事物'</span>, <span class="string">'今天'</span>, <span class="string">'光是在'</span>, <span class="string">'几百万年'</span>, <span class="string">'发出'</span>, <span class="string">'取决于'</span>, <span class="string">'只用'</span>, <span class="string">'后天'</span>, <span class="string">'含义'</span>, <span class="string">'大部分'</span>, <span class="string">'如何'</span>, <span class="string">'如果'</span>, <span class="string">'宇宙'</span>, <span class="string">'我们'</span>, <span class="string">'所以'</span>, <span class="string">'放弃'</span>, <span class="string">'方式'</span>, <span class="string">'明天'</span>, <span class="string">'星系'</span>, <span class="string">'晚上'</span>, <span class="string">'某样'</span>, <span class="string">'残酷'</span>, <span class="string">'每个'</span>, <span class="string">'看到'</span>, <span class="string">'真正'</span>, <span class="string">'秘密'</span>, <span class="string">'绝对'</span>, <span class="string">'美好'</span>, <span class="string">'联系'</span>, <span class="string">'过去'</span>, <span class="string">'还是'</span>, <span class="string">'这样'</span>]</span><br></pre></td></tr></table></figure><h5 id="Tf-idf的重要性"><a href="#Tf-idf的重要性" class="headerlink" title="Tf-idf的重要性"></a>Tf-idf的重要性</h5><p> <strong>分类机器学习算法进行文章分类中前期数据处理方式</strong> </p><p>举例：</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">文章库中  <span class="number">1000</span>篇文章</span><br><span class="line">的  在 <span class="number">1000</span> 篇文章中都出现过</span><br><span class="line">python  在<span class="number">100</span> 文章中出现过</span><br><span class="line">java       在 <span class="number">100</span> 文章中出现过</span><br><span class="line"></span><br><span class="line">idf(的)  = lg(<span class="number">1000</span>/<span class="number">1000</span>)  = <span class="number">0</span></span><br><span class="line">idf(python) = lg(<span class="number">1000</span>/<span class="number">100</span>) = <span class="number">1</span></span><br><span class="line">idf(java) = lg(<span class="number">1000</span>/<span class="number">100</span>) = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">文章<span class="number">1</span> python  出现了 <span class="number">100</span>次，java也出现了<span class="number">10</span>次， 文章<span class="number">1</span>中一共有一千个词</span><br><span class="line">文章<span class="number">2</span> python  出现了 <span class="number">10</span>次，java也出现了<span class="number">100</span>次， 文章<span class="number">2</span>中一共有一千个词</span><br><span class="line"></span><br><span class="line">tf(python , 文章<span class="number">1</span>) = <span class="number">100</span> /<span class="number">1000</span> = <span class="number">0.1</span></span><br><span class="line">tf(java, 文章<span class="number">1</span>) = <span class="number">10</span>/<span class="number">1000</span> = <span class="number">0.01</span></span><br><span class="line">tf(python , 文章<span class="number">2</span>) = <span class="number">10</span> /<span class="number">1000</span> = <span class="number">00.1</span></span><br><span class="line">tf(java, 文章<span class="number">2</span>) = <span class="number">100</span>/<span class="number">1000</span> = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line">tfidf(python , 文章<span class="number">1</span>) = <span class="number">0.1</span> * <span class="number">1</span>  = <span class="number">0.1</span></span><br><span class="line">tfidf(java, 文章<span class="number">1</span>) = <span class="number">0.01</span></span><br><span class="line">tfidf(的, 文章<span class="number">1</span>) = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">tfidf(python , 文章<span class="number">2</span>)  = <span class="number">00.1</span></span><br><span class="line">tfidf(java, 文章<span class="number">2</span>)  = <span class="number">0.1</span></span><br><span class="line">tfidf(的, 文章<span class="number">2</span>) = <span class="number">0</span></span><br><span class="line">相对而言，Java代表文章<span class="number">2</span>的主题，python代表文章<span class="number">1</span>的主题</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;特征提取&quot;&gt;&lt;a href=&quot;#特征提取&quot; class=&quot;headerlink&quot; title=&quot;特征提取&quot;&gt;&lt;/a&gt;特征提取&lt;/h4&gt;&lt;ul&gt;
&lt;li&gt;&lt;p&gt;定义&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;将任意数据（如文本或图像）转换为可用于机器学习的数字特征&lt;/strong&gt;&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;注：特征值化是为了计算机更好的去理解数据&lt;/p&gt;
&lt;/blockquote&gt;
&lt;ul&gt;
&lt;li&gt;特征提取分类:&lt;ul&gt;
&lt;li&gt;字典特征提取(特征离散化)&lt;/li&gt;
&lt;li&gt;文本特征提取&lt;/li&gt;
&lt;li&gt;图像特征提取（深度学习将介绍）&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="机器学习" scheme="https://xiaoliaozi.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="机器学习" scheme="https://xiaoliaozi.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="特征工程" scheme="https://xiaoliaozi.com/tags/%E7%89%B9%E5%BE%81%E5%B7%A5%E7%A8%8B/"/>
    
      <category term="特征提取" scheme="https://xiaoliaozi.com/tags/%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96/"/>
    
      <category term="jieba分词处理" scheme="https://xiaoliaozi.com/tags/jieba%E5%88%86%E8%AF%8D%E5%A4%84%E7%90%86/"/>
    
      <category term="词袋模型" scheme="https://xiaoliaozi.com/tags/%E8%AF%8D%E8%A2%8B%E6%A8%A1%E5%9E%8B/"/>
    
  </entry>
  
  <entry>
    <title>决策树算法案例</title>
    <link href="https://xiaoliaozi.com/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E6%A1%88%E4%BE%8B/"/>
    <id>https://xiaoliaozi.com/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91%E7%AE%97%E6%B3%95%E6%A1%88%E4%BE%8B/</id>
    <published>2020-01-10T12:58:33.000Z</published>
    <updated>2020-02-27T02:12:16.558Z</updated>
    
    <content type="html"><![CDATA[<h4 id="决策树算法API"><a href="#决策树算法API" class="headerlink" title="决策树算法API"></a>决策树算法API</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">sklearn</span>.<span class="title">tree</span>.<span class="title">DecisionTreeClassifier</span><span class="params">(criterion=’gini’, max_depth=None,random_state=None)</span></span></span><br></pre></td></tr></table></figure><a id="more"></a><ul><li>criterion</li><li>特征选择标准<ul><li>“gini”或者”entropy”，前者代表基尼系数，后者代表信息增益。一默认”gini”，即CART算法。</li></ul></li><li>min_samples_split</li><li>内部节点再划分所需最小样本数<ul><li>这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2.如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。我之前的一个项目例子，有大概10万样本，建立决策树时，我选择了min_samples_split=10。可以作为参考。</li></ul></li><li>min_samples_leaf</li><li>叶子节点最少样本数<ul><li>这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起被剪枝。 默认是1，可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。之前的10万样本项目使用min_samples_leaf的值为5，仅供参考。</li></ul></li><li>max_depth</li><li>决策树最大深度<ul><li>决策树的最大深度，默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，具体的取值取决于数据的分布。常用的可以取值10-100之间</li></ul></li><li>random_state</li><li>随机数种子</li></ul><h4 id="案例-泰坦尼克号乘客生存预测"><a href="#案例-泰坦尼克号乘客生存预测" class="headerlink" title="案例-泰坦尼克号乘客生存预测"></a>案例-泰坦尼克号乘客生存预测</h4><h5 id="案例背景"><a href="#案例背景" class="headerlink" title="案例背景"></a>案例背景</h5><p>泰坦尼克号沉没是历史上最臭名昭着的沉船之一。1912年4月15日，在她的处女航中，泰坦尼克号在与冰山相撞后沉没，在2224名乘客和机组人员中造成1502人死亡。这场耸人听闻的悲剧震惊了国际社会，并为船舶制定了更好的安全规定。 造成海难失事的原因之一是乘客和机组人员没有足够的救生艇。尽管幸存下沉有一些运气因素，但有些人比其他人更容易生存，例如妇女，儿童和上流社会。 在这个案例中，我们要求您完成对哪些人可能存活的分析。特别是，我们要求您运用机器学习工具来预测哪些乘客幸免于悲剧。</p><p>案例：<a href="https://www.kaggle.com/c/titanic/overview" target="_blank" rel="noopener">https://www.kaggle.com/c/titanic/overview</a></p><p>我们提取到的数据集中的特征包括票的类别，是否存活，乘坐班次，年龄，登陆home.dest，房间，船和性别等。</p><p>数据：<a href="http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt" target="_blank" rel="noopener">http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt</a></p><p>经过观察数据得到:</p><ul><li><strong>1 乘坐班是指乘客班（1，2，3），是社会经济阶层的代表。</strong></li><li><strong>2 其中age数据存在缺失。</strong></li></ul><h5 id="步骤分析"><a href="#步骤分析" class="headerlink" title="步骤分析"></a>步骤分析</h5><ul><li><p>1.获取数据</p></li><li><p>2.数据基本处理</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">2.1</span> 确定特征值,目标值</span><br><span class="line"><span class="number">2.2</span> 缺失值处理</span><br><span class="line"><span class="number">2.3</span> 数据集划分</span><br></pre></td></tr></table></figure></li><li><p>3.特征工程(字典特征抽取)</p></li><li><p>4.机器学习(决策树)</p></li><li><p>5.模型评估</p></li></ul><h5 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入模块</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction <span class="keyword">import</span> DictVectorizer</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier, export_graphviz</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1.获取数据</span></span><br><span class="line">taitan = pd.read_csv(<span class="string">"http://biostat.mc.vanderbilt.edu/wiki/pub/Main/DataSets/titanic.txt"</span>)</span><br><span class="line">taitan.describe()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.数据基本处理</span></span><br><span class="line"><span class="comment"># 2.1 确定特征值,目标值</span></span><br><span class="line">x = taitan[[<span class="string">"pclass"</span>, <span class="string">"age"</span>, <span class="string">"sex"</span>]]</span><br><span class="line">y = taitan[<span class="string">"survived"</span>]</span><br><span class="line"><span class="comment"># 2.2 缺失值处理</span></span><br><span class="line"><span class="comment"># inplace:True:会修改原数据，False:不替换修改原数据，生成新的对象</span></span><br><span class="line">x[<span class="string">'age'</span>].fillna(x[<span class="string">'age'</span>].mean(), inplace=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 2.3 数据集划分</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(x, y, random_state=<span class="number">22</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3.特征工程(字典特征抽取)</span></span><br><span class="line"><span class="comment"># 特征中出现类别符号，需要进行one-hot编码处理(DictVectorizer)</span></span><br><span class="line"><span class="comment"># x.to_dict(orient="records") 需要将数组特征转换成字典数据</span></span><br><span class="line"><span class="comment"># 对于x转换成字典数据x.to_dict(orient="records")</span></span><br><span class="line">transfer = DictVectorizer(sparse=<span class="literal">False</span>)</span><br><span class="line">x_train = transfer.fit_transform(x_train.to_dict(orient=<span class="string">"records"</span>))</span><br><span class="line">x_test = transfer.fit_transform(x_test.to_dict(orient=<span class="string">"records"</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4.机器学习(决策树)</span></span><br><span class="line">estimator = DecisionTreeClassifier(criterion=<span class="string">"entropy"</span>, max_depth=<span class="number">5</span>)</span><br><span class="line">estimator.fit(x_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5.模型评估</span></span><br><span class="line"><span class="comment"># 精确率</span></span><br><span class="line">estimator.score(x_test, y_test)</span><br><span class="line"><span class="comment"># 预测值</span></span><br><span class="line">estimator.predict(x_test)</span><br></pre></td></tr></table></figure><h4 id="决策树可视化"><a href="#决策树可视化" class="headerlink" title="决策树可视化"></a>决策树可视化</h4><h5 id="保存树的结构到dot文件"><a href="#保存树的结构到dot文件" class="headerlink" title="保存树的结构到dot文件"></a>保存树的结构到dot文件</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sklearn.tree.export_graphviz() 该函数能够导出DOT格式</span></span><br><span class="line"><span class="comment"># tree.export_graphviz(estimator,out_file='tree.dot’,feature_names=[‘’,’’])</span></span><br><span class="line"></span><br><span class="line">export_graphviz(estimator, out_file=<span class="string">"./data/tree.dot"</span>, feature_names=[<span class="string">'age'</span>, <span class="string">'pclass=1st'</span>, <span class="string">'pclass=2nd'</span>, <span class="string">'pclass=3rd'</span>, <span class="string">'女性'</span>, <span class="string">'男性'</span>])</span><br></pre></td></tr></table></figure><h5 id="网站显示结构"><a href="#网站显示结构" class="headerlink" title="网站显示结构"></a>网站显示结构</h5><figure class="highlight dts"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">http:</span><span class="comment">//webgraphviz.com/</span></span><br></pre></td></tr></table></figure><h5 id="结果显示"><a href="#结果显示" class="headerlink" title="结果显示"></a>结果显示</h5><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227093538.png"/></p><h5 id="决策树优缺点"><a href="#决策树优缺点" class="headerlink" title="决策树优缺点"></a>决策树优缺点</h5><ul><li>优点：简单的理解和解释，树木可视化。</li><li>缺点：<strong>决策树学习者可以创建不能很好地推广数据的过于复杂的树，容易发生过拟合。</strong></li><li>改进：<ul><li>剪枝cart算法</li><li><strong>随机森林</strong>（集成学习的一种）</li></ul></li></ul><p><strong>注：企业重要决策，由于决策树很好的分析能力，在决策过程应用较多， 可以选择特征</strong></p>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;决策树算法API&quot;&gt;&lt;a href=&quot;#决策树算法API&quot; class=&quot;headerlink&quot; title=&quot;决策树算法API&quot;&gt;&lt;/a&gt;决策树算法API&lt;/h4&gt;&lt;figure class=&quot;highlight python&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;&lt;span class=&quot;class&quot;&gt;&lt;span class=&quot;keyword&quot;&gt;class&lt;/span&gt; &lt;span class=&quot;title&quot;&gt;sklearn&lt;/span&gt;.&lt;span class=&quot;title&quot;&gt;tree&lt;/span&gt;.&lt;span class=&quot;title&quot;&gt;DecisionTreeClassifier&lt;/span&gt;&lt;span class=&quot;params&quot;&gt;(criterion=’gini’, max_depth=None,random_state=None)&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
    
      <category term="机器学习算法" scheme="https://xiaoliaozi.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="机器学习算法" scheme="https://xiaoliaozi.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
      <category term="机器学习" scheme="https://xiaoliaozi.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="决策树可视化" scheme="https://xiaoliaozi.com/tags/%E5%86%B3%E7%AD%96%E6%A0%91%E5%8F%AF%E8%A7%86%E5%8C%96/"/>
    
      <category term="监督学习算法" scheme="https://xiaoliaozi.com/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
  </entry>
  
  <entry>
    <title>决策树算法</title>
    <link href="https://xiaoliaozi.com/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/"/>
    <id>https://xiaoliaozi.com/2020/01/10/%E5%86%B3%E7%AD%96%E6%A0%91/</id>
    <published>2020-01-10T01:34:04.000Z</published>
    <updated>2020-02-27T02:11:48.935Z</updated>
    
    <content type="html"><![CDATA[<h4 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h4><p>决策树思想的来源非常朴素，程序设计中的条件分支结构就是if-else结构，最早的决策树就是利用这类结构分割数据的一种分类学习方法。</p><p><strong>决策树：是一种树形结构，其中每个内部节点表示一个属性上的判断，每个分支代表一个判断结果的输出，最后每个叶节点代表一种分类结果，本质是一颗由多个判断节点组成的树</strong>。</p><a id="more"></a><h4 id="熵的概念"><a href="#熵的概念" class="headerlink" title="熵的概念"></a>熵的概念</h4><p> 物理学上，<strong>熵 Entropy</strong> 是“混乱”程度的量度。</p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227093747.png"/> </p><p> <strong>系统越有序，熵值越低；系统越混乱或者分散，熵值越高</strong>。 </p><ul><li><h5 id="信息理论"><a href="#信息理论" class="headerlink" title="信息理论"></a>信息理论</h5><ol><li><p><strong>从信息的完整性上进行的描述：</strong></p><p> 当<strong>系统的有序状态一致时</strong>，数据越集中的地方熵值越小，数据越分散的地方熵值越大。 </p></li><li><p><strong>从信息的有序性上进行的描述：</strong></p><p>  当<strong>数据量一致时</strong>，<strong>系统越有序，熵值越低；系统越混乱或者分散，熵值越高</strong>。 </p></li></ol></li></ul><p>“<strong>信息熵</strong>“ (information entropy)是度量样本集合纯度最常用的一种指标。</p><p>假定当前样本集合 D 中第 k 类样本所占的比例为$p_k(k = 1, 2,. . . , |y|)$ ，</p><p>$p_k=\frac{C^k}{D}$, D为样本的所有数量，$C^k$为第$k$类样本的数量。</p><p>则 D的信息熵定义为(（log是以2为底）:</p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227093959.png"/></p><p> 其中：Ent(D) 的值越小，则 D 的纯度越高. </p><ul><li><h5 id="举例说明"><a href="#举例说明" class="headerlink" title="举例说明"></a>举例说明</h5></li></ul><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">假设我们没有看世界杯的比赛，但是想知道哪支球队会是冠军，</span><br><span class="line">我们只能猜测某支球队是或不是冠军，然后观众用对或不对来回答，</span><br><span class="line">我们想要猜测次数尽可能少，你会用什么方法？</span><br><span class="line"></span><br><span class="line">答案：</span><br><span class="line">二分法：</span><br><span class="line">假如有 <span class="number">16</span> 支球队，分别编号，先问是否在 <span class="number">1</span><span class="number">-8</span> 之间，如果是就继续问是否在 <span class="number">1</span><span class="number">-4</span> 之间，</span><br><span class="line">以此类推，直到最后判断出冠军球队是哪支。</span><br><span class="line">如果球队数量是 <span class="number">16</span>，我们需要问 <span class="number">4</span> 次来得到最后的答案。那么世界冠军这条消息的信息熵就是 <span class="number">4</span>。</span><br><span class="line"></span><br><span class="line">那么信息熵等于<span class="number">4</span>，是如何进行计算的呢？</span><br><span class="line">Ent(D) = -（p1 * logp1 + p2 * logp2 + ... + p16 * logp16），</span><br><span class="line">其中 p1, ..., p16 分别是这 <span class="number">16</span> 支球队夺冠的概率。</span><br><span class="line">当每支球队夺冠概率相等都是 <span class="number">1</span>/<span class="number">16</span> 的时：Ent(D) = -（<span class="number">16</span> * <span class="number">1</span>/<span class="number">16</span> * log1/<span class="number">16</span>） = <span class="number">4</span></span><br><span class="line">每个事件概率相同时，熵最大，这件事越不确定。</span><br><span class="line"></span><br><span class="line">篮球比赛里，有<span class="number">4</span>个球队 &#123;A,B,C,D&#125; ，获胜概率分别为&#123;<span class="number">1</span>/<span class="number">2</span>, <span class="number">1</span>/<span class="number">4</span>, <span class="number">1</span>/<span class="number">8</span>, <span class="number">1</span>/<span class="number">8</span>&#125;，求Ent(D)</span><br><span class="line">Ent(D)=<span class="number">7</span>/<span class="number">4</span></span><br></pre></td></tr></table></figure><h4 id="决策树的划分依据"><a href="#决策树的划分依据" class="headerlink" title="决策树的划分依据"></a>决策树的划分依据</h4><h5 id="1-信息增益"><a href="#1-信息增益" class="headerlink" title="1.信息增益"></a>1.信息增益</h5><p> <strong>信息增益：</strong>以某特征划分数据集前后的熵的差值。熵可以表示样本集合的不确定性，熵越大，样本的不确定性就越大。因此可以<strong>使用划分前后集合熵的差值来衡量使用当前特征对于样本集合D划分效果的好坏</strong>。 </p><p> <strong>信息增益 = entroy(前) - entroy(后)</strong> </p><figure class="highlight tp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">注意：信息增益表示得知特征<span class="keyword">X</span>的信息而使得类<span class="keyword">Y</span>的信息熵减少的程度</span><br></pre></td></tr></table></figure><ul><li><p>定义与公式</p><p>假定离散属性a有 V 个可能的取值：</p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227093755.png"/></p><p>若使用a来对样本集 $D$ 进行划分，则会产生 V 个分支结点，其中第$v$个分支结点包含了 $D$ 中所有在属性$a$上取值为$a^v$的样本，记为$D^v$. 我们可根据前面给出的信息熵公式计算出$D^v$的信息熵，再考虑到不同的分支结点所包含的样本数不同，给分支结点赋予权重$\frac{|D^v|}{|D|}$</p><p>即样本数越多的分支结点的影响越大，于是可计算出用属性a对样本集 D 进行划分所获得的”信息增益” (information gain)</p><p>其中：</p><p>特征$a$对训练数据集D的信息增益$Gain(D,a)$，定义为<strong>集合D的信息熵$Ent(D)$</strong>与<strong>给定特征$a$条件下$D$的信息条件熵$Ent(D|a)$</strong>之差，即公式为：</p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227093951.png"/></p><p>  信息熵的计算： </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227093943.png"/></p><p> 条件熵的计算： </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227093935.png"/></p><p>其中：</p><p>$D^v$ 表示$a$属性中第$v$个分支节点包含的样本数</p><p>$C^{kv}$ 表示$a$属性中第$v$个分支节点包含的样本数中，第$k$个类别下包含的样本数</p><p>一般而言，信息增益越大，则意味着<strong>使用属性 $a$ 来进行划分所获得的”纯度提升”越大</strong>。因此，我们可用信息增益来进行决策树的划分属性选择，著名的 <strong>ID3 决策树学习算法</strong> [Quinlan， 1986] 就是以<strong>信息增益为准则</strong>来选择划分属性。 </p></li><li><p>举例说明</p><p>如下图，第一列为论坛号码，第二列为性别，第三列为活跃度，最后一列用户是否流失。</p><p>我们要解决一个问题：<strong>性别和活跃度两个特征，哪个对用户流失影响更大</strong>？</p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227094202.png"/> </p><p>通过计算信息增益可以解决这个问题，统计上右表信息</p><p>其中Positive为正样本（已流失），Negative为负样本（未流失），下面的数值为不同划分下对应的人数。</p><p>可得到三个熵：</p><p><strong>a.计算类别信息熵</strong></p><p>整体熵：</p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227093928.png"/></p><p> <strong>b.计算性别属性的信息熵(a=”性别”)</strong> </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227093638.png"/></p><p> <strong>c.计算性别的信息增益(a=”性别”)</strong> </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227093732.png"/> </p><p> <strong>b.计算活跃度属性的信息熵(a=”活跃度”)</strong> </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227093802.png"/></p><p> <strong>c.计算活跃度的信息增益(a=”活跃度”)</strong> </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227093724.png"/> </p><p> <strong>活跃度的信息增益比性别的信息增益大，也就是说，活跃度对用户流失的影响比性别大。</strong>在做特征选择或者数据分析的时候，我们应该重点考察活跃度这个指标。 </p></li></ul><h5 id="2-信息增益率"><a href="#2-信息增益率" class="headerlink" title="2.信息增益率"></a>2.信息增益率</h5><p> 在上面的介绍中，我们有意忽略了”编号”这一列.若把”编号”也作为一个候选划分属性，则根据信息增益公式可计算出它的信息增益为 0.9182，远大于其他候选划分属性。 </p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">计算每个属性的信息熵过程中,我们发现,该属性的值为<span class="number">0</span>, 也就是其信息增益为<span class="number">0.9182</span>. 但是很明显这么分类,最后出现的结果不具有泛化效果.无法对新样本进行有效预测.</span><br></pre></td></tr></table></figure><p> 实际上，<strong>信息增益准则对可取值数目较多的属性有所偏好</strong>，为减少这种偏好可能带来的不利影响，著名的 <strong>C4.5 决策树算法 [Quinlan， 1993J 不直接使用信息增益，而是使用”增益率” (gain ratio) 来选择最优划分属性</strong>。</p><p> <strong>增益率：</strong>增益率是用前面的信息增益$Gain(D, a)$和属性$a$对应的”固有值”(intrinsic value) [Quinlan , 1993J的比值来共同定义的。 </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227093921.png"/></p><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">属性 a 的可能取值数目越多<span class="comment">(即 V 越大)</span>，则 IV<span class="comment">(a)</span> 的值通常会越大.</span><br></pre></td></tr></table></figure><p> <strong>上个案例中</strong></p><figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">a.计算类别信息熵</span><br><span class="line">b.计算性别属性的信息熵<span class="comment">(性别、活跃度)</span></span><br><span class="line">c.计算活跃度的信息增益<span class="comment">(性别、活跃度)</span></span><br></pre></td></tr></table></figure><p> <strong>d.计算属性分裂信息度量</strong> </p><p> 用分裂信息度量来考虑某种属性进行分裂时分支的数量信息和尺寸信息，我们把这些信息称为属性的内在信息（instrisic information）。信息增益率用信息增益/内在信息，会导致属性的重要性随着内在信息的增大而减小<strong>（也就是说，如果这个属性本身不确定性就很大，那我就越不倾向于选取它）</strong>，这样算是对单纯用信息增益有所补偿。 </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227093717.png"/> </p><p> <strong>e.计算信息增益率</strong> </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227093709.png"/> </p><p>活跃度的信息增益率更高一些，所以在构建决策树的时候，优先选择</p><p>通过这种方式，在选取节点的过程中，我们可以降低取值较多的属性的选取偏好。</p><p><strong>案例二</strong></p><p>如下图，第一列为天气，第二列为温度，第三列为湿度，第四列为风速，最后一列该活动是否进行。</p><p>我们要解决：<strong>根据下面表格数据，判断在对应天气下，活动是否会进行</strong>？</p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227094054.png"/> </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227094008.png"/> </p><p> 该数据集有四个属性，属性集合A={ 天气，温度，湿度，风速}， 类别标签有两个，类别集合L={进行，取消}。 </p><p> <strong>a.计算类别信息熵</strong> </p><p> 类别信息熵表示的是所有样本中各种类别出现的不确定性之和。根据熵的概念，熵越大，不确定性就越大，把事情搞清楚所需要的信息量就越多。 </p><script type="math/tex; mode=display">Ent(D)=-\frac{9}{14}log_2\frac{9}{14}-\frac{5}{14}log_2\frac{5}{14}=0.940</script><p><strong>b.计算每个属性的信息熵</strong></p><p>每个属性的信息熵相当于一种条件熵。他表示的是在某种属性的条件下，各种类别出现的不确定性之和。属性的信息熵越大，表示这个属性中拥有的样本类别越不“纯”。</p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227093911.png"/>  </p><p><strong>c.计算信息增益</strong></p><p>信息增益的 = 熵 - 条件熵，在这里就是 类别信息熵 - 属性信息熵，它表示的是信息不确定性减少的程度。如果一个属性的信息增益越大，就表示用这个属性进行样本划分可以更好的减少划分后样本的不确定性，当然，选择该属性就可以更快更好地完成我们的分类目标。</p><p> <strong>信息增益就是ID3算法的特征选择指标。</strong></p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227093632.png"/></p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">假设我们把上面表格<span class="number">1</span>的数据前面添加一列为<span class="string">"编号"</span>,取值(<span class="number">1</span>-<span class="number">-14</span>). 若把<span class="string">"编号"</span>也作为一个候选划分属性,则根据前面步骤: 计算每个属性的信息熵过程中,我们发现,该属性的值为<span class="number">0</span>, 也就是其信息增益为<span class="number">0.940</span>. 但是很明显这么分类,最后出现的结果不具有泛化效果.此时根据信息增益就无法选择出有效分类特征。所以，C4<span class="number">.5</span>选择使用信息增益率对ID3进行改进。</span><br></pre></td></tr></table></figure><p> <strong>d.计算属性分裂信息度量</strong></p><p>  用分裂信息度量来考虑某种属性进行分裂时分支的数量信息和尺寸信息，我们把这些信息称为属性的内在信息（instrisic information）。信息增益率用信息增益/内在信息，会导致属性的重要性随着内在信息的增大而减小<strong>（也就是说，如果这个属性本身不确定性就很大，那我就越不倾向于选取它）</strong>，这样算是对单纯用信息增益有所补偿。 </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227093701.png"/> </p><p> <strong>e.计算信息增益率</strong> </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227093654.png"/></p><p> 天气的信息增益率最高，选择天气为分裂属性。发现分裂了之后，天气是“阴”的条件下，类别是”纯“的，所以把它定义为叶子节点，选择不“纯”的结点继续分裂。 </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227094038.png"/> </p><p>在子结点当中重复过程1~5，直到所有的叶子结点足够”纯”。</p><p>现在我们来总结一下C4.5的算法流程</p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span>(当前节点<span class="string">"不纯"</span>)：</span><br><span class="line">    <span class="number">1.</span>计算当前节点的类别熵(以类别取值计算)</span><br><span class="line">    <span class="number">2.</span>计算当前阶段的属性熵(按照属性取值吓得类别取值计算)</span><br><span class="line">    <span class="number">3.</span>计算信息增益</span><br><span class="line">    <span class="number">4.</span>计算各个属性的分裂信息度量</span><br><span class="line">    <span class="number">5.</span>计算各个属性的信息增益率</span><br><span class="line">end <span class="keyword">while</span></span><br><span class="line">当前阶段设置为叶子节点</span><br></pre></td></tr></table></figure><h5 id="3-基尼值和基尼指数"><a href="#3-基尼值和基尼指数" class="headerlink" title="3.基尼值和基尼指数"></a>3.基尼值和基尼指数</h5><p> CART 决策树 [Breiman et al., 1984] 使用”基尼指数” (Gini index)来选择划分属性。</p><figure class="highlight armasm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="symbol">CART</span> 是Classification <span class="keyword">and </span>Regression Tree的简称，这是一种著名的决策树学习算法,分类和回归任务都可用。</span><br></pre></td></tr></table></figure><p> <strong>基尼值Gini（D）：</strong>从数据集D中随机抽取两个样本，其类别标记不一致的概率。<strong>故，Gini（D）值越小，数据集D的纯度越高。</strong> </p><p> 数据集 D 的纯度可用基尼值来度量: </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227093905.png"/></p><p> $*p_k=\frac{C^k}{D}$, D为样本的所有数量，$C^k$为第k类样本的数量。 </p><p> <strong>基尼指数Gini_index（D）：</strong>一般，选择使划分后基尼系数最小的属性作为最优化分属性。 </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227093857.png"/></p><p><strong>案例</strong></p><p> 请根据下图列表，按照基尼指数的划分依据，做出决策树。 </p><div class="table-container"><table><thead><tr><th>序号</th><th>是否有房</th><th>婚姻状况</th><th>年收入</th><th>是否拖欠贷款</th></tr></thead><tbody><tr><td>1</td><td>yes</td><td>single</td><td>125k</td><td>no</td></tr><tr><td>2</td><td>no</td><td>married</td><td>100k</td><td>no</td></tr><tr><td>3</td><td>no</td><td>single</td><td>70k</td><td>no</td></tr><tr><td>4</td><td>yes</td><td>married</td><td>120k</td><td>no</td></tr><tr><td>5</td><td>no</td><td>divorced</td><td>95k</td><td>yes</td></tr><tr><td>6</td><td>no</td><td>married</td><td>60k</td><td>no</td></tr><tr><td>7</td><td>yes</td><td>divorced</td><td>220k</td><td>no</td></tr><tr><td>8</td><td>no</td><td>single</td><td>85k</td><td>yes</td></tr><tr><td>9</td><td>no</td><td>married</td><td>75k</td><td>no</td></tr><tr><td>10</td><td>No</td><td>Single</td><td>90k</td><td>Yes</td></tr></tbody></table></div><ol><li><p>对数据集非序列标号属性{是否有房，婚姻状况，年收入}分别计算它们的Gini指数，<strong>取Gini指数最小的属性作为决策树的根节点属性。</strong> </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第一次大循环</span><br></pre></td></tr></table></figure></li><li><p>根节点的Gini值为： </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227093849.png"/></p></li><li><p>当根据是否有房来进行划分时，Gini指数计算过程为： </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227093646.png"/></p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227094152.png"/>  </p></li><li><p>若按婚姻状况属性来划分，属性婚姻状况有三个可能的取值{married，single，divorced}，分别计算划分后的Gini系数增益。 </p></li><li><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227093842.png"/></p><p>对比计算结果，根据婚姻状况属性来划分根节点时取Gini指数最小的分组作为划分结果，即:</p><p>{married} | {single,divorced} </p></li><li><p>同理可得年收入Gini： </p><p>对于年收入属性为数值型属性，首先需要对数据按升序排序，然后从小到大依次用相邻值的中间值作为分隔将样本划分为两组。例如当面对年收入为60和70这两个值时，我们算得其中间值为65。以中间值65作为分割点求出Gini指数。 </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227094032.png"/> </p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227100629.png"/> </p><p>根据计算知道，三个属性划分根节点的指数最小的有两个：年收入属性和婚姻状况，他们的指数都为0.3。此时，选取首先出现的属性【married】作为第一次划分。 </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">第二次大循环</span><br></pre></td></tr></table></figure></li><li><p>接下来，采用同样的方法，分别计算剩下属性，其中根节点的Gini系数为（此时是否拖欠贷款的各有3个records） </p><p> <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227093826.png"/> </p></li><li><p>对于是否有房属性，可得： </p></li><li><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227093818.png"/></p></li><li><p>对于年收入属性则有： </p></li><li><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227094025.png"/></p><p>经过如上流程，构建的决策树，如下图： </p></li></ol><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227094102.png"/></p><p>总结一下CART的算法流程 </p><figure class="highlight ada"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span>(当前节点<span class="string">"不纯"</span>)：</span><br><span class="line">    <span class="number">1</span>.遍历每个变量的每一种分割方式，找到最好的分割点</span><br><span class="line">    <span class="number">2</span>.分割成两个节点N1和N2</span><br><span class="line"><span class="keyword">end</span> <span class="keyword">while</span></span><br><span class="line">每个节点足够“纯”为止</span><br></pre></td></tr></table></figure><h4 id="常见决策树的启发函数比较"><a href="#常见决策树的启发函数比较" class="headerlink" title="常见决策树的启发函数比较"></a>常见决策树的启发函数比较</h4><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227093810.png"/></p><div class="table-container"><table><thead><tr><th>名称</th><th>提出时间</th><th>分支方式</th><th>备注</th></tr></thead><tbody><tr><td>ID3</td><td>1975</td><td>信息增益</td><td>ID3只能对离散属性的数据集构成决策树</td></tr><tr><td>C4.5</td><td>1993</td><td>信息增益率</td><td>优化后解决了ID3分支过程中总喜欢偏向选择值较多的 属性</td></tr><tr><td>CART</td><td>1984</td><td>Gini系数</td><td>可以进行分类和回归，可以处理离散属性，也可以处理连续属性</td></tr></tbody></table></div><h5 id="ID3-算法"><a href="#ID3-算法" class="headerlink" title="ID3 算法"></a>ID3 算法</h5><p><strong>存在的缺点</strong></p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"> (<span class="number">1</span>) ID3算法在选择根节点和各内部节点中的分支属性时，采用信息增益作为评价标准。信息增益的缺点是倾向于选择取值较多的属性，在有些情况下这类属性可能不会提供太多有价值的信息.</span><br><span class="line">(<span class="number">2</span>) ID3算法只能对描述属性为离散型属性的数据集构造决策树。</span><br></pre></td></tr></table></figure><h5 id="C4-5算法"><a href="#C4-5算法" class="headerlink" title="C4.5算法"></a>C4.5算法</h5><p><strong>1.用信息增益率来选择属性</strong></p><p>克服了用信息增益来选择属性时偏向选择值多的属性的不足。</p><p><strong>2.采用了一种后剪枝方法</strong></p><p>避免树的高度无节制的增长，避免过度拟合数据</p><p><strong>3.对于缺失值的处理</strong></p><p>在某些情况下，可供使用的数据可能缺少某些属性的值。假如〈x，c(x)〉是样本集S中的一个训练实例，但是其属性A的值A(x)未知。处理缺少属性值的一种策略是赋给它结点n所对应的训练实例中该属性的最常见值；</p><p>另外一种更复杂的策略是为A的每个可能值赋予一个概率。</p><p>例如，给定一个布尔属性A，如果结点n包含6个已知A=1和4个A=0的实例，那么A(x)=1的概率是0.6，而A(x)=0的概率是0.4。于是，实例x的$60\%$被分配到A=1的分支，$40\%$被分配到另一个分支。</p><p><strong>C4.5就是使用这种方法处理缺少的属性值</strong>。</p><p><strong>C4.5算法的优缺点</strong></p><p> 优点：产生的分类规则易于理解，准确率较高。</p><p> 缺点： 在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。此外，C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。</p><h5 id="CART算法"><a href="#CART算法" class="headerlink" title="CART算法"></a>CART算法</h5><p> CART算法相比C4.5算法的分类方法，采用了简化的二叉树模型，同时特征选择采用了近似的基尼系数来简化计算。 </p><p> <strong>C4.5不一定是二叉树，但CART一定是二叉树</strong> </p><h5 id="多变量决策树-multi-variate-decision-tree"><a href="#多变量决策树-multi-variate-decision-tree" class="headerlink" title="多变量决策树(multi-variate decision tree)"></a>多变量决策树(multi-variate decision tree)</h5><p> 同时，无论是ID3, C4.5还是CART,在做特征选择的时候都是选择最优的一个特征来做分类决策，但是大多数，<strong>分类决策不应该是由某一个特征决定的，而是应该由一组特征决定的。</strong>这样决策得到的决策树更加准确。这个决策树叫做多变量决策树(multi-variate decision tree)。在选择最优特征的时候，多变量决策树不是选择某一个最优特征，而是选择最优的一个特征线性组合来做决策。这个算法的代表是OC1 （了解一下）。</p><p> 如果样本发生一点点的改动，就会导致树结构的剧烈改变。这个可以通过集成学习里面的随机森林之类的方法解决。 </p><h4 id="决策树变量的两种类型"><a href="#决策树变量的两种类型" class="headerlink" title="决策树变量的两种类型"></a>决策树变量的两种类型</h4><ol><li>数字型（Numeric）：变量类型是整数或浮点数，如前面例子中的“年收入”。用“&gt;=”，“&gt;”,“&lt;”或“&lt;=”作为分割条件（排序后，利用已有的分割情况，可以优化分割算法的时间复杂度）。</li><li>名称型（Nominal）：类似编程语言中的枚举类型，变量只能从有限的选项中选取，比如前面例子中的“婚姻情况”，只能是“单身”，“已婚”或“离婚”，使用“=”来分割。</li></ol><h4 id="如何评估分割点的好坏"><a href="#如何评估分割点的好坏" class="headerlink" title="如何评估分割点的好坏"></a>如何评估分割点的好坏</h4><p>如果一个分割点可以将当前的所有节点分为两类，使得每一类都很“纯”，也就是同一类的记录较多，那么就是一个好分割点。</p><p>比如上面的例子，“拥有房产”，可以将记录分成了两类，“是”的节点全部都可以偿还债务，非常“纯”；“否”的节点，可以偿还贷款和无法偿还贷款的人都有，不是很“纯”，但是两个节点加起来的纯度之和与原始节点的纯度之差最大，所以按照这种方法分割。</p><p>构建决策树采用贪心算法，只考虑当前纯度差最大的情况作为分割点。</p><h4 id="cart剪枝"><a href="#cart剪枝" class="headerlink" title="cart剪枝"></a>cart剪枝</h4><h5 id="为什么要剪枝"><a href="#为什么要剪枝" class="headerlink" title="为什么要剪枝"></a>为什么要剪枝</h5><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200227094206.png"/> </p><ul><li><strong>图形描述</strong><ul><li>横轴表示在决策树创建过程中树的结点总数，纵轴表示决策树的预测精度。</li><li>实线显示的是决策树在训练集上的精度，虚线显示的则是在一个独立的测试集上测量出来的精度。</li><li>随着树的增长，在训练样集上的精度是单调上升的， 然而在独立的测试样例上测出的精度先上升后下降。</li></ul></li><li><strong>出现这种情况的原因：</strong><ul><li>原因1：噪声、样本冲突，即错误的样本数据。</li><li>原因2：特征即属性不能完全作为分类标准。</li><li>原因3：巧合的规律性，数据量不够大。</li></ul></li></ul><h5 id="常用的减枝方法"><a href="#常用的减枝方法" class="headerlink" title="常用的减枝方法"></a>常用的减枝方法</h5><ol><li><p><strong>预剪枝</strong></p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1</span>）每一个结点所包含的最小样本数目，例如<span class="number">10</span>，则该结点总样本数小于<span class="number">10</span>时，则不再分；</span><br><span class="line"><span class="number">2</span>）指定树的高度或者深度，例如树的最大深度为<span class="number">4</span>；</span><br><span class="line"><span class="number">3</span>）指定结点的熵小于某个值，不再划分。随着树的增长， 在训练样集上的精度是单调上升的， 然而在独立的测试样例上测出的精度先上升后下降。</span><br></pre></td></tr></table></figure></li><li><p><strong>后剪枝</strong></p><p>把一棵树，构建完成之后，再进行从下往上的剪枝 </p></li></ol>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;决策树&quot;&gt;&lt;a href=&quot;#决策树&quot; class=&quot;headerlink&quot; title=&quot;决策树&quot;&gt;&lt;/a&gt;决策树&lt;/h4&gt;&lt;p&gt;决策树思想的来源非常朴素，程序设计中的条件分支结构就是if-else结构，最早的决策树就是利用这类结构分割数据的一种分类学习方法。&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;决策树：是一种树形结构，其中每个内部节点表示一个属性上的判断，每个分支代表一个判断结果的输出，最后每个叶节点代表一种分类结果，本质是一颗由多个判断节点组成的树&lt;/strong&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="机器学习算法" scheme="https://xiaoliaozi.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
    
      <category term="机器学习算法" scheme="https://xiaoliaozi.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
      <category term="机器学习" scheme="https://xiaoliaozi.com/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
      <category term="监督学习算法" scheme="https://xiaoliaozi.com/tags/%E7%9B%91%E7%9D%A3%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/"/>
    
      <category term="决策树分类原理" scheme="https://xiaoliaozi.com/tags/%E5%86%B3%E7%AD%96%E6%A0%91%E5%88%86%E7%B1%BB%E5%8E%9F%E7%90%86/"/>
    
  </entry>
  
  <entry>
    <title>TCP协议</title>
    <link href="https://xiaoliaozi.com/2020/01/09/TCP%E5%8D%8F%E8%AE%AE/"/>
    <id>https://xiaoliaozi.com/2020/01/09/TCP%E5%8D%8F%E8%AE%AE/</id>
    <published>2020-01-09T15:32:41.000Z</published>
    <updated>2020-02-22T07:05:47.158Z</updated>
    
    <content type="html"><![CDATA[<h4 id="网络应用程序之间的通信流程"><a href="#网络应用程序之间的通信流程" class="headerlink" title="网络应用程序之间的通信流程"></a>网络应用程序之间的通信流程</h4><p> 通过 IP 地址能够找到对应的设备，然后再通过端口号找到对应的端口，再通过端口把数据传输给应用程序，<strong>这里要注意，数据不能随便发送，在发送之前还需要选择一个对应的传输协议，保证程序之间按照指定的传输规则进行数据的通信，</strong> 而这个传输协议就是TCP协议。</p> <a id="more"></a><h4 id="TCP-的概念"><a href="#TCP-的概念" class="headerlink" title="TCP 的概念"></a>TCP 的概念</h4><p>TCP 的英文全拼(Transmission Control Protocol)简称<strong>传输控制协议</strong>，它是一种<strong>面向连接的、可靠的、基于字节流的传输层通信协议</strong>。</p><p><strong>面向连接的效果图:</strong></p><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200222150331.png"/> </p><p><strong>TCP 通信步骤:</strong></p><ol><li>创建连接</li><li>传输数据</li><li>关闭连接</li></ol><h4 id="TCP-的特点"><a href="#TCP-的特点" class="headerlink" title="TCP 的特点"></a>TCP 的特点</h4><ol><li>面向连接<ul><li>通信双方必须先建立好连接才能进行数据的传输，数据传输完成后，双方必须断开此连接，以释放系统资源。双方间的数据传输都可以通过这一个连接进行，完成数据交换后，双方必须断开此连接，以释放系统资源。<strong>这种连接是一对一的，因此TCP不适用于广播的应用程序，基于广播的应用程序请使用UDP协议</strong></li></ul></li><li>可靠传输<ul><li>TCP 采用发送应答机制：通过TCP这种方式发送的每一个报文段都必须得到接收方的应答才认为这个TCP报文传送成功。</li><li>超时重传：指定时间内没有应答会重新发送。</li><li>错误校验：传输数据必须与接收数据一致，否则传输失败。</li><li>流量控制和阻塞管理：流量控制用来避免发送端发送过快而使得接收方来不及接收。</li></ul></li><li>TCP 是一个<strong>稳定、可靠的传输协议，常用于对数据进行准确无误的传输，比如: 文件下载，浏览器上网</strong>。 </li></ol><h4 id="TCP在建立连接时的三次握手"><a href="#TCP在建立连接时的三次握手" class="headerlink" title="TCP在建立连接时的三次握手"></a>TCP在建立连接时的三次握手</h4><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">TCP在建立连接时需要通过三次握手过程来完成。</span><br><span class="line">  原因：TCP 协议为了实现可靠传输， 通信双方需要判断自己已经发送的数据包是否都被接收方收到， 如果没收到， 就需要重发。 为了实现这个需求， 很自然地就会引出序号（sequence number） 和确认号（acknowledgement number） 的使用。发送方在发送数据包（假设大小为 <span class="number">10</span> byte）时， 同时送上一个序号( 假设为 <span class="number">500</span>)，那么接收方收到这个数据包以后， 就可以回复一个确认号（<span class="number">510</span> = <span class="number">500</span> + <span class="number">10</span>） 告诉发送方 “我已经收到了你的数据包， 你可以发送下一个数据包， 序号从 <span class="number">510</span> 开始” 。这样发送方就可以知道哪些数据被接收到，哪些数据没被接收到， 需要重发。</span><br></pre></td></tr></table></figure><ul><li>第一次握手：Client将标志位SYN置为1，随机产生一个值seq=J，并将该数据包发送给Server，Client进入SYN_SENT状态，等待Server确认。</li><li>第二次握手：Server收到数据包后由标志位SYN=1知道Client请求建立连接，Server将标志位SYN和ACK都置为1，ack (number )=J+1，随机产生一个值seq=K，并将该数据包发送给Client以确认连接请求，Server进入SYN_RCVD状态。</li><li>第三次握手：Client收到确认后，检查ack是否为J+1，ACK是否为1，如果正确则将标志位ACK置为1，ack=K+1，并将该数据包发送给Server，Server检查ack是否为K+1，ACK是否为1，如果正确则连接建立成功，Client和Server进入ESTABLISHED状态，完成三次握手，随后Client与Server之间可以开始传输数据了。</li></ul><h4 id="TCP在断开连接时的四次挥手"><a href="#TCP在断开连接时的四次挥手" class="headerlink" title="TCP在断开连接时的四次挥手"></a>TCP在断开连接时的四次挥手</h4><p>TCP在断开连接时，需要通过四次挥手的过程来完成。</p><ul><li>第一次挥手：Client发送一个FIN，用来关闭Client到Server的数据传送。</li><li>第二次挥手：Server收到FIN后，发送一个ACK给Client，确认序号为收到序号+1。</li><li>第三次挥手：Server发送一个FIN，用来关闭Server到Client的数据传送。</li><li>第四次挥手：Client收到FIN后，接着发送一个ACK给Server，确认序号为收到序号+1。</li></ul><h4 id="TCP-网络应用程序开发流程"><a href="#TCP-网络应用程序开发流程" class="headerlink" title="TCP 网络应用程序开发流程"></a>TCP 网络应用程序开发流程</h4><p>TCP 网络应用程序开发分为:</p><ul><li>TCP 客户端程序开发</li><li>TCP 服务端程序开发</li></ul><p><strong>说明:</strong></p><p>客户端程序是指运行在<strong>用户设备上的程序</strong> 服务端程序是指运行在<strong>服务器设备上的程序</strong>，专门为客户端提供数据服务。</p><h5 id="TCP-客户端程序开发流程的介绍"><a href="#TCP-客户端程序开发流程的介绍" class="headerlink" title="TCP 客户端程序开发流程的介绍"></a>TCP 客户端程序开发流程的介绍</h5><p>  <img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200222150415.png"/></p><p><strong>步骤说明:</strong></p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span> 创建客户端套接字对象</span><br><span class="line"><span class="number">2.</span> 和服务端套接字建立连接</span><br><span class="line"><span class="number">3.</span> 发送数据</span><br><span class="line"><span class="number">4.</span> 接收数据</span><br><span class="line"><span class="number">5.</span> 关闭客户端套接字</span><br></pre></td></tr></table></figure><h5 id="TCP-服务端程序开发流程的介绍"><a href="#TCP-服务端程序开发流程的介绍" class="headerlink" title="TCP 服务端程序开发流程的介绍"></a>TCP 服务端程序开发流程的介绍</h5><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200222150440.png"/> </p><p><strong>步骤说明:</strong></p><figure class="highlight angelscript"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="number">1.</span> 创建服务端端套接字对象</span><br><span class="line"><span class="number">2.</span> 绑定端口号</span><br><span class="line"><span class="number">3.</span> 设置监听</span><br><span class="line"><span class="number">4.</span> 等待接受客户端的连接请求</span><br><span class="line"><span class="number">5.</span> 接收数据</span><br><span class="line"><span class="number">6.</span> 发送数据</span><br><span class="line"><span class="number">7.</span> 关闭套接字</span><br></pre></td></tr></table></figure><p><strong>主动发起建立连接请求的</strong>是客户端程序</p><p><strong>等待接受连接请求的</strong>是服务端程序</p><h4 id="socket-介绍"><a href="#socket-介绍" class="headerlink" title="socket 介绍"></a>socket 介绍</h4><p> socket (简称 套接字) 是<strong>进程之间通信一个工具</strong>，好比现实生活中的<strong>插座</strong>，所有的家用电器要想工作都是基于插座进行，<strong>进程之间想要进行网络通信需要基于这个 socket</strong>。 </p><p>socket 的作用：负责<strong>进程之间的网络数据传输</strong>，好比数据的搬运工。 </p><h4 id="TCP-客户端程序开发API"><a href="#TCP-客户端程序开发API" class="headerlink" title="TCP 客户端程序开发API"></a>TCP 客户端程序开发API</h4><p><strong>socket 类的介绍</strong>：</p><p>1.导入 socket 模块</p><figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br></pre></td></tr></table></figure><p>2.创建客户端 socket 对象</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">socket.socket(AddressFamily, Type)</span><br><span class="line">参数说明:</span><br><span class="line">- AddressFamily 表示IP地址类型, 分为IPv4和IPv6</span><br><span class="line">-<span class="built_in"> Type </span>表示传输协议类型</span><br><span class="line">方法说明:</span><br><span class="line">- connect((host, port)) 表示和服务端套接字建立连接, host是服务器ip地址，port是应用程序的端口号</span><br><span class="line">- send(data) 表示发送数据，data是二进制数据</span><br><span class="line">- recv(buffersize) 表示接收数据, buffersize是每次接收数据的长度</span><br></pre></td></tr></table></figure><p>3.创建服务端 socket 对象</p><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">socket.socket(AddressFamily, Type)</span><br><span class="line">参数说明:</span><br><span class="line">- AddressFamily 表示IP地址类型, 分为IPv4和IPv6</span><br><span class="line">-<span class="built_in"> Type </span>表示传输协议类型</span><br><span class="line">方法说明:</span><br><span class="line">- bind((host, port)) 表示绑定端口号, host 是<span class="built_in"> ip </span>地址，port 是端口号，ip 地址一般不指定，表示本机的任何一个ip地址都可以。</span><br><span class="line">- listen (backlog) 表示设置监听，backlog参数表示最大等待建立连接的个数。</span><br><span class="line">- accept() 表示等待接受客户端的连接请求</span><br><span class="line">- send(data) 表示发送数据，data 是二进制数据</span><br><span class="line">- recv(buffersize) 表示接收数据, buffersize 是每次接收数据的长度</span><br></pre></td></tr></table></figure><h4 id="TCP-客户端程序开发示例代码"><a href="#TCP-客户端程序开发示例代码" class="headerlink" title="TCP 客户端程序开发示例代码"></a>TCP 客户端程序开发示例代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 创建tcp客户端套接字</span></span><br><span class="line">    <span class="comment"># 1. AF_INET：表示ipv4</span></span><br><span class="line">    <span class="comment"># 2. SOCK_STREAM: tcp传输协议</span></span><br><span class="line">    tcp_client_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)</span><br><span class="line">    <span class="comment"># 和服务端应用程序建立连接</span></span><br><span class="line">    tcp_client_socket.connect((<span class="string">"192.168.131.62"</span>, <span class="number">8080</span>))</span><br><span class="line">    <span class="comment"># 代码执行到此，说明连接建立成功</span></span><br><span class="line">    <span class="comment"># 准备发送的数据</span></span><br><span class="line">    send_data = <span class="string">"你好服务端，我是客户端小黑!"</span>.encode(<span class="string">"gbk"</span>)</span><br><span class="line">    <span class="comment"># 发送数据</span></span><br><span class="line">    tcp_client_socket.send(send_data)</span><br><span class="line">    <span class="comment"># 接收数据, 这次接收的数据最大字节数是1024</span></span><br><span class="line">    recv_data = tcp_client_socket.recv(<span class="number">1024</span>)</span><br><span class="line">    <span class="comment"># 返回的直接是服务端程序发送的二进制数据</span></span><br><span class="line">    print(recv_data)</span><br><span class="line">    <span class="comment"># 对数据进行解码</span></span><br><span class="line">    recv_content = recv_data.decode(<span class="string">"gbk"</span>)</span><br><span class="line">    print(<span class="string">"接收服务端的数据为:"</span>, recv_content)</span><br><span class="line">    <span class="comment"># 关闭套接字</span></span><br><span class="line">    tcp_client_socket.close()</span><br></pre></td></tr></table></figure><h4 id="案例-多任务版TCP服务端程序开发"><a href="#案例-多任务版TCP服务端程序开发" class="headerlink" title="案例-多任务版TCP服务端程序开发"></a>案例-多任务版TCP服务端程序开发</h4><h5 id="需求"><a href="#需求" class="headerlink" title="需求"></a>需求</h5><p>目前我们开发的TCP服务端程序只能服务于一个客户端，如何开发一个多任务版的TCP服务端程序能够服务于多个客户端呢?</p><p>完成多任务，可以使用<strong>线程</strong>，比进程更加节省内存资源。</p><h5 id="具体实现步骤"><a href="#具体实现步骤" class="headerlink" title="具体实现步骤"></a>具体实现步骤</h5><ol><li>编写一个TCP服务端程序，循环等待接受客户端的连接请求</li><li>当客户端和服务端建立连接成功，创建子线程，使用子线程专门处理客户端的请求，防止主线程阻塞</li><li>把创建的子线程设置成为守护主线程，防止主线程无法退出。</li></ol><h5 id="多任务版TCP服务端程序的示例代码"><a href="#多任务版TCP服务端程序的示例代码" class="headerlink" title="多任务版TCP服务端程序的示例代码"></a>多任务版TCP服务端程序的示例代码</h5><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> socket</span><br><span class="line"><span class="keyword">import</span> threading</span><br><span class="line"></span><br><span class="line"><span class="comment"># 处理客户端的请求操作</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handle_client_request</span><span class="params">(service_client_socket, ip_port)</span>:</span></span><br><span class="line">    <span class="comment"># 循环接收客户端发送的数据</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># 接收客户端发送的数据</span></span><br><span class="line">        recv_data = service_client_socket.recv(<span class="number">1024</span>)</span><br><span class="line">        <span class="comment"># 容器类型判断是否有数据可以直接使用if语句进行判断，如果容器类型里面有数据表示条件成立，否则条件失败</span></span><br><span class="line">        <span class="comment"># 容器类型: 列表、字典、元组、字符串、set、range、二进制数据</span></span><br><span class="line">        <span class="keyword">if</span> recv_data:</span><br><span class="line">            print(recv_data.decode(<span class="string">"gbk"</span>), ip_port)</span><br><span class="line">            <span class="comment"># 回复</span></span><br><span class="line">            service_client_socket.send(<span class="string">"ok，问题正在处理中..."</span>.encode(<span class="string">"gbk"</span>))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            print(<span class="string">"客户端下线了:"</span>, ip_port)</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="comment"># 终止和客户端进行通信</span></span><br><span class="line">    service_client_socket.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 创建tcp服务端套接字</span></span><br><span class="line">    tcp_server_socket = socket.socket(socket.AF_INET, socket.SOCK_STREAM)</span><br><span class="line">    <span class="comment"># 设置端口号复用，让程序退出端口号立即释放</span></span><br><span class="line">    tcp_server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, <span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># 绑定端口号</span></span><br><span class="line">    tcp_server_socket.bind((<span class="string">""</span>, <span class="number">9090</span>))</span><br><span class="line">    <span class="comment"># 设置监听, listen后的套接字是被动套接字，只负责接收客户端的连接请求</span></span><br><span class="line">    tcp_server_socket.listen(<span class="number">128</span>)</span><br><span class="line">    <span class="comment"># 循环等待接收客户端的连接请求</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># 等待接收客户端的连接请求</span></span><br><span class="line">        service_client_socket, ip_port = tcp_server_socket.accept()</span><br><span class="line">        print(<span class="string">"客户端连接成功:"</span>, ip_port)</span><br><span class="line">        <span class="comment"># 当客户端和服务端建立连接成功以后，需要创建一个子线程，不同子线程负责接收不同客户端的消息</span></span><br><span class="line">        sub_thread = threading.Thread(target=handle_client_request, args=(service_client_socket, ip_port))</span><br><span class="line">        <span class="comment"># 设置守护主线程</span></span><br><span class="line">        sub_thread.setDaemon(<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 启动子线程</span></span><br><span class="line">        sub_thread.start()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment"># tcp服务端套接字可以不需要关闭，因为服务端程序需要一直运行</span></span><br><span class="line">    <span class="comment"># tcp_server_socket.close()</span></span><br></pre></td></tr></table></figure><p><strong>说明:</strong></p><p>当客户端和服务端建立连接后，<strong>服务端程序退出后端口号不会立即释放，需要等待大概1-2分钟。</strong></p><p>解决办法有两种:</p><ol><li>更换服务端端口号</li><li>设置端口号复用(推荐大家使用)，也就是说让服务端程序退出后端口号立即释放。</li></ol><p>设置端口号复用的代码如下:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 参数1: 表示当前套接字</span></span><br><span class="line"><span class="comment"># 参数2: 设置端口号复用选项</span></span><br><span class="line"><span class="comment"># 参数3: 设置端口号复用选项对应的值</span></span><br><span class="line">tcp_server_socket.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, <span class="literal">True</span>)</span><br></pre></td></tr></table></figure><h4 id="socket之send和recv原理剖析"><a href="#socket之send和recv原理剖析" class="headerlink" title="socket之send和recv原理剖析"></a>socket之send和recv原理剖析</h4><h5 id="认识TCP-socket的发送和接收缓冲区"><a href="#认识TCP-socket的发送和接收缓冲区" class="headerlink" title="认识TCP socket的发送和接收缓冲区"></a>认识TCP socket的发送和接收缓冲区</h5><p>当创建一个TCP socket对象的时候会有一个<strong>发送缓冲区</strong>和一个<strong>接收缓冲区</strong>，<strong>这个发送和接收缓冲区指的就是内存中的一片空间。</strong></p><h5 id="send原理剖析"><a href="#send原理剖析" class="headerlink" title="send原理剖析"></a>send原理剖析</h5><p>send是不是直接把数据发给服务端?</p><p>不是，要想发数据，必须得<strong>通过网卡发送数据</strong>，应用程序是无法直接通过网卡发送数据的，它需要调用操作系统接口，也就是说，应用程序把发送的数据先写入到<strong>发送缓冲区</strong>(内存中的一片空间)，再<strong>由操作系统控制网卡把发送缓冲区的数据发送给服务端网卡</strong> 。</p><h5 id="recv原理剖析"><a href="#recv原理剖析" class="headerlink" title="recv原理剖析"></a>recv原理剖析</h5><p>recv是不是直接从客户端接收数据?</p><p>不是，<strong>应用软件是无法直接通过网卡接收数据的</strong>，它需要调用操作系统接口，<strong>由操作系统通过网卡接收数据</strong>，把接收的数据<strong>写入到接收缓冲区</strong>(内存中的一片空间），应用程序<strong>再从接收缓存区获取客户端发送的数据</strong>。</p><h5 id="send和recv原理剖析图"><a href="#send和recv原理剖析图" class="headerlink" title="send和recv原理剖析图"></a>send和recv原理剖析图</h5><p><img src="https://raw.githubusercontent.com/xlztongxue/picgo/master/img/20200222150519.png"/> </p><ul><li>发送数据是发送到发送缓冲区</li><li>接收数据是从接收缓冲区 获取</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;h4 id=&quot;网络应用程序之间的通信流程&quot;&gt;&lt;a href=&quot;#网络应用程序之间的通信流程&quot; class=&quot;headerlink&quot; title=&quot;网络应用程序之间的通信流程&quot;&gt;&lt;/a&gt;网络应用程序之间的通信流程&lt;/h4&gt;&lt;p&gt; 通过 IP 地址能够找到对应的设备，然后再通过端口号找到对应的端口，再通过端口把数据传输给应用程序，&lt;strong&gt;这里要注意，数据不能随便发送，在发送之前还需要选择一个对应的传输协议，保证程序之间按照指定的传输规则进行数据的通信，&lt;/strong&gt; 而这个传输协议就是TCP协议。&lt;/p&gt;
    
    </summary>
    
    
      <category term="网络编程" scheme="https://xiaoliaozi.com/categories/%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B/"/>
    
    
      <category term="传输层通信协议" scheme="https://xiaoliaozi.com/tags/%E4%BC%A0%E8%BE%93%E5%B1%82%E9%80%9A%E4%BF%A1%E5%8D%8F%E8%AE%AE/"/>
    
      <category term="TCP协议" scheme="https://xiaoliaozi.com/tags/TCP%E5%8D%8F%E8%AE%AE/"/>
    
      <category term="TCP应用程序开发" scheme="https://xiaoliaozi.com/tags/TCP%E5%BA%94%E7%94%A8%E7%A8%8B%E5%BA%8F%E5%BC%80%E5%8F%91/"/>
    
  </entry>
  
</feed>
